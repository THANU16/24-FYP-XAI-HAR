{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2078683,"sourceType":"datasetVersion","datasetId":1246162}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"papermill":{"default_parameters":{},"duration":3551.555585,"end_time":"2024-10-25T17:51:18.926873","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-10-25T16:52:07.371288","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#### Necessary Imports\nimport os\nimport logging\nimport inspect \nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom matplotlib.pyplot import figure\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport pickle\nfrom torch import nn\nimport torch\nimport subprocess\nfrom torch.utils.data import Dataset, DataLoader\nimport math\nimport time\nimport seaborn as sns\nimport matplotlib as mpl\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics import silhouette_score\nimport shutil\nimport random\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score,f1_score,confusion_matrix\nfrom sklearn.decomposition import PCA\nimport torch.optim as optim\nimport torch.optim.lr_scheduler as lr_scheduler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport wandb\nimport random","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":7.349917,"end_time":"2024-10-25T16:52:17.611726","exception":false,"start_time":"2024-10-25T16:52:10.261809","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T17:22:49.582147Z","iopub.execute_input":"2024-10-27T17:22:49.582703Z","iopub.status.idle":"2024-10-27T17:22:55.355759Z","shell.execute_reply.started":"2024-10-27T17:22:49.582654Z","shell.execute_reply":"2024-10-27T17:22:55.354534Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# pip install wandb","metadata":{"papermill":{"duration":0.026915,"end_time":"2024-10-25T16:52:17.659772","exception":false,"start_time":"2024-10-25T16:52:17.632857","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T17:22:55.357838Z","iopub.execute_input":"2024-10-27T17:22:55.358537Z","iopub.status.idle":"2024-10-27T17:22:55.363065Z","shell.execute_reply.started":"2024-10-27T17:22:55.358492Z","shell.execute_reply":"2024-10-27T17:22:55.361768Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Validate the parameter for check\n\n# Read parameters from environment variables\nepochs_p = int(os.getenv(\"EPOCHS\", 5))\nepoch_per_batch_count_p = int(os.getenv(\"EPOCH_PER_BATCH_COUNT\", 2))\nweighted_loss_factor_p = float(os.getenv(\"WEIGHTED_LOSS_FACTOR\", 0.1))\nprint(f\"Parameters: epochs={epochs_p}, epoch_per_batch_count={epoch_per_batch_count_p}, weighted_loss_factor={weighted_loss_factor_p}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T17:22:55.364890Z","iopub.execute_input":"2024-10-27T17:22:55.365764Z","iopub.status.idle":"2024-10-27T17:22:55.376750Z","shell.execute_reply.started":"2024-10-27T17:22:55.365693Z","shell.execute_reply":"2024-10-27T17:22:55.375619Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Parameters: epochs=5, epoch_per_batch_count=2, weighted_loss_factor=0.1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Define Constants","metadata":{"papermill":{"duration":0.019921,"end_time":"2024-10-25T16:52:17.699430","exception":false,"start_time":"2024-10-25T16:52:17.679509","status":"completed"},"tags":[]}},{"cell_type":"code","source":"LOG_LEVEL = \"INFO\" # Adjust this to \"DEBUG\", \"INFO\", \"WARNING\" or \"ERROR\"\n\nWANDB_API_KEY = \"b15ee5c84e51289dd7b5dd11ea38949957d772f9\"\n\n\nactivity_id_mapping = {\n    0 : \"Stand\",\n    1 : \"Sit\",\n    2 : \"Talk-sit\",\n    3 : \"Talk-stand\",\n    4 : \"Stand-sit\",\n    5 : \"Lay\",\n    6 : \"Lay-stand\",\n    7 : \"Pick\",\n    8 : \"Jump\",\n    9 : \"Push-up\",\n    10 : \"Sit-up\",\n    11 : \"Walk\",\n    12 : \"Walk-backward\",\n    13 : \"Walk-circle\",\n    14 : \"Run\",\n    15 : \"Stair-up\",\n    16 : \"Stair-down\",\n    17 : \"Table-tennis\"\n}\n\ncolumns = [\"activityID\", \"subjectID\", \n               \"acc_x\", \"acc_y\", \"acc_z\", \n               \"gyro_x\", \"gyro_y\", \"gyro_z\", \n               \"ori_x\", \"ori_y\", \"ori_z\"]\n\nEPOCH_BATCH_COUNT = 20\nEPOCHS = 5\nBATCH_SIZE = 2\nIMU_FEATURE_COUNT = 24\nCLASSES = 18\nLEARNING_RATE = 0.0005 # change learning rate < 0.01\n\nBEST_ACCURACY = 0.0\nBEST_LOSS = 1000\n\nMAX_SAVED_MODELS = 6   # Maximum number of models to keep\n\n\nWEIGHT_LOSS= 0.5\n\n# WEIGHT_CLASSIFIER_LOSS  = 0.9  # >= 8 \n\n# WEIGHT_TRIPLET_LOSS = 0.09  # < 0.15\n\n\nWEIGHT_NPAIR_LOSS = 0.2\nWEIGHT_CLASSIFIER_LOSS  = 1 - WEIGHT_NPAIR_LOSS\n\nSEQ_LENTH = 500\nWINDOW_SIZE = 20\nNUM_NEGATIVES = 4","metadata":{"papermill":{"duration":0.031746,"end_time":"2024-10-25T16:52:17.750863","exception":false,"start_time":"2024-10-25T16:52:17.719117","status":"completed"},"scrolled":true,"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:41:46.841728Z","iopub.execute_input":"2024-10-26T09:41:46.842089Z","iopub.status.idle":"2024-10-26T09:41:46.852421Z","shell.execute_reply.started":"2024-10-26T09:41:46.842022Z","shell.execute_reply":"2024-10-26T09:41:46.851521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Export logs","metadata":{"papermill":{"duration":0.019738,"end_time":"2024-10-25T16:52:17.790694","exception":false,"start_time":"2024-10-25T16:52:17.770956","status":"completed"},"tags":[]}},{"cell_type":"code","source":"wandb.login(key=WANDB_API_KEY)","metadata":{"papermill":{"duration":0.572727,"end_time":"2024-10-25T16:52:18.383714","exception":false,"start_time":"2024-10-25T16:52:17.810987","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:41:46.853740Z","iopub.execute_input":"2024-10-26T09:41:46.854061Z","iopub.status.idle":"2024-10-26T09:41:46.868901Z","shell.execute_reply.started":"2024-10-26T09:41:46.854007Z","shell.execute_reply":"2024-10-26T09:41:46.868097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# start a new wandb run to track this script\nwandb.init(\n    # set the wandb project where this run will be logged\n    project=\"har-using-imu-data-npair\",\n#     name = \"Version-17-lr\",\n    name = \"Version-test-Tigger\",\n#     name = \"Interctive-7\",\n#     notes = \"Same as version 53, losss- Triplet cousine\",\n\n    # track hyperparameters and run metadata\n    config={\n        \"architecture\": \"Transformer\",\n        \"dataset\": \"KU-HAR\",\n        \"epochs\": EPOCHS,\n        \"epoch_batch_count\" : EPOCH_BATCH_COUNT,\n        \"batch_size\" : BATCH_SIZE,\n        \"imu_feature_count\" : IMU_FEATURE_COUNT,\n        \"classes\" : CLASSES,\n        \"learning_rate\" : LEARNING_RATE,\n        \"weight_loss\" : WEIGHT_LOSS,\n        \"WEIGHT_CLASSIFIER_LOSS\" : WEIGHT_CLASSIFIER_LOSS,\n#         \"WEIGHT_TRIPLET_LOSS\" : WEIGHT_TRIPLET_LOSS,\n        \"WEIGHT_NPAIR_LOSS\" : WEIGHT_NPAIR_LOSS,\n        \"SEQ_LENTH\" : SEQ_LENTH,\n        \"WINDOW_SIZE\" : WINDOW_SIZE,\n        \"NUM_NEGATIVES\" : NUM_NEGATIVES,\n    }\n)\n\n# # simulate training\n# epochs = 10\n# offset = random.random() / 5\n# for epoch in range(2, epochs):\n#     acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n#     loss = 2 ** -epoch + random.random() / epoch + offset\n\n#     # log metrics to wandb\n#     wandb.log({\"acc\": acc, \"loss\": loss})\n\n# # [optional] finish the wandb run, necessary in notebooks\n# wandb.finish()","metadata":{"papermill":{"duration":3.86546,"end_time":"2024-10-25T16:52:22.270472","exception":false,"start_time":"2024-10-25T16:52:18.405012","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:41:46.871100Z","iopub.execute_input":"2024-10-26T09:41:46.871447Z","iopub.status.idle":"2024-10-26T09:41:49.733740Z","shell.execute_reply.started":"2024-10-26T09:41:46.871407Z","shell.execute_reply":"2024-10-26T09:41:49.732708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Genarate logs","metadata":{"papermill":{"duration":0.022221,"end_time":"2024-10-25T16:52:22.316864","exception":false,"start_time":"2024-10-25T16:52:22.294643","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Map log level strings to logging constants\nlog_levels = {\n    \"DEBUG\": logging.DEBUG,\n    \"INFO\": logging.INFO,\n    \"WARNING\": logging.WARNING,\n    \"ERROR\": logging.ERROR\n}\nset_log_level = log_levels.get(LOG_LEVEL, logging.INFO)  # Default to INFO if an unrecognized level is given\n\n# Configure the logging format and level\nlogging.basicConfig(\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    level=set_log_level,\n    datefmt=\"%Y-%m-%d %H:%M:%S\"\n)\n\ndef log_message(level, message, block=None, log_title=\"\", caller_frame=None):\n    \"\"\"\n    Logs a message with a specific logging level and additional details.\n    \n    Args:\n        level (str): Logging level ('DEBUG', 'INFO', 'WARNING', 'ERROR').\n        message (str): The message to log.\n        block (str, optional): Additional block/section name for context.\n        log_title (str): Title to specify log type.\n        caller_frame (frame, optional): Frame object of the calling function.\n    \"\"\"\n    # Get the calling function's details for context\n    line_number = caller_frame.f_lineno if caller_frame else \"N/A\"\n    function_name = caller_frame.f_code.co_name if caller_frame else \"N/A\"\n\n    # Format log title and line number to a fixed width\n    formatted_title = log_title.ljust(7)  # Pad log title to 7 characters\n    formatted_line = f\"Line {line_number}\".ljust(8)  # Pad line info to 8 characters\n\n    # Format the log with extra details\n    # log_msg = f\"{formatted_title} | {formatted_line} | {function_name} | {message}\"\n    log_msg = f\"{formatted_title} | {formatted_line} | {message}\"\n    if block:\n        log_msg += f\" - Block: {block}\"\n\n    # Check if the log level should print based on the configured log level\n    should_print = log_levels[level.upper()] >= set_log_level\n\n    if should_print:\n        print(log_msg)  # Print the message if it meets or exceeds the log level\n\n    # Log the message based on the specified level\n    if level.upper() == \"DEBUG\":\n        logging.debug(log_msg)\n    elif level.upper() == \"INFO\":\n        logging.info(log_msg)\n    elif level.upper() == \"WARNING\":\n        logging.warning(log_msg)\n    elif level.upper() == \"ERROR\":\n        logging.error(log_msg)\n    else:\n        logging.info(\"Unknown log level specified.\")\n\n# Shortcut functions for different levels\ndef print_log(message, block=None):\n    caller_frame = inspect.currentframe().f_back\n    log_message(\"INFO\", message, block, log_title=\"INFO\", caller_frame=caller_frame)\n\ndef debug_log(message, block=None):\n    caller_frame = inspect.currentframe().f_back\n    log_message(\"DEBUG\", message, block, log_title=\"DEBUG\", caller_frame=caller_frame)\n\ndef warn_log(message, block=None):\n    caller_frame = inspect.currentframe().f_back\n    log_message(\"WARNING\", message, block, log_title=\"WARNING\", caller_frame=caller_frame)\n\ndef error_log(message, block=None):\n    caller_frame = inspect.currentframe().f_back\n    log_message(\"ERROR\", message, block, log_title=\"ERROR\", caller_frame=caller_frame)","metadata":{"papermill":{"duration":0.039811,"end_time":"2024-10-25T16:52:22.378713","exception":false,"start_time":"2024-10-25T16:52:22.338902","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:41:49.736026Z","iopub.execute_input":"2024-10-26T09:41:49.736980Z","iopub.status.idle":"2024-10-26T09:41:49.752093Z","shell.execute_reply.started":"2024-10-26T09:41:49.736926Z","shell.execute_reply":"2024-10-26T09:41:49.751002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The example usage of logs\nprint_log(\"Training started\", block=\"Training Phase\")\ndebug_log(\"Loaded 500 records\", block=\"Data Loading\")\nwarn_log(\"Missing values detected\", block=\"Data Validation\")\nerror_log(\"Failed to save model\", block=\"Model Saving\")","metadata":{"papermill":{"duration":0.031746,"end_time":"2024-10-25T16:52:22.432487","exception":false,"start_time":"2024-10-25T16:52:22.400741","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:41:49.753395Z","iopub.execute_input":"2024-10-26T09:41:49.753728Z","iopub.status.idle":"2024-10-26T09:41:49.769992Z","shell.execute_reply.started":"2024-10-26T09:41:49.753694Z","shell.execute_reply":"2024-10-26T09:41:49.769081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Clear Working Directory","metadata":{"papermill":{"duration":0.022217,"end_time":"2024-10-25T16:52:22.477253","exception":false,"start_time":"2024-10-25T16:52:22.455036","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\nimport shutil\n\n\ndef clear_working_directory():\n    # Define the directory to clear\n    directory_to_clear = '/kaggle/working/'\n\n    # Iterate through all files and directories in the specified directory\n    for filename in os.listdir(directory_to_clear):\n        file_path = os.path.join(directory_to_clear, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)  # Remove the file or symbolic link\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)  # Remove the directory and its contents\n            print_log(f\"Removed: {file_path}\")  # Optional: Print the removed file or directory\n        except Exception as e:\n            error_log(f\"Failed to remove {file_path}. Reason: {e}\")\n","metadata":{"papermill":{"duration":0.032796,"end_time":"2024-10-25T16:52:22.532417","exception":false,"start_time":"2024-10-25T16:52:22.499621","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:41:49.772804Z","iopub.execute_input":"2024-10-26T09:41:49.773798Z","iopub.status.idle":"2024-10-26T09:41:49.781090Z","shell.execute_reply.started":"2024-10-26T09:41:49.773755Z","shell.execute_reply":"2024-10-26T09:41:49.780291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clear_working_directory()","metadata":{"papermill":{"duration":0.032509,"end_time":"2024-10-25T16:52:22.587070","exception":false,"start_time":"2024-10-25T16:52:22.554561","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:41:49.782304Z","iopub.execute_input":"2024-10-26T09:41:49.782609Z","iopub.status.idle":"2024-10-26T09:41:49.794644Z","shell.execute_reply.started":"2024-10-26T09:41:49.782576Z","shell.execute_reply":"2024-10-26T09:41:49.793648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing Data","metadata":{"papermill":{"duration":0.021882,"end_time":"2024-10-25T16:52:22.631605","exception":false,"start_time":"2024-10-25T16:52:22.609723","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#Reading data:\n\ndf = pd.read_csv(\"/kaggle/input/3.Time_domain_subsamples/KU-HAR_time_domain_subsamples_20750x300.csv\",header=None)\ndff = df.values\nsignals = dff[:, 0: 1800] #These are the time-domian subsamples (signals) \nsignals = np.array(signals, dtype=np.float32)\nlabels = dff[:, 1800] #These are their associated class labels (signals)\n\nprint_log(f\"signals shape: {signals.shape} and labels shape: {labels.shape}\")\n","metadata":{"papermill":{"duration":10.535092,"end_time":"2024-10-25T16:52:33.246600","exception":false,"start_time":"2024-10-25T16:52:22.711508","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:41:49.796072Z","iopub.execute_input":"2024-10-26T09:41:49.797090Z","iopub.status.idle":"2024-10-26T09:41:56.897814Z","shell.execute_reply.started":"2024-10-26T09:41:49.797022Z","shell.execute_reply":"2024-10-26T09:41:56.896703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reshape signals into 300 samples per axis for each sensor type (acc, gyro, orientation)\n# For accelerometer data (X, Y, Z axis)\nacc_x = signals[:, :300]\nacc_y = signals[:, 300:600]\nacc_z = signals[:, 600:900]\ngyro_x = signals[:, 900:1200]\ngyro_y = signals[:, 1200:1500]\ngyro_z = signals[:, 1500:1800]\n\n# Create a list to hold all rows of data\ndata_rows = []\n\n# Loop over each activity ID and create rows for each sample and axis\nfor i in range(len(labels)):\n    activity_id = labels[i]\n    \n    # For each of the 300 samples, create a row with each sensor axis value\n    for sample in range(300):\n        data_rows.append({\"activityID\": int(activity_id), \n                          \"acc_x\": acc_x[i, sample],\n                         \"acc_y\" : acc_y[i, sample],\n                         \"acc_z\" : acc_z[i, sample],\n                         \"gyro_x\" : gyro_x[i, sample],\n                         \"gyro_y\" : gyro_y[i, sample],\n                         \"gyro_z\" : gyro_z[i, sample]})\n\n# Create a DataFrame from the expanded data\nnew_df = pd.DataFrame(data_rows)\nprint_log(\"The basic new dataframe is genarated\")","metadata":{"papermill":{"duration":41.223454,"end_time":"2024-10-25T16:53:14.492451","exception":false,"start_time":"2024-10-25T16:52:33.268997","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:41:56.899809Z","iopub.execute_input":"2024-10-26T09:41:56.900328Z","iopub.status.idle":"2024-10-26T09:42:39.496980Z","shell.execute_reply.started":"2024-10-26T09:41:56.900272Z","shell.execute_reply":"2024-10-26T09:42:39.495679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Standardize the dataframe** \nStandardize the record count across all activities for each subject.","metadata":{"papermill":{"duration":0.021737,"end_time":"2024-10-25T16:53:14.536460","exception":false,"start_time":"2024-10-25T16:53:14.514723","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# activity_counts = new_df.groupby('activityID').size()\n\n# # Print the counts for each activityID\n# print(activity_counts)","metadata":{"papermill":{"duration":0.030118,"end_time":"2024-10-25T16:53:14.588884","exception":false,"start_time":"2024-10-25T16:53:14.558766","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:42:39.498430Z","iopub.execute_input":"2024-10-26T09:42:39.498865Z","iopub.status.idle":"2024-10-26T09:42:39.505018Z","shell.execute_reply.started":"2024-10-26T09:42:39.498815Z","shell.execute_reply":"2024-10-26T09:42:39.504113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count the number of records for each activityID\nactivity_counts = new_df.groupby('activityID').size()\n\n# Find the minimum count of records\nmin_records = activity_counts.min()\n\n# Print the minimum count and the corresponding activityID(s)\nmin_activity_ids = activity_counts[activity_counts == min_records].index.tolist()\n\nprint_log(f\"Minimum number of records: {min_records}\")\nprint_log(f\"Activity IDs with minimum records: {min_activity_ids}\")\n","metadata":{"papermill":{"duration":0.141145,"end_time":"2024-10-25T16:53:14.752962","exception":false,"start_time":"2024-10-25T16:53:14.611817","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:42:39.506436Z","iopub.execute_input":"2024-10-26T09:42:39.506856Z","iopub.status.idle":"2024-10-26T09:42:39.614231Z","shell.execute_reply.started":"2024-10-26T09:42:39.506790Z","shell.execute_reply":"2024-10-26T09:42:39.613228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count the number of records for each activityID\nactivity_counts = new_df.groupby('activityID').size()\n\n# Find the minimum count of records\nmin_records = activity_counts.min()\n\n# Initialize a list to store the resampled DataFrames\nequalized_dfs = []\n\n# Iterate through each activityID\nfor activity_id in activity_counts.index:\n    # Filter the DataFrame for the current activityID\n    activity_data = new_df[new_df['activityID'] == activity_id]\n    \n    # Resample the data to the minimum count with replacement\n    resampled_data = activity_data.sample(n=min_records, replace=True, random_state=42)\n    \n    # Append the resampled DataFrame to the list\n    equalized_dfs.append(resampled_data)\n\n# Concatenate all resampled DataFrames into one\nequalized_df = pd.concat(equalized_dfs, ignore_index=True)\nprint_log(\"The dataframe is Standardized\")\n\n# Show the final equalized DataFrame\nprint(equalized_df.groupby('activityID').size())  # Check the counts per activityID\n\n# Reassign the dataframe\nnew_df = equalized_df\nnew_df","metadata":{"papermill":{"duration":0.52007,"end_time":"2024-10-25T16:53:15.295509","exception":false,"start_time":"2024-10-25T16:53:14.775439","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:42:39.619133Z","iopub.execute_input":"2024-10-26T09:42:39.619535Z","iopub.status.idle":"2024-10-26T09:42:40.113001Z","shell.execute_reply.started":"2024-10-26T09:42:39.619482Z","shell.execute_reply":"2024-10-26T09:42:40.112083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize","metadata":{"papermill":{"duration":0.023986,"end_time":"2024-10-25T16:53:15.344031","exception":false,"start_time":"2024-10-25T16:53:15.320045","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def visualize_activity_data(activity_id, df):\n    # Filter data for the specific activity ID\n    activity_data = df[df['activityID'] == activity_id]\n    \n    # Ensure there are enough samples for plotting (300 samples)\n    if len(activity_data) < 300:\n        warn_log(f\"Not enough samples for activity ID {activity_id}\")\n        return\n    \n    # Extract sensor data for the first 300 samples for each axis\n    acc_x = activity_data['acc_x'].values[:300]\n    acc_y = activity_data['acc_y'].values[:300]\n    acc_z = activity_data['acc_z'].values[:300]\n    gyro_x = activity_data['gyro_x'].values[:300]\n    gyro_y = activity_data['gyro_y'].values[:300]\n    gyro_z = activity_data['gyro_z'].values[:300]\n    \n    # Generate time array (assuming each sample is 0.01 seconds apart)\n    time = np.linspace(0.01, 3, 300)\n    \n    # Create figure for acceleration\n    plt.figure(figsize=(10, 6))\n    \n    # Plot accelerometer X, Y, Z axis on the same plot\n    plt.plot(time, acc_x, color='b', label='Accelerometer X')\n    plt.plot(time, acc_y, color='g', label='Accelerometer Y')\n    plt.plot(time, acc_z, color='r', label='Accelerometer Z')\n    \n    plt.title('Accelerometer Data (X, Y, Z)')\n    plt.xlabel('time (s)')\n    plt.ylabel('Acceleration (m/s^2)')\n    plt.grid(True)\n    plt.legend()\n    print_log(\"Accelerometer plot is created\")\n    \n    # Show the first plot\n    plt.tight_layout()\n    plt.show()\n    \n    # Create figure for gyroscope\n    plt.figure(figsize=(10, 6))\n    \n    # Plot gyroscope X, Y, Z axis on the same plot\n    plt.plot(time, gyro_x, color='b', label='Gyroscope X')\n    plt.plot(time, gyro_y, color='g', label='Gyroscope Y')\n    plt.plot(time, gyro_z, color='r', label='Gyroscope Z')\n    \n    plt.title('Gyroscope Data (X, Y, Z)')\n    plt.xlabel('time (s)')\n    plt.ylabel('Angular rotation (rad/s)')\n    plt.grid(True)\n    plt.legend()\n    print_log(\"Gyroscope plot is created\")\n    \n    # Show the second plot\n    plt.tight_layout()\n    plt.show()\n","metadata":{"papermill":{"duration":0.041234,"end_time":"2024-10-25T16:53:15.408391","exception":false,"start_time":"2024-10-25T16:53:15.367157","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:42:40.114631Z","iopub.execute_input":"2024-10-26T09:42:40.115505Z","iopub.status.idle":"2024-10-26T09:42:40.129366Z","shell.execute_reply.started":"2024-10-26T09:42:40.115448Z","shell.execute_reply":"2024-10-26T09:42:40.128294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_activity_data(activity_id=9, df=new_df)","metadata":{"papermill":{"duration":1.091067,"end_time":"2024-10-25T16:53:16.523065","exception":false,"start_time":"2024-10-25T16:53:15.431998","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:42:40.130731Z","iopub.execute_input":"2024-10-26T09:42:40.131124Z","iopub.status.idle":"2024-10-26T09:42:41.559225Z","shell.execute_reply.started":"2024-10-26T09:42:40.131082Z","shell.execute_reply":"2024-10-26T09:42:41.558173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transform the data","metadata":{"papermill":{"duration":0.030186,"end_time":"2024-10-25T16:53:16.583888","exception":false,"start_time":"2024-10-25T16:53:16.553702","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Function to create a DataFrame with original values, derivatives, and Fourier transforms\ndef create_transformed_df(df):\n    transformed_data = {}\n\n    # Include original values for each column, including activityID\n    transformed_data['activityID'] = df['activityID']\n    for col in ['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z']:\n        transformed_data[col] = df[col]  # Add original data\n\n        # Compute first derivative\n        transformed_data[f'{col}_fd'] = np.gradient(df[col])\n        \n        # Compute second derivative\n        transformed_data[f'{col}_sd'] = np.gradient(transformed_data[f'{col}_fd'])\n        \n        # Compute Fourier Transform (absolute values to keep real magnitudes)\n        transformed_data[f'{col}_fourier'] = np.abs(np.fft.fft(df[col]))\n\n    # Create the final DataFrame\n    transformed_df = pd.DataFrame(transformed_data)\n    print_log(\"The first and secind derivatives, and Fourier transforms values of sensor data added in the dataframe\")\n\n    return transformed_df\n","metadata":{"papermill":{"duration":0.040979,"end_time":"2024-10-25T16:53:16.654513","exception":false,"start_time":"2024-10-25T16:53:16.613534","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:42:41.560824Z","iopub.execute_input":"2024-10-26T09:42:41.561270Z","iopub.status.idle":"2024-10-26T09:42:41.570543Z","shell.execute_reply.started":"2024-10-26T09:42:41.561224Z","shell.execute_reply":"2024-10-26T09:42:41.569349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def assign_subject_ids(df):\n    # Get unique activity IDs in the dataset\n    activity_ids = df['activityID'].unique()\n    \n    # Initialize list to hold subject IDs\n    subject_ids = []\n    \n    # Loop over each activity ID\n    for activity_id in activity_ids:\n        # Get subset for the specific activity\n        activity_df = df[df['activityID'] == activity_id]\n        \n        # Number of records for the activity\n        n_records = len(activity_df)\n        \n        # Use round-robin assignment of subject IDs\n        subject_id_repeated = np.tile(np.arange(1, 11), n_records // 10 + 1)[:n_records]\n        \n        # Append these subject IDs to the list\n        subject_ids.extend(subject_id_repeated)\n    \n    # Add subjectID to the DataFrame\n    df['subjectID'] = subject_ids\n    print_log(\"subjectID is assigned for every record\")\n    return df\n\n\ndef assign_subject_ids_random(df):\n    # Get unique activity IDs in the dataset\n    activity_ids = df['activityID'].unique()\n    \n    # Initialize list to hold subject IDs\n    subject_ids = []\n    \n    # Loop over each activity ID\n    for activity_id in activity_ids:\n        # Get subset for the specific activity\n        activity_df = df[df['activityID'] == activity_id]\n        \n        # Number of records for the activity\n        n_records = len(activity_df)\n        \n        # Randomly assign subject IDs from 1 to 10\n        subject_id_random = np.random.choice(np.arange(1, 11), n_records, replace=True)\n        \n        # Append these subject IDs to the list\n        subject_ids.extend(subject_id_random)\n    \n    # Add subjectID to the DataFrame\n    df['subjectID'] = subject_ids\n    print_log(\"Random subjectID is assigned for every record\")\n    return df\n\n","metadata":{"papermill":{"duration":0.042362,"end_time":"2024-10-25T16:53:16.728807","exception":false,"start_time":"2024-10-25T16:53:16.686445","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:42:41.572054Z","iopub.execute_input":"2024-10-26T09:42:41.572418Z","iopub.status.idle":"2024-10-26T09:42:41.588100Z","shell.execute_reply.started":"2024-10-26T09:42:41.572383Z","shell.execute_reply":"2024-10-26T09:42:41.587058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cretae transformed df it include first and second derivative and fourior, Assign the subject IDs from 0 to 10\ntransformed_df = create_transformed_df(new_df)\nfinal_df = assign_subject_ids(transformed_df)","metadata":{"papermill":{"duration":5.758772,"end_time":"2024-10-25T16:53:22.517502","exception":false,"start_time":"2024-10-25T16:53:16.758730","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:42:41.589330Z","iopub.execute_input":"2024-10-26T09:42:41.589699Z","iopub.status.idle":"2024-10-26T09:42:47.288661Z","shell.execute_reply.started":"2024-10-26T09:42:41.589665Z","shell.execute_reply":"2024-10-26T09:42:47.287706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"activity_counts = final_df.groupby('activityID').size()\n\n# Print the counts for each activityID\nprint_log(activity_counts)","metadata":{"papermill":{"duration":0.064077,"end_time":"2024-10-25T16:53:22.612164","exception":false,"start_time":"2024-10-25T16:53:22.548087","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:42:47.289850Z","iopub.execute_input":"2024-10-26T09:42:47.290176Z","iopub.status.idle":"2024-10-26T09:42:47.319324Z","shell.execute_reply.started":"2024-10-26T09:42:47.290143Z","shell.execute_reply":"2024-10-26T09:42:47.318409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_data(df, train_ratio=0.7, test_ratio=0.2, validation_ratio=0.1):\n    # Validate that the ratios sum to 1\n    if not (train_ratio + test_ratio + validation_ratio != 1):\n        error_log(\"Train : Test: Validation => Ratios must sum to 1.\")\n        raise ValueError(\"Ratios must sum to 1.\")\n    \n    # Initialize empty DataFrames for train, test, and validation\n    train_df = pd.DataFrame(columns=df.columns)\n    test_df = pd.DataFrame(columns=df.columns)\n    validation_df = pd.DataFrame(columns=df.columns)\n\n    # Iterate over each group of (activityID, subjectID)\n    for (activity_id, subject_id), group in df.groupby(['activityID', 'subjectID']):\n        n = len(group)\n        \n        if n == 0:\n            warn_log(f\"The group of activity_id : {activity_id} and subject_id : {subject_id} is empty\")\n            continue  # Skip empty groups\n        \n        # Shuffle the group for randomness (if needed)\n        group = group.sample(frac=1).reset_index(drop=True)\n\n        # Calculate indices for splitting\n        train_end = int(train_ratio * n)\n        test_end = train_end + int(test_ratio * n)\n\n        # Split into train, test, validation based on indices\n        train_data = group.iloc[:train_end]\n        test_data = group.iloc[train_end:test_end]\n        validation_data = group.iloc[test_end:]\n\n        # Concatenate only if the DataFrames are not empty\n        train_df = pd.concat([train_df, train_data], ignore_index=True)\n        test_df = pd.concat([test_df, test_data], ignore_index=True)\n        validation_df = pd.concat([validation_df, validation_data], ignore_index=True)\n\n    print_log(\"Data is split into three parts: train, test, and validation.\")\n\n    return train_df, test_df, validation_df\n","metadata":{"papermill":{"duration":0.044813,"end_time":"2024-10-25T16:53:22.688447","exception":false,"start_time":"2024-10-25T16:53:22.643634","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:42:47.320806Z","iopub.execute_input":"2024-10-26T09:42:47.321660Z","iopub.status.idle":"2024-10-26T09:42:47.331865Z","shell.execute_reply.started":"2024-10-26T09:42:47.321612Z","shell.execute_reply":"2024-10-26T09:42:47.330796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df, test_df, validation_df = split_data(final_df)\n\nprint_log(f\"Train data shape : {train_df.shape}\")\nprint_log(f\"Test data shape: {test_df.shape}\")\nprint_log(f\"Validation data shape: {validation_df.shape}\")\n\n# # Define the file paths for saving the CSV files\n# train_csv_path = '/kaggle/working/train_data.csv'\n# test_csv_path = '/kaggle/working/test_data.csv'\n# validation_csv_path = '/kaggle/working/validation_data.csv'\n\n# # Save the DataFrames to CSV files\n# train_df.to_csv(train_csv_path, index=False)\n# print_log(\"train_data.csv file is created\")\n# test_df.to_csv(test_csv_path, index=False)\n# print_log(\"test_data.csv file is created\")\n# validation_df.to_csv(validation_csv_path, index=False)\n# print_log(\"validation_data.csv file is created\")","metadata":{"papermill":{"duration":6.641311,"end_time":"2024-10-25T16:53:29.360253","exception":false,"start_time":"2024-10-25T16:53:22.718942","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:42:47.333263Z","iopub.execute_input":"2024-10-26T09:42:47.333917Z","iopub.status.idle":"2024-10-26T09:42:53.556933Z","shell.execute_reply.started":"2024-10-26T09:42:47.333871Z","shell.execute_reply":"2024-10-26T09:42:53.555728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_df.groupby('activityID').size()","metadata":{"papermill":{"duration":0.037192,"end_time":"2024-10-25T16:53:29.428366","exception":false,"start_time":"2024-10-25T16:53:29.391174","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:42:53.558420Z","iopub.execute_input":"2024-10-26T09:42:53.558750Z","iopub.status.idle":"2024-10-26T09:42:53.563378Z","shell.execute_reply.started":"2024-10-26T09:42:53.558714Z","shell.execute_reply":"2024-10-26T09:42:53.562230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualizing Activity Distribution and Understanding Variability","metadata":{"papermill":{"duration":0.029354,"end_time":"2024-10-25T16:53:29.487599","exception":false,"start_time":"2024-10-25T16:53:29.458245","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Create a DataFrame from your groupby result\nactivity_distribution = train_df.groupby(['activityID', 'subjectID']).size().reset_index(name='count')\n\n# Plotting\nplt.figure(figsize=(12, 6))\nsns.barplot(data=activity_distribution, x='activityID', y='count', hue='subjectID')\nplt.title('Activity Distribution Across Subjects')\nplt.xlabel('Activity ID')\nplt.ylabel('Count')\nplt.legend(title='Subject ID')\nplt.show()","metadata":{"papermill":{"duration":2.122848,"end_time":"2024-10-25T16:53:31.640571","exception":false,"start_time":"2024-10-25T16:53:29.517723","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:42:53.564688Z","iopub.execute_input":"2024-10-26T09:42:53.565003Z","iopub.status.idle":"2024-10-26T09:42:55.707369Z","shell.execute_reply.started":"2024-10-26T09:42:53.564968Z","shell.execute_reply":"2024-10-26T09:42:55.706123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"activity_variability = train_df.groupby('activityID')['subjectID'].value_counts().unstack()\nvariability_std = activity_variability.std(axis=1)\nprint_log(f\" activity variability : {variability_std} \")\n","metadata":{"papermill":{"duration":0.34519,"end_time":"2024-10-25T16:53:32.017797","exception":false,"start_time":"2024-10-25T16:53:31.672607","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:42:55.709113Z","iopub.execute_input":"2024-10-26T09:42:55.709478Z","iopub.status.idle":"2024-10-26T09:42:56.016576Z","shell.execute_reply.started":"2024-10-26T09:42:55.709444Z","shell.execute_reply":"2024-10-26T09:42:56.015478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_sequence_list(df, sequence_length=500, overlap=20):\n    # Initialize the main list to hold all sequences for all activities\n    imu_data_sequence = []\n\n    # Group by activityID to handle each activity separately\n    grouped_by_activity = df.groupby('activityID')\n\n    # Iterate over each activity group\n    for activity_id, activity_group in grouped_by_activity:\n        # Initialize a list for this activity\n        activity_data_sequence = []\n\n        # Group by subjectID within the activity group\n        grouped_by_subject = activity_group.groupby('subjectID')\n        \n        # Iterate over each subject group within the activity\n        for subject_id, subject_group in grouped_by_subject:\n            # Extract the data values excluding activityID and subjectID\n            data_values = subject_group.drop(columns=['activityID', 'subjectID']).values\n            \n            # Calculate the number of sequences\n            num_samples = len(data_values)\n            num_sequences = (num_samples - sequence_length) // overlap + 1\n            \n            # Create a list for this subject's sequences\n            subject_sequences = []\n            \n            # Generate sequences for this subject\n            for i in range(num_sequences):\n                sequence_start = i * overlap\n                sequence_end = sequence_start + sequence_length\n                if sequence_end <= num_samples:\n                    # Append the sequence to the subject's sequence list\n                    subject_sequences.append(data_values[sequence_start:sequence_end])\n            \n            # Add the subject sequences to the activity's list\n            activity_data_sequence.append(subject_sequences)\n            debug_log(f\"The sequences are generated for subjectID: {subject_id}\")\n        \n        # After processing all subjects, print activity completion\n        print_log(f\"The sequences are generated for activityID: {activity_id}\")\n\n        # Add the activity data sequence to the main list\n        imu_data_sequence.append(activity_data_sequence)\n\n    return imu_data_sequence\n","metadata":{"papermill":{"duration":0.044125,"end_time":"2024-10-25T16:53:32.095277","exception":false,"start_time":"2024-10-25T16:53:32.051152","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:42:56.018074Z","iopub.execute_input":"2024-10-26T09:42:56.018431Z","iopub.status.idle":"2024-10-26T09:42:56.028672Z","shell.execute_reply.started":"2024-10-26T09:42:56.018394Z","shell.execute_reply":"2024-10-26T09:42:56.027446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate sequences for each DataFrame\nimu_data_sequence_train = generate_sequence_list(train_df, SEQ_LENTH, WINDOW_SIZE)\nimu_data_sequence_test = generate_sequence_list(test_df, SEQ_LENTH, WINDOW_SIZE)\nimu_data_sequence_validation = generate_sequence_list(validation_df, SEQ_LENTH, WINDOW_SIZE)","metadata":{"papermill":{"duration":1.54724,"end_time":"2024-10-25T16:53:33.673405","exception":false,"start_time":"2024-10-25T16:53:32.126165","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:42:56.029835Z","iopub.execute_input":"2024-10-26T09:42:56.030160Z","iopub.status.idle":"2024-10-26T09:42:57.496445Z","shell.execute_reply.started":"2024-10-26T09:42:56.030125Z","shell.execute_reply":"2024-10-26T09:42:57.495364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(imu_data_sequence_train[1][3])","metadata":{"papermill":{"duration":0.04328,"end_time":"2024-10-25T16:53:33.750693","exception":false,"start_time":"2024-10-25T16:53:33.707413","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:42:57.497992Z","iopub.execute_input":"2024-10-26T09:42:57.498434Z","iopub.status.idle":"2024-10-26T09:42:57.505762Z","shell.execute_reply.started":"2024-10-26T09:42:57.498394Z","shell.execute_reply":"2024-10-26T09:42:57.504660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Store data as a pickle","metadata":{"papermill":{"duration":0.034271,"end_time":"2024-10-25T16:53:33.819669","exception":false,"start_time":"2024-10-25T16:53:33.785398","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# # # Store the imu data as a pickle\n# pickle_imu_data_directory = \"/kaggle/working/pickle_imu_data\"\n\n# # # Create the directory if it doesn't exist\n# os.makedirs(pickle_imu_data_directory, exist_ok=True)\n\n# # # File paths for pickle files\n# # train_pickle_path = os.path.join(pickle_imu_data_directory, 'imu_data_sequence_train.pkl')\n# test_pickle_path = os.path.join(pickle_imu_data_directory, 'imu_data_sequence_test.pkl')\n# # validation_pickle_path = os.path.join(pickle_imu_data_directory, 'imu_data_sequence_validation.pkl')\n\n# # # Save the training data sequence to a pickle file\n# # with open(train_pickle_path, 'wb') as train_file:\n# #     pickle.dump(imu_data_sequence_train[0:8], train_file)\n# # print_log(\"Train Pickle file has been saved successfully\")\n\n# # # Save the testing data sequence to a pickle file\n# with open(test_pickle_path, 'wb') as test_file:\n#     pickle.dump(imu_data_sequence_test[0:8], test_file)\n# print_log(\"Test Pickle file has been saved successfully\")\n\n# # # Save the validation data sequence to a pickle file\n# # with open(validation_pickle_path, 'wb') as validation_file:\n# #     pickle.dump(imu_data_sequence_validation[0:8], validation_file)\n# # print_log(\"Validation Pickle file has been saved successfully\")\n\n# # print(\"Pickle files have been saved successfully.\")\n\n","metadata":{"papermill":{"duration":0.043613,"end_time":"2024-10-25T16:53:33.897725","exception":false,"start_time":"2024-10-25T16:53:33.854112","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:42:57.507458Z","iopub.execute_input":"2024-10-26T09:42:57.507901Z","iopub.status.idle":"2024-10-26T09:42:57.519720Z","shell.execute_reply.started":"2024-10-26T09:42:57.507848Z","shell.execute_reply":"2024-10-26T09:42:57.518711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{"papermill":{"duration":0.033564,"end_time":"2024-10-25T16:53:33.965999","exception":false,"start_time":"2024-10-25T16:53:33.932435","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, k, d_model, seq_len):\n        super().__init__()\n        \n        self.embedding = nn.Parameter(torch.zeros([k, d_model], dtype=torch.float), requires_grad=True)\n        nn.init.xavier_uniform_(self.embedding, gain=1)\n        self.positions = torch.tensor([i for i in range(seq_len)], requires_grad=False).unsqueeze(1).repeat(1, k)\n        s = 0.0\n        interval = seq_len / k\n        mu = []\n        for _ in range(k):\n            mu.append(nn.Parameter(torch.tensor(s, dtype=torch.float), requires_grad=True))\n            s = s + interval\n        self.mu = nn.Parameter(torch.tensor(mu, dtype=torch.float).unsqueeze(0), requires_grad=True)\n        self.sigma = nn.Parameter(torch.tensor([torch.tensor([50.0], dtype=torch.float, requires_grad=True) for _ in range(k)]).unsqueeze(0))\n        \n    def normal_pdf(self, pos, mu, sigma):\n        a = pos - mu\n        log_p = -1*torch.mul(a, a)/(2*(sigma**2)) - torch.log(sigma)\n        return torch.nn.functional.softmax(log_p, dim=1)\n\n    def forward(self, inputs):\n        pdfs = self.normal_pdf(self.positions, self.mu, self.sigma)\n        pos_enc = torch.matmul(pdfs, self.embedding)\n        \n        return inputs + pos_enc.unsqueeze(0).repeat(inputs.size(0), 1, 1)\n\nclass TransformerEncoderLayer(nn.Module):\n    def __init__(self, d_model, heads, _heads, dropout, seq_len):\n        super(TransformerEncoderLayer, self).__init__()\n        \n        self.attention = nn.MultiheadAttention(d_model, heads, batch_first=True)\n        self._attention = nn.MultiheadAttention(seq_len, _heads, batch_first=True)\n        \n        self.attn_norm = nn.LayerNorm(d_model)\n        \n        self.cnn_units = 1\n        \n        self.cnn = nn.Sequential(\n            nn.Conv2d(1, self.cnn_units, (1, 1)),\n            nn.BatchNorm2d(self.cnn_units),\n            nn.Dropout(dropout),\n            nn.ReLU(),\n            nn.Conv2d(self.cnn_units, self.cnn_units, (3, 3), padding=1),\n            nn.BatchNorm2d(self.cnn_units),\n            nn.Dropout(dropout),\n            nn.ReLU(),\n            nn.Conv2d(self.cnn_units, 1, (5, 5), padding=2),\n            nn.BatchNorm2d(1),\n            nn.Dropout(dropout),\n            nn.ReLU()\n        )\n        \n        self.final_norm = nn.LayerNorm(d_model)\n\n    def forward(self, src, src_mask=None):\n        src = self.attn_norm(src + self.attention(src, src, src)[0] + self._attention(src.transpose(-1, -2), src.transpose(-1, -2), src.transpose(-1, -2))[0].transpose(-1, -2))\n        \n        src = self.final_norm(src + self.cnn(src.unsqueeze(dim=1)).squeeze(dim=1))\n            \n        return src\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, d_model, heads, _heads, seq_len, num_layer=2, dropout=0.1):\n        super(TransformerEncoder, self).__init__()\n\n        self.layers = nn.ModuleList()\n        for i in range(num_layer):\n            self.layers.append(TransformerEncoderLayer(d_model, heads, _heads, dropout, seq_len))\n\n    def forward(self, src):\n        for layer in self.layers:\n            src = layer(src)\n\n        return src\n\nclass Transformer(nn.Module):\n    def __init__(self, num_layer, d_model, k, heads, _heads, seq_len, trg_len, dropout):\n        super(Transformer, self).__init__()\n\n        self.pos_encoding = PositionalEncoding(k, d_model, seq_len)\n\n        self.encoder = TransformerEncoder(d_model, heads, _heads, seq_len, num_layer, dropout)\n\n    def forward(self, inputs):\n        encoded_inputs = self.pos_encoding(inputs)\n\n        return self.encoder(encoded_inputs)\n\nclass Model(nn.Module):\n    def __init__(self, feature_count, l, trg_len, num_classes):\n        super(Model, self).__init__()\n        \n        self.imu_transformer = Transformer(5, feature_count, 100, 4, 4, l, trg_len, 0.1)\n        \n        self.linear_imu = nn.Sequential(\n            nn.Linear(feature_count*l, (feature_count*l)//2),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear((feature_count*l)//2, trg_len),\n            nn.ReLU()\n        )\n        \n        # Batch normalization and dropout layers\n        self.batch_norm = nn.BatchNorm1d(trg_len)\n        self.dropout = nn.Dropout(0.5)\n        \n        # Classifier layer\n        self.classifier = nn.Linear(trg_len, num_classes)\n\n    def forward(self, inputs):\n        \n        embedding = self.linear_imu(torch.flatten(self.imu_transformer(inputs), start_dim=1, end_dim=2))\n        \n        # Apply batch normalization\n        embedding = self.batch_norm(embedding)\n        \n        # Apply dropout\n        embedding = self.dropout(embedding)\n        \n        # Get class scores\n        class_scores = self.classifier(embedding)\n        \n        return class_scores, embedding","metadata":{"papermill":{"duration":0.068012,"end_time":"2024-10-25T16:53:34.066740","exception":false,"start_time":"2024-10-25T16:53:33.998728","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:42:57.521533Z","iopub.execute_input":"2024-10-26T09:42:57.521915Z","iopub.status.idle":"2024-10-26T09:42:57.554464Z","shell.execute_reply.started":"2024-10-26T09:42:57.521875Z","shell.execute_reply":"2024-10-26T09:42:57.553321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# If you have GPU","metadata":{"papermill":{"duration":0.03319,"end_time":"2024-10-25T16:53:34.133584","exception":false,"start_time":"2024-10-25T16:53:34.100394","status":"completed"},"tags":[]}},{"cell_type":"code","source":"torch.set_default_tensor_type('torch.cuda.FloatTensor')","metadata":{"papermill":{"duration":0.045144,"end_time":"2024-10-25T16:53:34.212143","exception":false,"start_time":"2024-10-25T16:53:34.166999","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:42:57.555873Z","iopub.execute_input":"2024-10-26T09:42:57.556302Z","iopub.status.idle":"2024-10-26T09:42:57.567714Z","shell.execute_reply.started":"2024-10-26T09:42:57.556258Z","shell.execute_reply":"2024-10-26T09:42:57.566627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size=BATCH_SIZE\nepoch_batch_count=EPOCH_BATCH_COUNT\nimu_l=SEQ_LENTH    #sequence length\nimu_feature_count=IMU_FEATURE_COUNT\ntrg_len=128 # this will be the size of feature embedding\nclasses=CLASSES","metadata":{"papermill":{"duration":0.042738,"end_time":"2024-10-25T16:53:34.292228","exception":false,"start_time":"2024-10-25T16:53:34.249490","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:42:57.576088Z","iopub.execute_input":"2024-10-26T09:42:57.576499Z","iopub.status.idle":"2024-10-26T09:42:57.582811Z","shell.execute_reply.started":"2024-10-26T09:42:57.576461Z","shell.execute_reply":"2024-10-26T09:42:57.581796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model_save_path = '/kaggle/working/best_models'\ncheckpoint_save_path = '/kaggle/working/checkpoints'\n\nsubprocess.run(f\"mkdir {best_model_save_path}\", shell=True)\nsubprocess.run(f\"mkdir {checkpoint_save_path}\", shell=True)","metadata":{"papermill":{"duration":0.052428,"end_time":"2024-10-25T16:53:34.378156","exception":false,"start_time":"2024-10-25T16:53:34.325728","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:42:57.584018Z","iopub.execute_input":"2024-10-26T09:42:57.584387Z","iopub.status.idle":"2024-10-26T09:42:57.605440Z","shell.execute_reply.started":"2024-10-26T09:42:57.584351Z","shell.execute_reply":"2024-10-26T09:42:57.604322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Dataset","metadata":{"papermill":{"duration":0.041967,"end_time":"2024-10-25T16:53:34.467186","exception":false,"start_time":"2024-10-25T16:53:34.425219","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# class TrainDataset(Dataset):\n#     def __init__(self, training_data, batch_size, epoch_batch_count):\n#         self.training_data = training_data\n#         self.batch_size = batch_size\n#         self.epoch_batch_count = epoch_batch_count\n\n#     def __len__(self):\n#         return self.batch_size * self.epoch_batch_count\n\n#     def __getitem__(self, idx):\n#         while True:\n#             try:\n#                 genuine_user_idx = np.random.randint(0, len(self.training_data))\n#                 imposter_user_idx = np.random.randint(0, len(self.training_data))\n                \n#                 # Ensure imposter_user_idx is different from genuine_user_idx\n#                 while imposter_user_idx == genuine_user_idx:\n#                     imposter_user_idx = np.random.randint(0, len(self.training_data))\n                \n#                 # Validate the lengths of genuine_user and imposter_user data\n#                 if len(self.training_data[genuine_user_idx]) == 0 or len(self.training_data[imposter_user_idx]) == 0:\n#                     raise ValueError(\"Empty user data detected.\")\n                \n#                 genuine_sess_1 = np.random.randint(0, len(self.training_data[genuine_user_idx]))\n#                 genuine_sess_2 = np.random.randint(0, len(self.training_data[genuine_user_idx]))\n                \n#                 # Ensure genuine_sess_2 is different from genuine_sess_1\n#                 while genuine_sess_2 == genuine_sess_1:\n#                     genuine_sess_2 = np.random.randint(0, len(self.training_data[genuine_user_idx]))\n                \n#                 # Validate the lengths of genuine_sess_1 and genuine_sess_2 data\n#                 if len(self.training_data[genuine_user_idx][genuine_sess_1]) == 0 or len(self.training_data[genuine_user_idx][genuine_sess_2]) == 0:\n#                     raise ValueError(\"Empty session data detected.\")\n                \n#                 imposter_sess = np.random.randint(0, len(self.training_data[imposter_user_idx]))\n                \n#                 # Validate the length of imposter_sess data\n#                 if len(self.training_data[imposter_user_idx][imposter_sess]) == 0:\n#                     raise ValueError(\"Empty imposter session data detected.\")\n                \n#                 genuine_seq_1 = np.random.randint(0, len(self.training_data[genuine_user_idx][genuine_sess_1]))\n#                 genuine_seq_2 = np.random.randint(0, len(self.training_data[genuine_user_idx][genuine_sess_2]))\n#                 imposter_seq = np.random.randint(0, len(self.training_data[imposter_user_idx][imposter_sess]))\n# #                 debug_log(f\"{genuine_user_idx}, {genuine_sess_1}, {genuine_sess_2}, {imposter_user_idx}, {imposter_sess}\")\n#                 anchor = self.training_data[genuine_user_idx][genuine_sess_1][genuine_seq_1]\n#                 positive = self.training_data[genuine_user_idx][genuine_sess_2][genuine_seq_2]\n#                 negative = self.training_data[imposter_user_idx][imposter_sess][imposter_seq]\n\n#                 return anchor, positive, negative, genuine_user_idx, imposter_user_idx\n            \n#             except ValueError as e:\n#                 error_log(f\"Encountered ValueError: {str(e)}. Retrying with new indices.\")","metadata":{"papermill":{"duration":0.051219,"end_time":"2024-10-25T16:53:34.552015","exception":false,"start_time":"2024-10-25T16:53:34.500796","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:42:57.606725Z","iopub.execute_input":"2024-10-26T09:42:57.607024Z","iopub.status.idle":"2024-10-26T09:42:57.614303Z","shell.execute_reply.started":"2024-10-26T09:42:57.606992Z","shell.execute_reply":"2024-10-26T09:42:57.613291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainDataset(Dataset):\n    def __init__(self, training_data, batch_size, epoch_batch_count, num_negatives):\n        self.training_data = training_data\n        self.batch_size = batch_size\n        self.epoch_batch_count = epoch_batch_count\n        self.num_negatives = num_negatives\n\n    def __len__(self):\n        return self.batch_size * self.epoch_batch_count\n\n    def __getitem__(self, idx):\n        while True:\n            try:\n                genuine_user_idx = np.random.randint(0, len(self.training_data))\n                imposter_user_idxs = []\n                # Validate the lengths of genuine_user and imposter_user data\n                if len(self.training_data[genuine_user_idx]) == 0:\n                    raise ValueError(\"Empty user data detected.\")\n                \n                for i in range(0, self.num_negatives):\n                    imposter_user_idx =  np.random.randint(0, len(self.training_data))\n                \n                    # Ensure imposter_user_idx is different from genuine_user_idx and diffrent form other imposter_user_idx\n                    while imposter_user_idx == genuine_user_idx or imposter_user_idx in imposter_user_idxs:\n                        imposter_user_idx = np.random.randint(0, len(self.training_data))\n                \n                    # Validate the lengths of genuine_user and imposter_user data\n                    if len(self.training_data[imposter_user_idx]) == 0:\n                        raise ValueError(\"Empty user data detected.\")\n                    \n                    imposter_user_idxs.append(imposter_user_idx)\n                \n                \n                genuine_sess_1 = np.random.randint(0, len(self.training_data[genuine_user_idx]))\n                genuine_sess_2 = np.random.randint(0, len(self.training_data[genuine_user_idx]))\n                \n                # Ensure genuine_sess_2 is different from genuine_sess_1\n                while genuine_sess_2 == genuine_sess_1:\n                    genuine_sess_2 = np.random.randint(0, len(self.training_data[genuine_user_idx]))\n                \n                # Validate the lengths of genuine_sess_1 and genuine_sess_2 data\n                if len(self.training_data[genuine_user_idx][genuine_sess_1]) == 0 or len(self.training_data[genuine_user_idx][genuine_sess_2]) == 0:\n                    raise ValueError(\"Empty session data detected.\")\n                \n                \n                genuine_seq_1 = np.random.randint(0, len(self.training_data[genuine_user_idx][genuine_sess_1]))\n                genuine_seq_2 = np.random.randint(0, len(self.training_data[genuine_user_idx][genuine_sess_2]))\n                \n                anchor = self.training_data[genuine_user_idx][genuine_sess_1][genuine_seq_1]\n                positive = self.training_data[genuine_user_idx][genuine_sess_2][genuine_seq_2]\n                negatives = []\n                \n                for i in imposter_user_idxs:\n                    imposter_sess = np.random.randint(0, len(self.training_data[imposter_user_idx]))\n                \n                    # Validate the length of imposter_sess data\n                    if len(self.training_data[imposter_user_idx][imposter_sess]) == 0:\n                        raise ValueError(\"Empty imposter session data detected.\")\n                \n                \n                    imposter_seq = np.random.randint(0, len(self.training_data[imposter_user_idx][imposter_sess]))\n                    negative = self.training_data[imposter_user_idx][imposter_sess][imposter_seq]\n                    negatives.append(negative)\n\n                return anchor, positive, negatives, genuine_user_idx, imposter_user_idxs\n            \n            except ValueError as e:\n                error_log(f\"Encountered ValueError: {str(e)}. Retrying with new indices.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-26T09:42:57.615857Z","iopub.execute_input":"2024-10-26T09:42:57.616342Z","iopub.status.idle":"2024-10-26T09:42:57.635259Z","shell.execute_reply.started":"2024-10-26T09:42:57.616287Z","shell.execute_reply":"2024-10-26T09:42:57.634165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss","metadata":{"papermill":{"duration":0.033253,"end_time":"2024-10-25T16:53:34.620411","exception":false,"start_time":"2024-10-25T16:53:34.587158","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class TripletLoss(nn.Module):\n    def __init__(self, margin=1.0):\n        super(TripletLoss, self).__init__()\n        self.margin = margin\n        \n    def calc_euclidean(self, x1, x2):\n        return (x1 - x2).pow(2).sum(dim=1).sqrt()\n    \n    def calc_cosine(self, x1, x2):\n        dot_product_sum = (x1*x2).sum(dim=1)\n        norm_multiply = (x1.pow(2).sum(dim=1).sqrt()) * (x2.pow(2).sum(dim=1).sqrt())\n        return dot_product_sum / norm_multiply\n    \n    def calc_manhattan(self, x1, x2):\n        return (x1-x2).abs().sum(dim=1)\n    \n    def forward(self, anchor, positive, negative):\n        distance_positive = self.calc_euclidean(anchor, positive)\n        distance_negative = self.calc_euclidean(anchor, negative)\n        losses = torch.relu(distance_positive - distance_negative + self.margin)\n        return losses.mean()","metadata":{"papermill":{"duration":0.044337,"end_time":"2024-10-25T16:53:34.697605","exception":false,"start_time":"2024-10-25T16:53:34.653268","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:42:57.636750Z","iopub.execute_input":"2024-10-26T09:42:57.637252Z","iopub.status.idle":"2024-10-26T09:42:57.648255Z","shell.execute_reply.started":"2024-10-26T09:42:57.637176Z","shell.execute_reply":"2024-10-26T09:42:57.647275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n\n# class NPairLoss(nn.Module):\n#     def __init__(self):\n#         super(NPairLoss, self).__init__()\n\n#     def calc_cosine(self, x1, x2):\n#         dot_product_sum = (x1 * x2).sum(dim=2)  # Change to dim=2 since x2 is now [batch_size, num_negatives, embedding_dim]\n#         norm_multiply = (x1.pow(2).sum(dim=2).sqrt()) * (x2.pow(2).sum(dim=2).sqrt())\n#         return dot_product_sum / norm_multiply\n\n#     def forward(self, anchor, positive, negatives):\n#         # Calculate cosine similarities for the positive pair\n#         pos_sim = self.calc_cosine(anchor.unsqueeze(1), positive.unsqueeze(1))  # Shape: (8, 1)\n        \n#         # Calculate cosine similarities for each negative\n#         neg_sim = self.calc_cosine(anchor.unsqueeze(1), negatives.permute(1, 0, 2))  # Shape: (8, 4)\n\n#         # Exponential of the similarities\n#         pos_exp = torch.exp(pos_sim.squeeze(1))  # Shape: (8,)\n#         neg_exp = torch.exp(neg_sim)  # Shape: (8, 4)\n\n#         # Compute loss\n#         numerator = pos_exp  # Shape: (8,)\n#         denominator = pos_exp.unsqueeze(1) + neg_exp.sum(dim=1)  # Shape: (8, 1) + (8, 4) -> (8, 4)\n\n#         # Calculate loss\n#         loss = -torch.log(numerator / denominator)\n#         return loss.mean()  # Average loss over the batch\n\n# # Example usage:\n# batch_size = 8\n# embedding_dim = 128\n# num_negatives = 4\n\n# # Example embeddings\n# anchor_features = torch.randn(batch_size, embedding_dim)  # Shape: (8, 128)\n# positive_features = torch.randn(batch_size, embedding_dim)  # Shape: (8, 128)\n# negative_features = torch.randn(num_negatives, batch_size, embedding_dim)  # Shape: (4, 8, 128)\n\n# # Debug prints to check sizes\n# print(f\"Anchor features shape: {anchor_features.shape}\")\n# print(f\"Positive features shape: {positive_features.shape}\")\n# print(f\"Negative features shape: {negative_features.shape}\")\n        \n\n# # Initialize loss function\n# loss_fn = NPairLoss()\n# loss = loss_fn(anchor_features, positive_features, negative_features)\n\n# print(\"Loss:\", loss.item())\n","metadata":{"execution":{"iopub.status.busy":"2024-10-26T09:42:57.649772Z","iopub.execute_input":"2024-10-26T09:42:57.650215Z","iopub.status.idle":"2024-10-26T09:42:57.664313Z","shell.execute_reply.started":"2024-10-26T09:42:57.650168Z","shell.execute_reply":"2024-10-26T09:42:57.663369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass NPairLoss(nn.Module):\n    def __init__(self):\n        super(NPairLoss, self).__init__()\n\n    def calc_cosine(self, x1, x2):\n        dot_product_sum = (x1 * x2).sum(dim=2)  # Change to dim=2 since x2 is now [batch_size, num_negatives, embedding_dim]\n        norm_multiply = (x1.pow(2).sum(dim=2).sqrt()) * (x2.pow(2).sum(dim=2).sqrt())\n        return dot_product_sum / norm_multiply\n\n    def forward(self, anchor, positive, negatives):\n        # Calculate cosine similarities for the positive pair\n        pos_sim = self.calc_cosine(anchor.unsqueeze(1), positive.unsqueeze(1))  # Shape: (8, 1)\n        \n        # Calculate cosine similarities for each negative\n        neg_sim = self.calc_cosine(anchor.unsqueeze(1), negatives.permute(1, 0, 2))  # Shape: (8, 4)\n\n        # Exponential of the similarities\n        pos_exp = torch.exp(pos_sim.squeeze(1))  # Shape: (8,)\n        neg_exp = torch.exp(neg_sim)  # Shape: (8, 4)\n\n        # Compute loss\n        numerator = pos_exp  # Shape: (8,)\n        denominator = pos_exp.unsqueeze(1) + neg_exp.sum(dim=1)  # Shape: (8, 1) + (8, 4) -> (8, 4)\n\n        # Calculate loss\n        loss = -torch.log(numerator / denominator)\n        return loss.mean()  # Average loss over the batch","metadata":{"papermill":{"duration":0.042755,"end_time":"2024-10-25T16:53:34.773452","exception":false,"start_time":"2024-10-25T16:53:34.730697","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:42:57.665397Z","iopub.execute_input":"2024-10-26T09:42:57.665755Z","iopub.status.idle":"2024-10-26T09:42:57.679520Z","shell.execute_reply.started":"2024-10-26T09:42:57.665720Z","shell.execute_reply":"2024-10-26T09:42:57.678362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Set the classes","metadata":{"papermill":{"duration":0.033573,"end_time":"2024-10-25T16:53:34.840754","exception":false,"start_time":"2024-10-25T16:53:34.807181","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_data = imu_data_sequence_train[0:CLASSES]\ntest_data = imu_data_sequence_test[0:CLASSES]\nvalidation_data = imu_data_sequence_validation[0:CLASSES]","metadata":{"papermill":{"duration":0.041655,"end_time":"2024-10-25T16:53:34.916309","exception":false,"start_time":"2024-10-25T16:53:34.874654","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:42:57.680646Z","iopub.execute_input":"2024-10-26T09:42:57.680957Z","iopub.status.idle":"2024-10-26T09:42:57.692693Z","shell.execute_reply.started":"2024-10-26T09:42:57.680922Z","shell.execute_reply":"2024-10-26T09:42:57.691731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# len(train_data[17][9][88])\n# train_data[17][9][88][999]","metadata":{"papermill":{"duration":0.040107,"end_time":"2024-10-25T16:53:34.989852","exception":false,"start_time":"2024-10-25T16:53:34.949745","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:42:57.693862Z","iopub.execute_input":"2024-10-26T09:42:57.694233Z","iopub.status.idle":"2024-10-26T09:42:57.704198Z","shell.execute_reply.started":"2024-10-26T09:42:57.694194Z","shell.execute_reply":"2024-10-26T09:42:57.703177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using only some activity classes for improve our model accuracy","metadata":{"papermill":{"duration":0.031657,"end_time":"2024-10-25T16:53:35.054692","exception":false,"start_time":"2024-10-25T16:53:35.023035","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# input_classes = [0,1,2,4, 6, 7]\n# train_data = [imu_data_sequence_train[i] for i in input_classes] \n# test_data = [imu_data_sequence_test[i] for i in input_classes]  \n# validation_data = [imu_data_sequence_validation[i] for i in input_classes] ","metadata":{"papermill":{"duration":0.04186,"end_time":"2024-10-25T16:53:35.129537","exception":false,"start_time":"2024-10-25T16:53:35.087677","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:42:57.705446Z","iopub.execute_input":"2024-10-26T09:42:57.705823Z","iopub.status.idle":"2024-10-26T09:42:57.714880Z","shell.execute_reply.started":"2024-10-26T09:42:57.705779Z","shell.execute_reply":"2024-10-26T09:42:57.713926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataset = TrainDataset(train_data, batch_size, epoch_batch_count)\n# dataloader = DataLoader(dataset, batch_size=batch_size)\nnum_negatives = NUM_NEGATIVES  # Number of negatives to sample for each anchor\n\n# Instantiate dataset and dataloader\ntrain_dataset = TrainDataset(train_data, batch_size, epoch_batch_count, num_negatives)\n\ndataloader = DataLoader(train_dataset, batch_size=batch_size)\n\n\nmodel = Model(imu_feature_count, imu_l, trg_len,classes)\n","metadata":{"papermill":{"duration":0.367414,"end_time":"2024-10-25T16:53:35.529626","exception":false,"start_time":"2024-10-25T16:53:35.162212","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:42:57.715943Z","iopub.execute_input":"2024-10-26T09:42:57.716309Z","iopub.status.idle":"2024-10-26T09:42:57.766085Z","shell.execute_reply.started":"2024-10-26T09:42:57.716273Z","shell.execute_reply":"2024-10-26T09:42:57.764946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_fn = NPairLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE,weight_decay=1e-5) # change the learning rate\n# optimizer = optim.Adam(model.parameters(), lr=0.0005)","metadata":{"papermill":{"duration":1.546992,"end_time":"2024-10-25T16:53:37.110148","exception":false,"start_time":"2024-10-25T16:53:35.563156","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:42:57.767618Z","iopub.execute_input":"2024-10-26T09:42:57.767996Z","iopub.status.idle":"2024-10-26T09:42:57.777114Z","shell.execute_reply.started":"2024-10-26T09:42:57.767955Z","shell.execute_reply":"2024-10-26T09:42:57.775485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g_eer = math.inf\ninit_epoch = 0\nepochs=EPOCHS #increase the epochs","metadata":{"papermill":{"duration":0.08433,"end_time":"2024-10-25T16:53:37.229805","exception":false,"start_time":"2024-10-25T16:53:37.145475","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:42:57.779198Z","iopub.execute_input":"2024-10-26T09:42:57.779663Z","iopub.status.idle":"2024-10-26T09:42:57.785231Z","shell.execute_reply.started":"2024-10-26T09:42:57.779607Z","shell.execute_reply":"2024-10-26T09:42:57.784133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use('seaborn-v0_8-bright')\nplt.rcParams['axes.facecolor'] = 'white'\nmpl.rcParams.update({\"axes.grid\" : True, \"grid.color\": \"black\"})\nmpl.rc('axes',edgecolor='black')\nmpl.rcParams.update({'font.size': 13})","metadata":{"papermill":{"duration":0.040794,"end_time":"2024-10-25T16:53:37.303948","exception":false,"start_time":"2024-10-25T16:53:37.263154","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:42:57.786818Z","iopub.execute_input":"2024-10-26T09:42:57.787232Z","iopub.status.idle":"2024-10-26T09:42:57.796647Z","shell.execute_reply.started":"2024-10-26T09:42:57.787191Z","shell.execute_reply":"2024-10-26T09:42:57.795464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test Dataset","metadata":{"papermill":{"duration":0.03067,"end_time":"2024-10-25T16:53:37.366025","exception":false,"start_time":"2024-10-25T16:53:37.335355","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# class TestDataset(Dataset):\n#     def __init__(self, eval_data):\n#         self.eval_data = eval_data\n#         self.num_sessions = len(self.eval_data[0])\n#         self.num_seqs = len(self.eval_data[0][0])\n\n#     def __len__(self):\n#         return math.ceil(len(self.eval_data) * self.num_sessions * self.num_seqs)\n\n#     def __getitem__(self, idx):\n#         t_session = idx // self.num_seqs\n#         user_idx = t_session // self.num_sessions\n#         session_idx = t_session % self.num_sessions\n#         seq_idx = idx % self.num_seqs\n        \n#         # Debugging statements\n#         debug_log(f\"Index: {idx}, User Index: {user_idx}, Session Index: {session_idx}, Sequence Index: {seq_idx}\")\n        \n#         # Ensure that indices are within valid range\n#         if user_idx < len(self.eval_data) and session_idx < len(self.eval_data[user_idx]) and seq_idx < len(self.eval_data[user_idx][session_idx]):\n            \n#             debug_log(f\"The length {len(self.eval_data[user_idx][session_idx][seq_idx])} \")\n#             debug_log(f\"test ,{user_idx}, {session_idx},{ seq_idx}\")\n#             data = self.eval_data[user_idx][session_idx][seq_idx]\n\n#             # Debugging statement to check the returned data\n#             if data is None:\n#                 error_log(f\"Returned data is None for index: {idx} in testdata\")\n\n#             return data,user_idx\n#         else:\n#             pass\n","metadata":{"papermill":{"duration":0.040555,"end_time":"2024-10-25T16:53:37.439832","exception":false,"start_time":"2024-10-25T16:53:37.399277","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:42:57.797896Z","iopub.execute_input":"2024-10-26T09:42:57.798347Z","iopub.status.idle":"2024-10-26T09:42:57.807768Z","shell.execute_reply.started":"2024-10-26T09:42:57.798267Z","shell.execute_reply":"2024-10-26T09:42:57.806789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The lenth of sequence calculate dynamically\nclass TestDataset(Dataset):\n    def __init__(self, eval_data):\n        self.eval_data = eval_data\n        self.num_sessions = [len(user_sessions) for user_sessions in self.eval_data]  # List of number of sessions for each user\n        self.num_seqs = [len(session) for user_sessions in self.eval_data for session in user_sessions]  # Total sequences across all users\n\n    def __len__(self):\n        # Total length of dataset will be the sum of all sequences across all users and sessions\n        return sum(len(self.eval_data[user_idx][session_idx]) for user_idx in range(len(self.eval_data))\n                   for session_idx in range(len(self.eval_data[user_idx])))\n\n    def __getitem__(self, idx):\n        # Find the user index and session index dynamically\n        cumulative_length = 0\n        for user_idx in range(len(self.eval_data)):\n            for session_idx in range(len(self.eval_data[user_idx])):\n                session_length = len(self.eval_data[user_idx][session_idx])\n                if cumulative_length + session_length > idx:\n                    seq_idx = idx - cumulative_length\n                    data = self.eval_data[user_idx][session_idx][seq_idx]\n\n                    # Debugging statements\n                    debug_log(f\"Index: {idx}, User Index: {user_idx}, Session Index: {session_idx}, Sequence Index: {seq_idx}\")\n\n                    # Check if data is None\n                    if data is None:\n                        error_log(f\"Returned data is None for index: {idx} in testdata\")\n                    return data, user_idx\n\n                cumulative_length += session_length\n        \n        # If we get here, idx is out of bounds\n        raise IndexError(\"Index out of bounds for dataset.\")","metadata":{"papermill":{"duration":0.047063,"end_time":"2024-10-25T16:53:37.519070","exception":false,"start_time":"2024-10-25T16:53:37.472007","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:42:57.808910Z","iopub.execute_input":"2024-10-26T09:42:57.809246Z","iopub.status.idle":"2024-10-26T09:42:57.823052Z","shell.execute_reply.started":"2024-10-26T09:42:57.809209Z","shell.execute_reply":"2024-10-26T09:42:57.821925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_accuracy = BEST_ACCURACY\nbest_loss = BEST_LOSS\ntraining_losses = []\nvalidation_losses = []\nfeature_embeddings_train=[]\nsaved_models = []\nmax_saved_models = MAX_SAVED_MODELS  ","metadata":{"papermill":{"duration":0.048945,"end_time":"2024-10-25T16:53:37.600620","exception":false,"start_time":"2024-10-25T16:53:37.551675","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:42:57.824477Z","iopub.execute_input":"2024-10-26T09:42:57.824856Z","iopub.status.idle":"2024-10-26T09:42:57.836092Z","shell.execute_reply.started":"2024-10-26T09:42:57.824821Z","shell.execute_reply":"2024-10-26T09:42:57.835148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training phase and store the model using both losses and Accuracy, Store Training accuracy\n\r\nThe number of model store based on variable MAX_SAVED_MODEL","metadata":{"papermill":{"duration":0.037337,"end_time":"2024-10-25T16:53:37.678743","exception":false,"start_time":"2024-10-25T16:53:37.641406","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def find_worst_loss_and_acc_in_existing_model(saved_models):\n    worst_loss = float('-inf')\n    worst_accuracy = float('inf')\n    for model_info in saved_models:\n        loss = model_info['loss']\n        accuracy = model_info['accuracy']\n        if worst_loss <  loss:\n            worst_loss = loss\n        if worst_accuracy > accuracy:\n            worst_accuracy = accuracy\n    return  worst_loss, worst_accuracy\n\ndef memory_is_low():\n    \"\"\"Check if memory is low. You can implement your own logic here.\"\"\"\n    # Placeholder logic; replace with actual memory checking\n    import psutil\n    return psutil.virtual_memory().available < (100 * 1024 * 1024)  # Less than 100 MB\n\ndef find_worst_model_by_accuracy(saved_models):\n    worst_model = None\n    worst_accuracy = float('inf')\n    for model_info in saved_models:\n        if model_info['accuracy'] < worst_accuracy:\n            worst_accuracy = model_info['accuracy']\n            worst_model = model_info\n    if worst_model:\n        print_log(f\"Worst model - Path: {worst_model['path']}, Accuracy: {worst_model['accuracy']:.6f}, Loss: {worst_model['loss']:.6f}\")\n    if worst_model:\n        if os.path.exists(worst_model['path']):\n            os.remove(worst_model['path'])\n            print_log(f\"Deleted worst model by less accuracy: {worst_model['path']} with Accuracy: {worst_model['accuracy']:.6f}, Loss: {worst_model['loss']:.6f}\")\n        saved_models.remove(worst_model)\n\ndef find_worst_model_by_loss(saved_models):\n    worst_model = None\n    worst_loss = float('-inf')\n    for model_info in saved_models:\n        if model_info['loss'] > worst_loss:\n            worst_loss = model_info['loss']\n            worst_model = model_info\n    if worst_model:\n        print_log(f\"Worst model - Path: {worst_model['path']}, Accuracy: {worst_model['accuracy']:.6f}, Loss: {worst_model['loss']:.6f}\")\n    if worst_model:\n        if os.path.exists(worst_model['path']):\n            os.remove(worst_model['path'])\n            print_log(f\"Deleted worst model by higher loss: {worst_model['path']} with Accuracy: {worst_model['accuracy']:.6f}, Loss: {worst_model['loss']:.6f}\")\n        saved_models.remove(worst_model)\n\ndef find_worst_model_by_combined_metric(saved_models, weight_loss=0.5, weight_accuracy=0.5):\n    worst_model = None\n    worst_combined_metric = float('inf')\n    combined_matrix_values = []\n    max_loss = 100\n    min_loss = 0\n    max_accuracy = 100\n    min_accuracy = 0\n    for model_info in saved_models:\n        normalized_loss = (model_info['loss'] - min_loss) / (max_loss - min_loss)\n        normalized_accuracy = (model_info['accuracy'] - min_accuracy) / (max_accuracy - min_accuracy)\n        # find combined metric, We want high accuracy and less losses\n        combined_metric = (weight_accuracy * normalized_accuracy) -(weight_loss * normalized_loss)\n        combined_matrix_values.append(combined_metric)\n        model_info[\"combined_metric\"] = combined_metric\n        \n        # Find the model with the smallest combined metric\n        if combined_metric < worst_combined_metric:\n            worst_combined_metric = combined_metric\n            worst_model = model_info\n    if worst_model:\n        print_log(f\"Worst model : Path: {worst_model['path']}, Accuracy: {worst_model['accuracy']:.6f}, Loss: {worst_model['loss']:.6f}\")\n        print_log(f\"Because this is have minimum combined matrix = {worst_combined_metric}, All combined matric {combined_matrix_values}\")\n    if worst_model:\n        if os.path.exists(worst_model['path']):\n            os.remove(worst_model['path'])\n            print_log(f\"Deleted worst model by less accuracy and higher loss: {worst_model['path']} with Accuracy: {worst_model['accuracy']:.6f}, Loss: {worst_model['loss']:.6f}\")\n        saved_models.remove(worst_model)\n\ntotal_losses = []\ntotal_accuracy = []\n\nfor i in range(init_epoch, epochs):\n# for epoch in range(epochs):\n    model_saved = 0\n    print(f\"Epoch {i+1} started\")\n    t_loss = 0.0\n    start = time.time()\n    model.train(True)\n    \n    for batch_idx, item in enumerate(dataloader):\n        anchor, positive, negatives, anchor_class, negative_classes  = item\n        optimizer.zero_grad()\n\n        # Forward pass for anchor, positive, and negatives\n        anchor_scores, anchor_features = model(anchor.float())\n        positive_scores, positive_features = model(positive.float())\n\n        # Separate negative class scores and features\n        negative_class_scores, negative_features = zip(*[model(neg.float()) for neg in negatives])\n        negative_class_scores = []\n        negative_features = []\n        for neg in negatives:\n            negative_class_score, negative_feature = model(neg.float())\n            negative_class_scores.append(negative_class_score)\n            negative_features.append(negative_feature)\n        \n        # Convert negative features to a tensor\n        negative_features = torch.stack(negative_features)  # Shape: (batch_size, num_negatives, embedding_dim)\n        \n        # Debug prints to check sizes\n#         print(f\"Anchor features shape: {anchor_features.shape}\")\n#         print(f\"Positive features shape: {positive_features.shape}\")\n#         print(f\"Negative features shape: {negative_features.shape}\")\n\n#         # Check if dimensions are correct\n#         assert anchor_features.size(0) == negative_features.size(0), \"Batch sizes must match\"\n\n        # Compute N-Pair Loss with negative features\n        nPair_loss = loss_fn(anchor_features, positive_features, negative_features)\n\n        # Classification with CrossEntropy Loss\n        all_class_scores = torch.cat([anchor_scores, positive_scores] + negative_class_scores, dim=0)\n        all_labels = torch.cat([anchor_class, anchor_class] + negative_classes, dim=0)\n        class_loss = nn.CrossEntropyLoss()(all_class_scores, all_labels)\n\n        # Combine the losses\n        total_loss = WEIGHT_NPAIR_LOSS * nPair_loss + WEIGHT_CLASSIFIER_LOSS * class_loss\n        total_loss.backward()\n        optimizer.step()\n        \n        \n        t_loss += total_loss.item()\n        \n    \n    t_loss /= len(dataloader)\n    training_losses.append(t_loss)\n    \n#     accuracy_train = accuracy_score(all_labels_train, predicted_classes_train)\n#     print(\"accuracy of train\", accuracy_train)\n\n    \n    # Validation phase\n    model.eval()\n    v_loss = 0.0\n    all_preds = []\n    all_labels = []\n    t_dataset = TestDataset(validation_data)\n    \n    t_dataloader = DataLoader(t_dataset, batch_size=batch_size, shuffle=False)\n    tot = 0\n    print_log(f\"The lenth of t_dataloader : {len(t_dataloader)}\")\n    for batch_idx_t, item_t in enumerate(t_dataloader):\n        with torch.no_grad():\n            val = tot // 992\n            tot += 1\n\n            item_t_in, class_label = item_t\n            # Get model outputs\n            logits = model(item_t_in.float()) \n            \n            # Apply softmax to get probabilities\n            probabilities = torch.softmax(logits[0], dim=1)  # Assuming logits[0] contains class scores\n            predicted_classes = torch.argmax(probabilities, dim=1)  # Get predicted classes\n            true_labels = class_label\n            correct_predictions = (predicted_classes == true_labels)\n            \n            accuracy = correct_predictions.sum().item() / len(true_labels)\n            \n#             print(f\"Batch {batch_idx_t} - True Labels: {true_labels.tolist()} - Predicted: {predicted_classes.tolist()} -Class Labels: {class_label}\")\n            all_preds.extend(predicted_classes.tolist())\n            all_labels.extend(true_labels.tolist())\n#             val_loss = classification_loss_fn(class_scores, true_labels)\n#             v_loss += val_loss.item()\n#     v_loss /= len(t_dataloader)\n#     validation_losses.append(v_loss)\n    accuracy = accuracy_score(all_labels, all_preds)\n    precision = precision_score(all_labels, all_preds, average='macro')\n    recall = recall_score(all_labels, all_preds, average='macro')\n    f1 = f1_score(all_labels, all_preds, average='macro')\n    print_log(f\"loop done  {tot}\")\n    end = time.time()\n    \n    total_losses.append(t_loss)\n    total_accuracy.append(accuracy)\n    \n    print_log(f\"------> Epoch No: {i+1} => Loss: {t_loss:.6f} >> Accuracy: {accuracy:.6f} >> Precision: {precision:.6f} >> Recall: {recall:.6f} >> F1: {f1:.6f} >> Time: {end-start:.2f}\")\n#     scheduler.step(t_loss)\n    # log metrics to wandb\n    wandb.log({\"accuracy\": accuracy, \"loss\": t_loss, \"precision\" : precision, \"recall\" : recall, \"f1\" : f1})\n\n    \n    # Check memory status\n    if memory_is_low():\n        print_log(f\"Memory is running low, attempting to free up space by deleting the worst model.\")\n        # find_worst_model_by_loss(saved_models)\n        # find_worst_model_by_accuracy(saved_models)\n        find_worst_model_by_combined_metric(saved_models)\n        # assign last best loss and accuracy in existing models\n        if len(saved_models) > max_saved_models:\n            best_loss, best_accuracy = find_worst_loss_and_acc_in_existing_model(saved_models)\n\n    # Save model if validation loss improves and This model not saved before\n    if t_loss < best_loss:\n        print_log(f\"Loss improved from {best_loss:.6f} to {t_loss:.6f}. \")\n        if model_saved:\n            print_log(\"Already model saved. Skip this model by loss.\")\n        else:\n            print_log(\"Saving model.................\")\n            model_path = f\"{best_model_save_path}/epoch_{i+1}_accuracy_{accuracy:.6f}_loss_{t_loss:.6f}.pt\"\n            torch.save(model, model_path)\n            print_log(f\"Best model saved at (by loss): {model_path}\")\n            saved_models.append({'path': model_path, 'loss': t_loss, 'accuracy' : accuracy})\n            # Model saved\n            model_saved = 1\n    \n            if len(saved_models) > max_saved_models:\n                # find_worst_model_by_loss(saved_models)\n                # find_worst_model_by_accuracy(saved_models)\n                find_worst_model_by_combined_metric(saved_models)\n            \n                # assign last best loss and accuracy in existing models\n                best_loss, best_accuracy = find_worst_loss_and_acc_in_existing_model(saved_models)\n\n\n    # Save model if validation accuracy improves\n    if accuracy > best_accuracy :\n        print_log(f\"Accuracy improved from {best_accuracy:.6f} to {accuracy:.6f}.\")\n        if model_saved:\n            print_log(\"But, Already model saved. Skip this model by accuracy.\")\n        else:\n            print_log(\"Saving model.................\")\n            model_path = f\"{best_model_save_path}/epoch_{i+1}_accuracy_{accuracy:.6f}_loss_{t_loss:.6f}.pt\"\n            torch.save(model, model_path)\n            print_log(f\"Best model saved at (by accuracy): {model_path}\")\n            saved_models.append({'path': model_path, 'loss' : t_loss, 'accuracy': -accuracy})\n            # Model saved\n            model_saved = 1\n    \n            if len(saved_models) > max_saved_models:\n                # find_worst_model_by_loss(saved_models)\n                # find_worst_model_by_accuracy(saved_models)\n                find_worst_model_by_combined_metric(saved_models)\n            \n                # assign last best loss and accuracy in existing models\n                best_loss, best_accuracy = find_worst_loss_and_acc_in_existing_model(saved_models)\n    \n\nprint(f\"total_losses : {total_losses}\")\nprint(f\"total_accuracy : {total_accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-26T09:42:57.837644Z","iopub.execute_input":"2024-10-26T09:42:57.837969Z","iopub.status.idle":"2024-10-26T09:51:14.076530Z","shell.execute_reply.started":"2024-10-26T09:42:57.837935Z","shell.execute_reply":"2024-10-26T09:51:14.075521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Get best model","metadata":{"papermill":{"duration":0.052091,"end_time":"2024-10-25T17:50:23.714674","exception":false,"start_time":"2024-10-25T17:50:23.662583","status":"completed"},"tags":[]}},{"cell_type":"code","source":"directory = '/kaggle/working/best_models'\n\n# Get the list of files in the directory\nfiles = os.listdir(directory)\n\n# Print the list of files\nprint_log(f\"The model file directory : {directory}\")\nfor file in files:\n    print_log(f\"file  :  {file}\")\n\n# # Function to extract EER from filename\n# def extract_accuracy(filename):\n#     try:\n#         parts = filename.split('_')\n#         accuracy_index = parts.index('accuracy') + 1\n#         accuracy_value = float(parts[accuracy_index].replace('.pt', ''))\n#         return accuracy_value\n#     except (ValueError, IndexError):\n#         return float('inf')\n\n# # Find the file with the lowest EER\n# best_model_file = max(files, key=extract_accuracy)\n# best_acc = extract_accuracy(best_model_file)\n\n# Function to extract accuracy and loss from filename\ndef extract_info(filename):\n    parts = filename.split('_')\n    epoch_value = int(parts[parts.index('epoch') + 1])\n    accuracy_value = float(parts[parts.index('accuracy') + 1])\n    loss_value = float(parts[parts.index('loss') + 1].replace('.pt', ''))\n    return epoch_value, accuracy_value, loss_value\n\n\n# Function to find the best model using both loss and accuracy\ndef find_best_model_using_normalization(file_list, weight_loss=0.5, weight_accuracy=0.5):\n    best_model = None\n    best_combined_metric = float('-inf')\n    combined_matrix_values = []\n    saved_models = []\n\n    # Extract accuracy and loss from the filenames\n    for f in file_list:\n        file_info =  extract_info(f)\n        saved_models.append({'filename': f,'epoch' : file_info[0], 'accuracy': file_info[1], 'loss': file_info[2]})\n\n    # Calculate statistical parameters needed for normalization\n    losses = [model['loss'] for model in saved_models]\n    accuracies = [model['accuracy'] for model in saved_models]\n\n    max_loss = 100\n    min_loss = 0\n    max_accuracy = 100\n    min_accuracy = 0\n\n    # Apply the selected normalization method\n    for model_info in saved_models:\n        loss = model_info['loss']\n        accuracy = model_info['accuracy']\n        normalized_loss = (loss - min_loss) / (max_loss - min_loss)\n        normalized_accuracy = (accuracy - min_accuracy)/(max_accuracy - min_accuracy)\n        \n        # Combined metric: prioritize higher accuracy and lower loss\n        combined_metric = (weight_accuracy * normalized_accuracy) - (weight_loss * normalized_loss)\n        combined_matrix_values.append(combined_metric)\n\n        # Find the model with the highest combined metric\n        if combined_metric > best_combined_metric:\n            best_combined_metric = combined_metric\n            best_model = model_info\n\n    # Print best model information\n    if best_model:\n        print_log(f\"Best model - File: {best_model['filename']}, Accuracy: {best_model['accuracy']:.6f}, Loss: {best_model['loss']:.6f}\")\n        print_log(f\"Best combined metric: {best_combined_metric} in all combined metrics: {combined_matrix_values}\")\n\n    return best_model\n\n# Find the best model using Min-Max Normalization\nprint_log(\"Best Model using Min-Max Normalization\")\nbest_model = find_best_model_using_normalization(files)\n\n\n\nbest_model_file = best_model['filename']\nbest_acc = best_model['accuracy']\nbest_loss = best_model['loss']\nwandb.log({\n    \"best_model_epoch\": best_model[\"epoch\"],\n    \"best_model_accuracy\": best_model['accuracy'],\n    \"best_model_loss\": best_model['loss']\n})\n\n# Load the best model\nbest_model_path = os.path.join(directory, best_model_file)\ntest_model = torch.load(best_model_path)\n\nprint_log(f\"Best model: {best_model_file} with accuracy: {best_acc} and loss: {best_loss}\")\nprint_log(f\"Loaded model from: {best_model_path}\")\n\n","metadata":{"papermill":{"duration":0.508947,"end_time":"2024-10-25T17:50:24.274782","exception":false,"start_time":"2024-10-25T17:50:23.765835","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:51:14.078779Z","iopub.execute_input":"2024-10-26T09:51:14.079198Z","iopub.status.idle":"2024-10-26T09:51:14.391770Z","shell.execute_reply.started":"2024-10-26T09:51:14.079151Z","shell.execute_reply":"2024-10-26T09:51:14.390761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting training and validation loss\nplt.figure(figsize=(10, 5))\nplt.plot(training_losses, label='Training Loss')\nplt.plot(validation_losses, label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training Loss Over Epochs')\nplt.show()\nprint_log(\"Training loss over epoches polt is create sucessfully\")","metadata":{"papermill":{"duration":0.395437,"end_time":"2024-10-25T17:50:24.722713","exception":false,"start_time":"2024-10-25T17:50:24.327276","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:51:14.392936Z","iopub.execute_input":"2024-10-26T09:51:14.393302Z","iopub.status.idle":"2024-10-26T09:51:14.702525Z","shell.execute_reply.started":"2024-10-26T09:51:14.393263Z","shell.execute_reply":"2024-10-26T09:51:14.701533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(test_data)\ntest_dataloader = DataLoader(test_dataset, batch_size=2)\nfeature_embeddings = []\ntot=0\nall_preds_test = []\nall_labels_test = []","metadata":{"papermill":{"duration":0.062886,"end_time":"2024-10-25T17:50:24.839711","exception":false,"start_time":"2024-10-25T17:50:24.776825","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:51:14.703829Z","iopub.execute_input":"2024-10-26T09:51:14.704176Z","iopub.status.idle":"2024-10-26T09:51:14.723753Z","shell.execute_reply.started":"2024-10-26T09:51:14.704141Z","shell.execute_reply":"2024-10-26T09:51:14.722631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tot=0\nfor batch_idx_t, item_t in enumerate(test_dataloader):\n    with torch.no_grad():\n        val=tot//162 ## this value needs to be changed based on the sequence from each activity\n        tot+=1\n        item_t_in,class_label=item_t\n#         print(\"tot\",tot)\n        true_labels=class_label\n\n        item_out = test_model(item_t_in.float())\n        class_scores=item_out[0]\n#         print(class_scores)\n        feature_embeddings.append(item_out[1])\n        predicted_classes = torch.argmax(class_scores, dim=1)\n        all_preds_test.extend(predicted_classes.tolist())\n        all_labels_test.extend(true_labels.tolist())","metadata":{"papermill":{"duration":40.866522,"end_time":"2024-10-25T17:51:05.759084","exception":false,"start_time":"2024-10-25T17:50:24.892562","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:51:14.724871Z","iopub.execute_input":"2024-10-26T09:51:14.725183Z","iopub.status.idle":"2024-10-26T09:51:56.054072Z","shell.execute_reply.started":"2024-10-26T09:51:14.725150Z","shell.execute_reply":"2024-10-26T09:51:56.053259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Move each tensor to CPU, detach from computation graph, and convert to NumPy array\nfeature_embeddings_cpu = [emb.cpu().detach().numpy() for emb in feature_embeddings]\n\n# Check for NaNs in individual arrays\nfor i, emb_np in enumerate(feature_embeddings_cpu):\n    if np.isnan(emb_np).any():\n        print(f\"NaN detected in feature_embeddings_cpu at index {i}\")\n\n# Concatenate the list of numpy arrays into a single numpy array\nfeature_embeddings_np = np.concatenate(feature_embeddings_cpu, axis=0)\n\n# Check for NaNs in the concatenated array\nif np.isnan(feature_embeddings_np).any():\n    print(\"NaN detected in feature_embeddings_np\")\n\n# Perform PCA to reduce dimensions to 2\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nprincipal_components = pca.fit_transform(feature_embeddings_np)\n\n# Check for NaNs in PCA results\nif np.isnan(principal_components).any():\n    print(\"NaN detected in principal_components\")\n\n# Plot PCA results\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 6))\nplt.scatter(principal_components[:, 0], principal_components[:, 1], alpha=0.5)\nplt.title('PCA of Feature Embeddings')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.grid(True)\nplt.show()\nprint_log(\"PCA of Feature embeddings polt is create sucessfully\")\n","metadata":{"papermill":{"duration":1.010718,"end_time":"2024-10-25T17:51:06.820910","exception":false,"start_time":"2024-10-25T17:51:05.810192","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:51:56.055305Z","iopub.execute_input":"2024-10-26T09:51:56.055647Z","iopub.status.idle":"2024-10-26T09:51:56.675852Z","shell.execute_reply.started":"2024-10-26T09:51:56.055611Z","shell.execute_reply":"2024-10-26T09:51:56.674903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming all_labels_test contains the true labels for the data points\n# Map each class to a different color using a colormap\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\n\n# Convert feature embeddings to NumPy array (already done in previous steps)\n# feature_embeddings_cpu = [emb.cpu().detach().numpy() for emb in feature_embeddings]\n# feature_embeddings_np = np.concatenate(feature_embeddings_cpu, axis=0)\n\n# Perform PCA to reduce dimensions to 2\npca = PCA(n_components=2)\nprincipal_components = pca.fit_transform(feature_embeddings_np)\n\n# Create a scatter plot with different colors for each class\nplt.figure(figsize=(8, 6))\n\n# Get a colormap with enough colors for each class\nnum_classes = len(np.unique(all_labels_test))\npalette = sns.color_palette(\"hsv\", num_classes)\n\n# Scatter plot with colors based on class labels\nfor class_id in np.unique(all_labels_test):\n    # Filter the points belonging to the current class\n    indices = np.where(np.array(all_labels_test) == class_id)\n    plt.scatter(principal_components[indices, 0], \n                principal_components[indices, 1], \n                alpha=0.6, \n                label=activity_id_mapping[class_id], \n                color=palette[class_id])\n\n# Add plot details\nplt.title('PCA of Feature Embeddings by Class')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.legend(title=\"Class\", loc='best', bbox_to_anchor=(1, 1), ncol=1)  # Show legend outside plot\nplt.grid(True)\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n\nprint_log(\"PCA of Feature embeddings plot with class colors created successfully\")\n","metadata":{"papermill":{"duration":1.452719,"end_time":"2024-10-25T17:51:08.331483","exception":false,"start_time":"2024-10-25T17:51:06.878764","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:51:56.677150Z","iopub.execute_input":"2024-10-26T09:51:56.677504Z","iopub.status.idle":"2024-10-26T09:51:57.944080Z","shell.execute_reply.started":"2024-10-26T09:51:56.677468Z","shell.execute_reply":"2024-10-26T09:51:57.943153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_test = accuracy_score(all_labels_test, all_preds_test)\nprecision_test = precision_score(all_labels_test, all_preds_test, average='macro')\nrecall_test = recall_score(all_labels_test, all_preds_test, average='macro')\nf1_test = f1_score(all_labels_test, all_preds_test, average='macro')\n\nprint_log(f\"Accuracy: {accuracy_test:.6f} - Precision: {precision_test:.6f} - Recall: {recall_test:.6f} - F1: {f1_test:.6f}\")\n","metadata":{"papermill":{"duration":0.121522,"end_time":"2024-10-25T17:51:08.515334","exception":false,"start_time":"2024-10-25T17:51:08.393812","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:51:57.945594Z","iopub.execute_input":"2024-10-26T09:51:57.945874Z","iopub.status.idle":"2024-10-26T09:51:58.005606Z","shell.execute_reply.started":"2024-10-26T09:51:57.945842Z","shell.execute_reply":"2024-10-26T09:51:58.004550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conf_matrix = confusion_matrix(all_labels_test, all_preds_test)\n\n# Calculate the accuracy for each class\nclass_accuracies = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n\n# Print the accuracy for each class\nfor i, class_accuracy in enumerate(class_accuracies):\n    print_log(f\"Accuracy for class {i}: {class_accuracy:.6f}\")\n\n\nplt.figure(figsize=(10, 7))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.savefig('confusion_matrix.png')\nplt.show()\nprint_log(\"Confusion matrix is create sucessfully using test\")\nwandb.log({\"confusion_matrix\": wandb.Image('confusion_matrix.png')})","metadata":{"papermill":{"duration":2.056571,"end_time":"2024-10-25T17:51:10.634603","exception":false,"start_time":"2024-10-25T17:51:08.578032","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:51:58.007320Z","iopub.execute_input":"2024-10-26T09:51:58.007789Z","iopub.status.idle":"2024-10-26T09:52:00.114655Z","shell.execute_reply.started":"2024-10-26T09:51:58.007742Z","shell.execute_reply":"2024-10-26T09:52:00.113667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score\nimport pandas as pd\n\n# Initialize lists to store results\nclass_names = []\nprecision_list = []\nrecall_list = []\nf1_list = []\n\n# Calculate precision, recall, and f1-score for each class\nfor class_id, class_name in activity_id_mapping.items():\n    # Precision, recall, and f1 for the current class without averaging\n    precision = precision_score(all_labels_test, all_preds_test, labels=[class_id], average=None, zero_division=0)\n    recall = recall_score(all_labels_test, all_preds_test, labels=[class_id], average=None, zero_division=0)\n    f1 = f1_score(all_labels_test, all_preds_test, labels=[class_id], average=None, zero_division=0)\n    \n    # Append results to respective lists\n    class_names.append(class_name)\n    precision_list.append(precision[0])  # Since labels=[class_id] returns a list\n    recall_list.append(recall[0])        # Extract the first element for that class\n    f1_list.append(f1[0])\n\n\nclass_names = class_names[0: CLASSES]\nprecision_list = precision_list[0: CLASSES]\nrecall_list = recall_list[0: CLASSES]\nf1_list = f1_list[0: CLASSES]\n\n\n# Calculate the average across all classes (macro average)\naverage_precision = sum(precision_list) / len(precision_list)\naverage_recall = sum(recall_list) / len(recall_list)\naverage_f1 = sum(f1_list) / len(f1_list)\n\n\n\nclass_names.append('Average')\nrecall_list.append(average_recall)\nprecision_list.append(average_precision)\nf1_list.append(average_f1)\n\n# Create a DataFrame to store the results\nresults_df = pd.DataFrame({\n    'Class': class_names,\n    'Recall': recall_list,\n    'Precision': precision_list,\n    'F1-score': f1_list\n})\n\nresults_df.to_csv('class_precision_recall_f1.csv', index=False)\n\n# Display the final table\nresults_df\n","metadata":{"papermill":{"duration":0.818041,"end_time":"2024-10-25T17:51:11.521174","exception":false,"start_time":"2024-10-25T17:51:10.703133","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:52:00.116077Z","iopub.execute_input":"2024-10-26T09:52:00.116742Z","iopub.status.idle":"2024-10-26T09:52:00.939483Z","shell.execute_reply.started":"2024-10-26T09:52:00.116699Z","shell.execute_reply":"2024-10-26T09:52:00.938411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conf_matrix = confusion_matrix(all_labels, all_preds)\n\n# Calculate the accuracy for each class\nclass_accuracies = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n\n# Print the accuracy for each class\nfor i, class_accuracy in enumerate(class_accuracies):\n    print_log(f\"Accuracy for class {i}: {class_accuracy:.6f}\")\n\nplt.figure(figsize=(10, 7))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.savefig('confusion_matrix_validation.png')\nplt.show()\nprint_log(\"Validation Confusion matrix is create sucessfully\")","metadata":{"papermill":{"duration":1.866157,"end_time":"2024-10-25T17:51:13.452167","exception":false,"start_time":"2024-10-25T17:51:11.586010","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:52:00.940797Z","iopub.execute_input":"2024-10-26T09:52:00.941220Z","iopub.status.idle":"2024-10-26T09:52:02.686616Z","shell.execute_reply.started":"2024-10-26T09:52:00.941173Z","shell.execute_reply":"2024-10-26T09:52:02.685608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# [optional] finish the wandb run, necessary in notebooks\nwandb.finish()","metadata":{"papermill":{"duration":2.612416,"end_time":"2024-10-25T17:51:16.135373","exception":false,"start_time":"2024-10-25T17:51:13.522957","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T09:52:02.688226Z","iopub.execute_input":"2024-10-26T09:52:02.688641Z","iopub.status.idle":"2024-10-26T09:52:04.335095Z","shell.execute_reply.started":"2024-10-26T09:52:02.688598Z","shell.execute_reply":"2024-10-26T09:52:04.333898Z"},"trusted":true},"execution_count":null,"outputs":[]}]}