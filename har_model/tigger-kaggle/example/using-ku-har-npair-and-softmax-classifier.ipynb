{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-10-26T09:41:46.818993Z","iopub.status.busy":"2024-10-26T09:41:46.818549Z","iopub.status.idle":"2024-10-26T09:41:46.828793Z","shell.execute_reply":"2024-10-26T09:41:46.827678Z","shell.execute_reply.started":"2024-10-26T09:41:46.818955Z"},"papermill":{"duration":7.349917,"end_time":"2024-10-25T16:52:17.611726","exception":false,"start_time":"2024-10-25T16:52:10.261809","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["#### Necessary Imports\n","import os\n","import logging\n","import inspect \n","import numpy as np\n","import pandas as pd\n","from matplotlib import pyplot as plt\n","from matplotlib.pyplot import figure\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","import pickle\n","from torch import nn\n","import torch\n","import subprocess\n","from torch.utils.data import Dataset, DataLoader\n","import math\n","import time\n","import seaborn as sns\n","import matplotlib as mpl\n","from sklearn.manifold import TSNE\n","from sklearn.metrics import silhouette_score\n","import shutil\n","import random\n","from sklearn.metrics import accuracy_score, precision_score, recall_score,f1_score,confusion_matrix\n","from sklearn.decomposition import PCA\n","import torch.optim as optim\n","import torch.optim.lr_scheduler as lr_scheduler\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","\n","import wandb\n","import random"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:41:46.831886Z","iopub.status.busy":"2024-10-26T09:41:46.831171Z","iopub.status.idle":"2024-10-26T09:41:46.840461Z","shell.execute_reply":"2024-10-26T09:41:46.839666Z","shell.execute_reply.started":"2024-10-26T09:41:46.831847Z"},"papermill":{"duration":0.026915,"end_time":"2024-10-25T16:52:17.659772","exception":false,"start_time":"2024-10-25T16:52:17.632857","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# pip install wandb"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Retrieve parameters from environment variables\n","EPOCHS_p = int(os.getenv(\"EPOCHS\", 5))  # Default to 20 if not provided\n","EPOCH_PER_BATCH_COUNT_p = int(os.getenv(\"EPOCH_PER_BATCH_COUNT\", 2))  # Default to 2 if not provided\n","WEIGHTED_LOSS_FACTOR_p = float(os.getenv(\"WEIGHTED_LOSS_FACTOR\", 0.1))  # Default to 0.2 if not provided\n","\n","# Print values for confirmation\n","print(f\"EPOCHS: {EPOCHS_p}\")\n","print(f\"EPOCH_PER_BATCH_COUNT: {EPOCH_PER_BATCH_COUNT_p}\")\n","print(f\"WEIGHTED_LOSS_FACTOR: {WEIGHTED_LOSS_FACTOR_p}\")\n"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.019921,"end_time":"2024-10-25T16:52:17.69943","exception":false,"start_time":"2024-10-25T16:52:17.679509","status":"completed"},"tags":[]},"source":["# Define Constants"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:41:46.842089Z","iopub.status.busy":"2024-10-26T09:41:46.841728Z","iopub.status.idle":"2024-10-26T09:41:46.852421Z","shell.execute_reply":"2024-10-26T09:41:46.851521Z","shell.execute_reply.started":"2024-10-26T09:41:46.842022Z"},"papermill":{"duration":0.031746,"end_time":"2024-10-25T16:52:17.750863","exception":false,"start_time":"2024-10-25T16:52:17.719117","status":"completed"},"scrolled":true,"tags":[],"trusted":true},"outputs":[],"source":["LOG_LEVEL = \"INFO\" # Adjust this to \"DEBUG\", \"INFO\", \"WARNING\" or \"ERROR\"\n","\n","WANDB_API_KEY = \"b15ee5c84e51289dd7b5dd11ea38949957d772f9\"\n","\n","\n","activity_id_mapping = {\n","    0 : \"Stand\",\n","    1 : \"Sit\",\n","    2 : \"Talk-sit\",\n","    3 : \"Talk-stand\",\n","    4 : \"Stand-sit\",\n","    5 : \"Lay\",\n","    6 : \"Lay-stand\",\n","    7 : \"Pick\",\n","    8 : \"Jump\",\n","    9 : \"Push-up\",\n","    10 : \"Sit-up\",\n","    11 : \"Walk\",\n","    12 : \"Walk-backward\",\n","    13 : \"Walk-circle\",\n","    14 : \"Run\",\n","    15 : \"Stair-up\",\n","    16 : \"Stair-down\",\n","    17 : \"Table-tennis\"\n","}\n","\n","columns = [\"activityID\", \"subjectID\", \n","               \"acc_x\", \"acc_y\", \"acc_z\", \n","               \"gyro_x\", \"gyro_y\", \"gyro_z\", \n","               \"ori_x\", \"ori_y\", \"ori_z\"]\n","\n","EPOCH_BATCH_COUNT = 200\n","EPOCHS = 100\n","BATCH_SIZE = 64\n","IMU_FEATURE_COUNT = 24\n","CLASSES = 18\n","LEARNING_RATE = 0.005 # change learning rate < 0.01\n","\n","BEST_ACCURACY = 0.0\n","BEST_LOSS = 1000\n","\n","MAX_SAVED_MODELS = 6   # Maximum number of models to keep\n","\n","\n","WEIGHT_LOSS= 0.5\n","\n","# WEIGHT_CLASSIFIER_LOSS  = 0.9  # >= 8 \n","\n","# WEIGHT_TRIPLET_LOSS = 0.09  # < 0.15\n","\n","\n","WEIGHT_NPAIR_LOSS = 0.2\n","WEIGHT_CLASSIFIER_LOSS  = 1 - WEIGHT_NPAIR_LOSS\n","\n","SEQ_LENTH = 500\n","WINDOW_SIZE = 20\n","NUM_NEGATIVES = 4"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.019738,"end_time":"2024-10-25T16:52:17.790694","exception":false,"start_time":"2024-10-25T16:52:17.770956","status":"completed"},"tags":[]},"source":["# Export logs"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:41:46.854061Z","iopub.status.busy":"2024-10-26T09:41:46.85374Z","iopub.status.idle":"2024-10-26T09:41:46.868901Z","shell.execute_reply":"2024-10-26T09:41:46.868097Z","shell.execute_reply.started":"2024-10-26T09:41:46.854007Z"},"papermill":{"duration":0.572727,"end_time":"2024-10-25T16:52:18.383714","exception":false,"start_time":"2024-10-25T16:52:17.810987","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["wandb.login(key=WANDB_API_KEY)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:41:46.871447Z","iopub.status.busy":"2024-10-26T09:41:46.8711Z","iopub.status.idle":"2024-10-26T09:41:49.73374Z","shell.execute_reply":"2024-10-26T09:41:49.732708Z","shell.execute_reply.started":"2024-10-26T09:41:46.871407Z"},"papermill":{"duration":3.86546,"end_time":"2024-10-25T16:52:22.270472","exception":false,"start_time":"2024-10-25T16:52:18.405012","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# start a new wandb run to track this script\n","wandb.init(\n","    # set the wandb project where this run will be logged\n","    project=\"har-using-imu-data-npair\",\n","    # name = \"Version-17-lr\",\n","    name = \"Version-test-tigger\",\n","#     name = \"Interctive-7\",\n","#     notes = \"Same as version 53, losss- Triplet cousine\",\n","\n","    # track hyperparameters and run metadata\n","    config={\n","        \"architecture\": \"Transformer\",\n","        \"dataset\": \"KU-HAR\",\n","        \"epochs\": EPOCHS,\n","        \"epoch_batch_count\" : EPOCH_BATCH_COUNT,\n","        \"batch_size\" : BATCH_SIZE,\n","        \"imu_feature_count\" : IMU_FEATURE_COUNT,\n","        \"classes\" : CLASSES,\n","        \"learning_rate\" : LEARNING_RATE,\n","        \"weight_loss\" : WEIGHT_LOSS,\n","        \"WEIGHT_CLASSIFIER_LOSS\" : WEIGHT_CLASSIFIER_LOSS,\n","#         \"WEIGHT_TRIPLET_LOSS\" : WEIGHT_TRIPLET_LOSS,\n","        \"WEIGHT_NPAIR_LOSS\" : WEIGHT_NPAIR_LOSS,\n","        \"SEQ_LENTH\" : SEQ_LENTH,\n","        \"WINDOW_SIZE\" : WINDOW_SIZE,\n","        \"NUM_NEGATIVES\" : NUM_NEGATIVES,\n","    }\n",")\n","\n","# # simulate training\n","# epochs = 10\n","# offset = random.random() / 5\n","# for epoch in range(2, epochs):\n","#     acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n","#     loss = 2 ** -epoch + random.random() / epoch + offset\n","\n","#     # log metrics to wandb\n","#     wandb.log({\"acc\": acc, \"loss\": loss})\n","\n","# # [optional] finish the wandb run, necessary in notebooks\n","# wandb.finish()"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.022221,"end_time":"2024-10-25T16:52:22.316864","exception":false,"start_time":"2024-10-25T16:52:22.294643","status":"completed"},"tags":[]},"source":["# Genarate logs"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:41:49.73698Z","iopub.status.busy":"2024-10-26T09:41:49.736026Z","iopub.status.idle":"2024-10-26T09:41:49.752093Z","shell.execute_reply":"2024-10-26T09:41:49.751002Z","shell.execute_reply.started":"2024-10-26T09:41:49.736926Z"},"papermill":{"duration":0.039811,"end_time":"2024-10-25T16:52:22.378713","exception":false,"start_time":"2024-10-25T16:52:22.338902","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Map log level strings to logging constants\n","log_levels = {\n","    \"DEBUG\": logging.DEBUG,\n","    \"INFO\": logging.INFO,\n","    \"WARNING\": logging.WARNING,\n","    \"ERROR\": logging.ERROR\n","}\n","set_log_level = log_levels.get(LOG_LEVEL, logging.INFO)  # Default to INFO if an unrecognized level is given\n","\n","# Configure the logging format and level\n","logging.basicConfig(\n","    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n","    level=set_log_level,\n","    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",")\n","\n","def log_message(level, message, block=None, log_title=\"\", caller_frame=None):\n","    \"\"\"\n","    Logs a message with a specific logging level and additional details.\n","    \n","    Args:\n","        level (str): Logging level ('DEBUG', 'INFO', 'WARNING', 'ERROR').\n","        message (str): The message to log.\n","        block (str, optional): Additional block/section name for context.\n","        log_title (str): Title to specify log type.\n","        caller_frame (frame, optional): Frame object of the calling function.\n","    \"\"\"\n","    # Get the calling function's details for context\n","    line_number = caller_frame.f_lineno if caller_frame else \"N/A\"\n","    function_name = caller_frame.f_code.co_name if caller_frame else \"N/A\"\n","\n","    # Format log title and line number to a fixed width\n","    formatted_title = log_title.ljust(7)  # Pad log title to 7 characters\n","    formatted_line = f\"Line {line_number}\".ljust(8)  # Pad line info to 8 characters\n","\n","    # Format the log with extra details\n","    # log_msg = f\"{formatted_title} | {formatted_line} | {function_name} | {message}\"\n","    log_msg = f\"{formatted_title} | {formatted_line} | {message}\"\n","    if block:\n","        log_msg += f\" - Block: {block}\"\n","\n","    # Check if the log level should print based on the configured log level\n","    should_print = log_levels[level.upper()] >= set_log_level\n","\n","    if should_print:\n","        print(log_msg)  # Print the message if it meets or exceeds the log level\n","\n","    # Log the message based on the specified level\n","    if level.upper() == \"DEBUG\":\n","        logging.debug(log_msg)\n","    elif level.upper() == \"INFO\":\n","        logging.info(log_msg)\n","    elif level.upper() == \"WARNING\":\n","        logging.warning(log_msg)\n","    elif level.upper() == \"ERROR\":\n","        logging.error(log_msg)\n","    else:\n","        logging.info(\"Unknown log level specified.\")\n","\n","# Shortcut functions for different levels\n","def print_log(message, block=None):\n","    caller_frame = inspect.currentframe().f_back\n","    log_message(\"INFO\", message, block, log_title=\"INFO\", caller_frame=caller_frame)\n","\n","def debug_log(message, block=None):\n","    caller_frame = inspect.currentframe().f_back\n","    log_message(\"DEBUG\", message, block, log_title=\"DEBUG\", caller_frame=caller_frame)\n","\n","def warn_log(message, block=None):\n","    caller_frame = inspect.currentframe().f_back\n","    log_message(\"WARNING\", message, block, log_title=\"WARNING\", caller_frame=caller_frame)\n","\n","def error_log(message, block=None):\n","    caller_frame = inspect.currentframe().f_back\n","    log_message(\"ERROR\", message, block, log_title=\"ERROR\", caller_frame=caller_frame)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:41:49.753728Z","iopub.status.busy":"2024-10-26T09:41:49.753395Z","iopub.status.idle":"2024-10-26T09:41:49.769992Z","shell.execute_reply":"2024-10-26T09:41:49.769081Z","shell.execute_reply.started":"2024-10-26T09:41:49.753694Z"},"papermill":{"duration":0.031746,"end_time":"2024-10-25T16:52:22.432487","exception":false,"start_time":"2024-10-25T16:52:22.400741","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# The example usage of logs\n","print_log(\"Training started\", block=\"Training Phase\")\n","debug_log(\"Loaded 500 records\", block=\"Data Loading\")\n","warn_log(\"Missing values detected\", block=\"Data Validation\")\n","error_log(\"Failed to save model\", block=\"Model Saving\")"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.022217,"end_time":"2024-10-25T16:52:22.477253","exception":false,"start_time":"2024-10-25T16:52:22.455036","status":"completed"},"tags":[]},"source":["# Clear Working Directory"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:41:49.773798Z","iopub.status.busy":"2024-10-26T09:41:49.772804Z","iopub.status.idle":"2024-10-26T09:41:49.78109Z","shell.execute_reply":"2024-10-26T09:41:49.780291Z","shell.execute_reply.started":"2024-10-26T09:41:49.773755Z"},"papermill":{"duration":0.032796,"end_time":"2024-10-25T16:52:22.532417","exception":false,"start_time":"2024-10-25T16:52:22.499621","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import os\n","import shutil\n","\n","\n","def clear_working_directory():\n","    # Define the directory to clear\n","    directory_to_clear = '/kaggle/working/'\n","\n","    # Iterate through all files and directories in the specified directory\n","    for filename in os.listdir(directory_to_clear):\n","        file_path = os.path.join(directory_to_clear, filename)\n","        try:\n","            if os.path.isfile(file_path) or os.path.islink(file_path):\n","                os.unlink(file_path)  # Remove the file or symbolic link\n","            elif os.path.isdir(file_path):\n","                shutil.rmtree(file_path)  # Remove the directory and its contents\n","            print_log(f\"Removed: {file_path}\")  # Optional: Print the removed file or directory\n","        except Exception as e:\n","            error_log(f\"Failed to remove {file_path}. Reason: {e}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:41:49.782609Z","iopub.status.busy":"2024-10-26T09:41:49.782304Z","iopub.status.idle":"2024-10-26T09:41:49.794644Z","shell.execute_reply":"2024-10-26T09:41:49.793648Z","shell.execute_reply.started":"2024-10-26T09:41:49.782576Z"},"papermill":{"duration":0.032509,"end_time":"2024-10-25T16:52:22.58707","exception":false,"start_time":"2024-10-25T16:52:22.554561","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["clear_working_directory()"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.021882,"end_time":"2024-10-25T16:52:22.631605","exception":false,"start_time":"2024-10-25T16:52:22.609723","status":"completed"},"tags":[]},"source":["# Preprocessing Data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:41:49.79709Z","iopub.status.busy":"2024-10-26T09:41:49.796072Z","iopub.status.idle":"2024-10-26T09:41:56.897814Z","shell.execute_reply":"2024-10-26T09:41:56.896703Z","shell.execute_reply.started":"2024-10-26T09:41:49.797022Z"},"papermill":{"duration":10.535092,"end_time":"2024-10-25T16:52:33.2466","exception":false,"start_time":"2024-10-25T16:52:22.711508","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["#Reading data:\n","\n","df = pd.read_csv(\"/kaggle/input/3.Time_domain_subsamples/KU-HAR_time_domain_subsamples_20750x300.csv\",header=None)\n","dff = df.values\n","signals = dff[:, 0: 1800] #These are the time-domian subsamples (signals) \n","signals = np.array(signals, dtype=np.float32)\n","labels = dff[:, 1800] #These are their associated class labels (signals)\n","\n","print_log(f\"signals shape: {signals.shape} and labels shape: {labels.shape}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:41:56.900328Z","iopub.status.busy":"2024-10-26T09:41:56.899809Z","iopub.status.idle":"2024-10-26T09:42:39.49698Z","shell.execute_reply":"2024-10-26T09:42:39.495679Z","shell.execute_reply.started":"2024-10-26T09:41:56.900272Z"},"papermill":{"duration":41.223454,"end_time":"2024-10-25T16:53:14.492451","exception":false,"start_time":"2024-10-25T16:52:33.268997","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Reshape signals into 300 samples per axis for each sensor type (acc, gyro, orientation)\n","# For accelerometer data (X, Y, Z axis)\n","acc_x = signals[:, :300]\n","acc_y = signals[:, 300:600]\n","acc_z = signals[:, 600:900]\n","gyro_x = signals[:, 900:1200]\n","gyro_y = signals[:, 1200:1500]\n","gyro_z = signals[:, 1500:1800]\n","\n","# Create a list to hold all rows of data\n","data_rows = []\n","\n","# Loop over each activity ID and create rows for each sample and axis\n","for i in range(len(labels)):\n","    activity_id = labels[i]\n","    \n","    # For each of the 300 samples, create a row with each sensor axis value\n","    for sample in range(300):\n","        data_rows.append({\"activityID\": int(activity_id), \n","                          \"acc_x\": acc_x[i, sample],\n","                         \"acc_y\" : acc_y[i, sample],\n","                         \"acc_z\" : acc_z[i, sample],\n","                         \"gyro_x\" : gyro_x[i, sample],\n","                         \"gyro_y\" : gyro_y[i, sample],\n","                         \"gyro_z\" : gyro_z[i, sample]})\n","\n","# Create a DataFrame from the expanded data\n","new_df = pd.DataFrame(data_rows)\n","print_log(\"The basic new dataframe is genarated\")"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.021737,"end_time":"2024-10-25T16:53:14.53646","exception":false,"start_time":"2024-10-25T16:53:14.514723","status":"completed"},"tags":[]},"source":["# **Standardize the dataframe** \n","Standardize the record count across all activities for each subject."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:42:39.498865Z","iopub.status.busy":"2024-10-26T09:42:39.49843Z","iopub.status.idle":"2024-10-26T09:42:39.505018Z","shell.execute_reply":"2024-10-26T09:42:39.504113Z","shell.execute_reply.started":"2024-10-26T09:42:39.498815Z"},"papermill":{"duration":0.030118,"end_time":"2024-10-25T16:53:14.588884","exception":false,"start_time":"2024-10-25T16:53:14.558766","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# activity_counts = new_df.groupby('activityID').size()\n","\n","# # Print the counts for each activityID\n","# print(activity_counts)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:42:39.506856Z","iopub.status.busy":"2024-10-26T09:42:39.506436Z","iopub.status.idle":"2024-10-26T09:42:39.614231Z","shell.execute_reply":"2024-10-26T09:42:39.613228Z","shell.execute_reply.started":"2024-10-26T09:42:39.50679Z"},"papermill":{"duration":0.141145,"end_time":"2024-10-25T16:53:14.752962","exception":false,"start_time":"2024-10-25T16:53:14.611817","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Count the number of records for each activityID\n","activity_counts = new_df.groupby('activityID').size()\n","\n","# Find the minimum count of records\n","min_records = activity_counts.min()\n","\n","# Print the minimum count and the corresponding activityID(s)\n","min_activity_ids = activity_counts[activity_counts == min_records].index.tolist()\n","\n","print_log(f\"Minimum number of records: {min_records}\")\n","print_log(f\"Activity IDs with minimum records: {min_activity_ids}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:42:39.619535Z","iopub.status.busy":"2024-10-26T09:42:39.619133Z","iopub.status.idle":"2024-10-26T09:42:40.113001Z","shell.execute_reply":"2024-10-26T09:42:40.112083Z","shell.execute_reply.started":"2024-10-26T09:42:39.619482Z"},"papermill":{"duration":0.52007,"end_time":"2024-10-25T16:53:15.295509","exception":false,"start_time":"2024-10-25T16:53:14.775439","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Count the number of records for each activityID\n","activity_counts = new_df.groupby('activityID').size()\n","\n","# Find the minimum count of records\n","min_records = activity_counts.min()\n","\n","# Initialize a list to store the resampled DataFrames\n","equalized_dfs = []\n","\n","# Iterate through each activityID\n","for activity_id in activity_counts.index:\n","    # Filter the DataFrame for the current activityID\n","    activity_data = new_df[new_df['activityID'] == activity_id]\n","    \n","    # Resample the data to the minimum count with replacement\n","    resampled_data = activity_data.sample(n=min_records, replace=True, random_state=42)\n","    \n","    # Append the resampled DataFrame to the list\n","    equalized_dfs.append(resampled_data)\n","\n","# Concatenate all resampled DataFrames into one\n","equalized_df = pd.concat(equalized_dfs, ignore_index=True)\n","print_log(\"The dataframe is Standardized\")\n","\n","# Show the final equalized DataFrame\n","print(equalized_df.groupby('activityID').size())  # Check the counts per activityID\n","\n","# Reassign the dataframe\n","new_df = equalized_df\n","new_df"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.023986,"end_time":"2024-10-25T16:53:15.344031","exception":false,"start_time":"2024-10-25T16:53:15.320045","status":"completed"},"tags":[]},"source":["# Visualize"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:42:40.115505Z","iopub.status.busy":"2024-10-26T09:42:40.114631Z","iopub.status.idle":"2024-10-26T09:42:40.129366Z","shell.execute_reply":"2024-10-26T09:42:40.128294Z","shell.execute_reply.started":"2024-10-26T09:42:40.115448Z"},"papermill":{"duration":0.041234,"end_time":"2024-10-25T16:53:15.408391","exception":false,"start_time":"2024-10-25T16:53:15.367157","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def visualize_activity_data(activity_id, df):\n","    # Filter data for the specific activity ID\n","    activity_data = df[df['activityID'] == activity_id]\n","    \n","    # Ensure there are enough samples for plotting (300 samples)\n","    if len(activity_data) < 300:\n","        warn_log(f\"Not enough samples for activity ID {activity_id}\")\n","        return\n","    \n","    # Extract sensor data for the first 300 samples for each axis\n","    acc_x = activity_data['acc_x'].values[:300]\n","    acc_y = activity_data['acc_y'].values[:300]\n","    acc_z = activity_data['acc_z'].values[:300]\n","    gyro_x = activity_data['gyro_x'].values[:300]\n","    gyro_y = activity_data['gyro_y'].values[:300]\n","    gyro_z = activity_data['gyro_z'].values[:300]\n","    \n","    # Generate time array (assuming each sample is 0.01 seconds apart)\n","    time = np.linspace(0.01, 3, 300)\n","    \n","    # Create figure for acceleration\n","    plt.figure(figsize=(10, 6))\n","    \n","    # Plot accelerometer X, Y, Z axis on the same plot\n","    plt.plot(time, acc_x, color='b', label='Accelerometer X')\n","    plt.plot(time, acc_y, color='g', label='Accelerometer Y')\n","    plt.plot(time, acc_z, color='r', label='Accelerometer Z')\n","    \n","    plt.title('Accelerometer Data (X, Y, Z)')\n","    plt.xlabel('time (s)')\n","    plt.ylabel('Acceleration (m/s^2)')\n","    plt.grid(True)\n","    plt.legend()\n","    print_log(\"Accelerometer plot is created\")\n","    \n","    # Show the first plot\n","    plt.tight_layout()\n","    plt.show()\n","    \n","    # Create figure for gyroscope\n","    plt.figure(figsize=(10, 6))\n","    \n","    # Plot gyroscope X, Y, Z axis on the same plot\n","    plt.plot(time, gyro_x, color='b', label='Gyroscope X')\n","    plt.plot(time, gyro_y, color='g', label='Gyroscope Y')\n","    plt.plot(time, gyro_z, color='r', label='Gyroscope Z')\n","    \n","    plt.title('Gyroscope Data (X, Y, Z)')\n","    plt.xlabel('time (s)')\n","    plt.ylabel('Angular rotation (rad/s)')\n","    plt.grid(True)\n","    plt.legend()\n","    print_log(\"Gyroscope plot is created\")\n","    \n","    # Show the second plot\n","    plt.tight_layout()\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:42:40.131124Z","iopub.status.busy":"2024-10-26T09:42:40.130731Z","iopub.status.idle":"2024-10-26T09:42:41.559225Z","shell.execute_reply":"2024-10-26T09:42:41.558173Z","shell.execute_reply.started":"2024-10-26T09:42:40.131082Z"},"papermill":{"duration":1.091067,"end_time":"2024-10-25T16:53:16.523065","exception":false,"start_time":"2024-10-25T16:53:15.431998","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["visualize_activity_data(activity_id=9, df=new_df)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.030186,"end_time":"2024-10-25T16:53:16.583888","exception":false,"start_time":"2024-10-25T16:53:16.553702","status":"completed"},"tags":[]},"source":["# Transform the data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:42:41.56127Z","iopub.status.busy":"2024-10-26T09:42:41.560824Z","iopub.status.idle":"2024-10-26T09:42:41.570543Z","shell.execute_reply":"2024-10-26T09:42:41.569349Z","shell.execute_reply.started":"2024-10-26T09:42:41.561224Z"},"papermill":{"duration":0.040979,"end_time":"2024-10-25T16:53:16.654513","exception":false,"start_time":"2024-10-25T16:53:16.613534","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Function to create a DataFrame with original values, derivatives, and Fourier transforms\n","def create_transformed_df(df):\n","    transformed_data = {}\n","\n","    # Include original values for each column, including activityID\n","    transformed_data['activityID'] = df['activityID']\n","    for col in ['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z']:\n","        transformed_data[col] = df[col]  # Add original data\n","\n","        # Compute first derivative\n","        transformed_data[f'{col}_fd'] = np.gradient(df[col])\n","        \n","        # Compute second derivative\n","        transformed_data[f'{col}_sd'] = np.gradient(transformed_data[f'{col}_fd'])\n","        \n","        # Compute Fourier Transform (absolute values to keep real magnitudes)\n","        transformed_data[f'{col}_fourier'] = np.abs(np.fft.fft(df[col]))\n","\n","    # Create the final DataFrame\n","    transformed_df = pd.DataFrame(transformed_data)\n","    print_log(\"The first and secind derivatives, and Fourier transforms values of sensor data added in the dataframe\")\n","\n","    return transformed_df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:42:41.572418Z","iopub.status.busy":"2024-10-26T09:42:41.572054Z","iopub.status.idle":"2024-10-26T09:42:41.5881Z","shell.execute_reply":"2024-10-26T09:42:41.587058Z","shell.execute_reply.started":"2024-10-26T09:42:41.572383Z"},"papermill":{"duration":0.042362,"end_time":"2024-10-25T16:53:16.728807","exception":false,"start_time":"2024-10-25T16:53:16.686445","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def assign_subject_ids(df):\n","    # Get unique activity IDs in the dataset\n","    activity_ids = df['activityID'].unique()\n","    \n","    # Initialize list to hold subject IDs\n","    subject_ids = []\n","    \n","    # Loop over each activity ID\n","    for activity_id in activity_ids:\n","        # Get subset for the specific activity\n","        activity_df = df[df['activityID'] == activity_id]\n","        \n","        # Number of records for the activity\n","        n_records = len(activity_df)\n","        \n","        # Use round-robin assignment of subject IDs\n","        subject_id_repeated = np.tile(np.arange(1, 11), n_records // 10 + 1)[:n_records]\n","        \n","        # Append these subject IDs to the list\n","        subject_ids.extend(subject_id_repeated)\n","    \n","    # Add subjectID to the DataFrame\n","    df['subjectID'] = subject_ids\n","    print_log(\"subjectID is assigned for every record\")\n","    return df\n","\n","\n","def assign_subject_ids_random(df):\n","    # Get unique activity IDs in the dataset\n","    activity_ids = df['activityID'].unique()\n","    \n","    # Initialize list to hold subject IDs\n","    subject_ids = []\n","    \n","    # Loop over each activity ID\n","    for activity_id in activity_ids:\n","        # Get subset for the specific activity\n","        activity_df = df[df['activityID'] == activity_id]\n","        \n","        # Number of records for the activity\n","        n_records = len(activity_df)\n","        \n","        # Randomly assign subject IDs from 1 to 10\n","        subject_id_random = np.random.choice(np.arange(1, 11), n_records, replace=True)\n","        \n","        # Append these subject IDs to the list\n","        subject_ids.extend(subject_id_random)\n","    \n","    # Add subjectID to the DataFrame\n","    df['subjectID'] = subject_ids\n","    print_log(\"Random subjectID is assigned for every record\")\n","    return df\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:42:41.589699Z","iopub.status.busy":"2024-10-26T09:42:41.58933Z","iopub.status.idle":"2024-10-26T09:42:47.288661Z","shell.execute_reply":"2024-10-26T09:42:47.287706Z","shell.execute_reply.started":"2024-10-26T09:42:41.589665Z"},"papermill":{"duration":5.758772,"end_time":"2024-10-25T16:53:22.517502","exception":false,"start_time":"2024-10-25T16:53:16.75873","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# cretae transformed df it include first and second derivative and fourior, Assign the subject IDs from 0 to 10\n","transformed_df = create_transformed_df(new_df)\n","final_df = assign_subject_ids(transformed_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:42:47.290176Z","iopub.status.busy":"2024-10-26T09:42:47.28985Z","iopub.status.idle":"2024-10-26T09:42:47.319324Z","shell.execute_reply":"2024-10-26T09:42:47.318409Z","shell.execute_reply.started":"2024-10-26T09:42:47.290143Z"},"papermill":{"duration":0.064077,"end_time":"2024-10-25T16:53:22.612164","exception":false,"start_time":"2024-10-25T16:53:22.548087","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["activity_counts = final_df.groupby('activityID').size()\n","\n","# Print the counts for each activityID\n","print_log(activity_counts)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:42:47.32166Z","iopub.status.busy":"2024-10-26T09:42:47.320806Z","iopub.status.idle":"2024-10-26T09:42:47.331865Z","shell.execute_reply":"2024-10-26T09:42:47.330796Z","shell.execute_reply.started":"2024-10-26T09:42:47.321612Z"},"papermill":{"duration":0.044813,"end_time":"2024-10-25T16:53:22.688447","exception":false,"start_time":"2024-10-25T16:53:22.643634","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def split_data(df, train_ratio=0.7, test_ratio=0.2, validation_ratio=0.1):\n","    # Validate that the ratios sum to 1\n","    if not (train_ratio + test_ratio + validation_ratio != 1):\n","        error_log(\"Train : Test: Validation => Ratios must sum to 1.\")\n","        raise ValueError(\"Ratios must sum to 1.\")\n","    \n","    # Initialize empty DataFrames for train, test, and validation\n","    train_df = pd.DataFrame(columns=df.columns)\n","    test_df = pd.DataFrame(columns=df.columns)\n","    validation_df = pd.DataFrame(columns=df.columns)\n","\n","    # Iterate over each group of (activityID, subjectID)\n","    for (activity_id, subject_id), group in df.groupby(['activityID', 'subjectID']):\n","        n = len(group)\n","        \n","        if n == 0:\n","            warn_log(f\"The group of activity_id : {activity_id} and subject_id : {subject_id} is empty\")\n","            continue  # Skip empty groups\n","        \n","        # Shuffle the group for randomness (if needed)\n","        group = group.sample(frac=1).reset_index(drop=True)\n","\n","        # Calculate indices for splitting\n","        train_end = int(train_ratio * n)\n","        test_end = train_end + int(test_ratio * n)\n","\n","        # Split into train, test, validation based on indices\n","        train_data = group.iloc[:train_end]\n","        test_data = group.iloc[train_end:test_end]\n","        validation_data = group.iloc[test_end:]\n","\n","        # Concatenate only if the DataFrames are not empty\n","        train_df = pd.concat([train_df, train_data], ignore_index=True)\n","        test_df = pd.concat([test_df, test_data], ignore_index=True)\n","        validation_df = pd.concat([validation_df, validation_data], ignore_index=True)\n","\n","    print_log(\"Data is split into three parts: train, test, and validation.\")\n","\n","    return train_df, test_df, validation_df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:42:47.333917Z","iopub.status.busy":"2024-10-26T09:42:47.333263Z","iopub.status.idle":"2024-10-26T09:42:53.556933Z","shell.execute_reply":"2024-10-26T09:42:53.555728Z","shell.execute_reply.started":"2024-10-26T09:42:47.333871Z"},"papermill":{"duration":6.641311,"end_time":"2024-10-25T16:53:29.360253","exception":false,"start_time":"2024-10-25T16:53:22.718942","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["train_df, test_df, validation_df = split_data(final_df)\n","\n","print_log(f\"Train data shape : {train_df.shape}\")\n","print_log(f\"Test data shape: {test_df.shape}\")\n","print_log(f\"Validation data shape: {validation_df.shape}\")\n","\n","# # Define the file paths for saving the CSV files\n","# train_csv_path = '/kaggle/working/train_data.csv'\n","# test_csv_path = '/kaggle/working/test_data.csv'\n","# validation_csv_path = '/kaggle/working/validation_data.csv'\n","\n","# # Save the DataFrames to CSV files\n","# train_df.to_csv(train_csv_path, index=False)\n","# print_log(\"train_data.csv file is created\")\n","# test_df.to_csv(test_csv_path, index=False)\n","# print_log(\"test_data.csv file is created\")\n","# validation_df.to_csv(validation_csv_path, index=False)\n","# print_log(\"validation_data.csv file is created\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:42:53.55875Z","iopub.status.busy":"2024-10-26T09:42:53.55842Z","iopub.status.idle":"2024-10-26T09:42:53.563378Z","shell.execute_reply":"2024-10-26T09:42:53.56223Z","shell.execute_reply.started":"2024-10-26T09:42:53.558714Z"},"papermill":{"duration":0.037192,"end_time":"2024-10-25T16:53:29.428366","exception":false,"start_time":"2024-10-25T16:53:29.391174","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# train_df.groupby('activityID').size()"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.029354,"end_time":"2024-10-25T16:53:29.487599","exception":false,"start_time":"2024-10-25T16:53:29.458245","status":"completed"},"tags":[]},"source":["# Visualizing Activity Distribution and Understanding Variability"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:42:53.565003Z","iopub.status.busy":"2024-10-26T09:42:53.564688Z","iopub.status.idle":"2024-10-26T09:42:55.707369Z","shell.execute_reply":"2024-10-26T09:42:55.706123Z","shell.execute_reply.started":"2024-10-26T09:42:53.564968Z"},"papermill":{"duration":2.122848,"end_time":"2024-10-25T16:53:31.640571","exception":false,"start_time":"2024-10-25T16:53:29.517723","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Create a DataFrame from your groupby result\n","activity_distribution = train_df.groupby(['activityID', 'subjectID']).size().reset_index(name='count')\n","\n","# Plotting\n","plt.figure(figsize=(12, 6))\n","sns.barplot(data=activity_distribution, x='activityID', y='count', hue='subjectID')\n","plt.title('Activity Distribution Across Subjects')\n","plt.xlabel('Activity ID')\n","plt.ylabel('Count')\n","plt.legend(title='Subject ID')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:42:55.709478Z","iopub.status.busy":"2024-10-26T09:42:55.709113Z","iopub.status.idle":"2024-10-26T09:42:56.016576Z","shell.execute_reply":"2024-10-26T09:42:56.015478Z","shell.execute_reply.started":"2024-10-26T09:42:55.709444Z"},"papermill":{"duration":0.34519,"end_time":"2024-10-25T16:53:32.017797","exception":false,"start_time":"2024-10-25T16:53:31.672607","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["activity_variability = train_df.groupby('activityID')['subjectID'].value_counts().unstack()\n","variability_std = activity_variability.std(axis=1)\n","print_log(f\" activity variability : {variability_std} \")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:42:56.018431Z","iopub.status.busy":"2024-10-26T09:42:56.018074Z","iopub.status.idle":"2024-10-26T09:42:56.028672Z","shell.execute_reply":"2024-10-26T09:42:56.027446Z","shell.execute_reply.started":"2024-10-26T09:42:56.018394Z"},"papermill":{"duration":0.044125,"end_time":"2024-10-25T16:53:32.095277","exception":false,"start_time":"2024-10-25T16:53:32.051152","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def generate_sequence_list(df, sequence_length=500, overlap=20):\n","    # Initialize the main list to hold all sequences for all activities\n","    imu_data_sequence = []\n","\n","    # Group by activityID to handle each activity separately\n","    grouped_by_activity = df.groupby('activityID')\n","\n","    # Iterate over each activity group\n","    for activity_id, activity_group in grouped_by_activity:\n","        # Initialize a list for this activity\n","        activity_data_sequence = []\n","\n","        # Group by subjectID within the activity group\n","        grouped_by_subject = activity_group.groupby('subjectID')\n","        \n","        # Iterate over each subject group within the activity\n","        for subject_id, subject_group in grouped_by_subject:\n","            # Extract the data values excluding activityID and subjectID\n","            data_values = subject_group.drop(columns=['activityID', 'subjectID']).values\n","            \n","            # Calculate the number of sequences\n","            num_samples = len(data_values)\n","            num_sequences = (num_samples - sequence_length) // overlap + 1\n","            \n","            # Create a list for this subject's sequences\n","            subject_sequences = []\n","            \n","            # Generate sequences for this subject\n","            for i in range(num_sequences):\n","                sequence_start = i * overlap\n","                sequence_end = sequence_start + sequence_length\n","                if sequence_end <= num_samples:\n","                    # Append the sequence to the subject's sequence list\n","                    subject_sequences.append(data_values[sequence_start:sequence_end])\n","            \n","            # Add the subject sequences to the activity's list\n","            activity_data_sequence.append(subject_sequences)\n","            debug_log(f\"The sequences are generated for subjectID: {subject_id}\")\n","        \n","        # After processing all subjects, print activity completion\n","        print_log(f\"The sequences are generated for activityID: {activity_id}\")\n","\n","        # Add the activity data sequence to the main list\n","        imu_data_sequence.append(activity_data_sequence)\n","\n","    return imu_data_sequence\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:42:56.03016Z","iopub.status.busy":"2024-10-26T09:42:56.029835Z","iopub.status.idle":"2024-10-26T09:42:57.496445Z","shell.execute_reply":"2024-10-26T09:42:57.495364Z","shell.execute_reply.started":"2024-10-26T09:42:56.030125Z"},"papermill":{"duration":1.54724,"end_time":"2024-10-25T16:53:33.673405","exception":false,"start_time":"2024-10-25T16:53:32.126165","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Generate sequences for each DataFrame\n","imu_data_sequence_train = generate_sequence_list(train_df, SEQ_LENTH, WINDOW_SIZE)\n","imu_data_sequence_test = generate_sequence_list(test_df, SEQ_LENTH, WINDOW_SIZE)\n","imu_data_sequence_validation = generate_sequence_list(validation_df, SEQ_LENTH, WINDOW_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:42:57.498434Z","iopub.status.busy":"2024-10-26T09:42:57.497992Z","iopub.status.idle":"2024-10-26T09:42:57.505762Z","shell.execute_reply":"2024-10-26T09:42:57.50466Z","shell.execute_reply.started":"2024-10-26T09:42:57.498394Z"},"papermill":{"duration":0.04328,"end_time":"2024-10-25T16:53:33.750693","exception":false,"start_time":"2024-10-25T16:53:33.707413","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["len(imu_data_sequence_train[1][3])"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.034271,"end_time":"2024-10-25T16:53:33.819669","exception":false,"start_time":"2024-10-25T16:53:33.785398","status":"completed"},"tags":[]},"source":["# Store data as a pickle"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:42:57.507901Z","iopub.status.busy":"2024-10-26T09:42:57.507458Z","iopub.status.idle":"2024-10-26T09:42:57.51972Z","shell.execute_reply":"2024-10-26T09:42:57.518711Z","shell.execute_reply.started":"2024-10-26T09:42:57.507848Z"},"papermill":{"duration":0.043613,"end_time":"2024-10-25T16:53:33.897725","exception":false,"start_time":"2024-10-25T16:53:33.854112","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# # # Store the imu data as a pickle\n","# pickle_imu_data_directory = \"/kaggle/working/pickle_imu_data\"\n","\n","# # # Create the directory if it doesn't exist\n","# os.makedirs(pickle_imu_data_directory, exist_ok=True)\n","\n","# # # File paths for pickle files\n","# # train_pickle_path = os.path.join(pickle_imu_data_directory, 'imu_data_sequence_train.pkl')\n","# test_pickle_path = os.path.join(pickle_imu_data_directory, 'imu_data_sequence_test.pkl')\n","# # validation_pickle_path = os.path.join(pickle_imu_data_directory, 'imu_data_sequence_validation.pkl')\n","\n","# # # Save the training data sequence to a pickle file\n","# # with open(train_pickle_path, 'wb') as train_file:\n","# #     pickle.dump(imu_data_sequence_train[0:8], train_file)\n","# # print_log(\"Train Pickle file has been saved successfully\")\n","\n","# # # Save the testing data sequence to a pickle file\n","# with open(test_pickle_path, 'wb') as test_file:\n","#     pickle.dump(imu_data_sequence_test[0:8], test_file)\n","# print_log(\"Test Pickle file has been saved successfully\")\n","\n","# # # Save the validation data sequence to a pickle file\n","# # with open(validation_pickle_path, 'wb') as validation_file:\n","# #     pickle.dump(imu_data_sequence_validation[0:8], validation_file)\n","# # print_log(\"Validation Pickle file has been saved successfully\")\n","\n","# # print(\"Pickle files have been saved successfully.\")\n","\n"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.033564,"end_time":"2024-10-25T16:53:33.965999","exception":false,"start_time":"2024-10-25T16:53:33.932435","status":"completed"},"tags":[]},"source":["# Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:42:57.521915Z","iopub.status.busy":"2024-10-26T09:42:57.521533Z","iopub.status.idle":"2024-10-26T09:42:57.554464Z","shell.execute_reply":"2024-10-26T09:42:57.553321Z","shell.execute_reply.started":"2024-10-26T09:42:57.521875Z"},"papermill":{"duration":0.068012,"end_time":"2024-10-25T16:53:34.06674","exception":false,"start_time":"2024-10-25T16:53:33.998728","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","    def __init__(self, k, d_model, seq_len):\n","        super().__init__()\n","        \n","        self.embedding = nn.Parameter(torch.zeros([k, d_model], dtype=torch.float), requires_grad=True)\n","        nn.init.xavier_uniform_(self.embedding, gain=1)\n","        self.positions = torch.tensor([i for i in range(seq_len)], requires_grad=False).unsqueeze(1).repeat(1, k)\n","        s = 0.0\n","        interval = seq_len / k\n","        mu = []\n","        for _ in range(k):\n","            mu.append(nn.Parameter(torch.tensor(s, dtype=torch.float), requires_grad=True))\n","            s = s + interval\n","        self.mu = nn.Parameter(torch.tensor(mu, dtype=torch.float).unsqueeze(0), requires_grad=True)\n","        self.sigma = nn.Parameter(torch.tensor([torch.tensor([50.0], dtype=torch.float, requires_grad=True) for _ in range(k)]).unsqueeze(0))\n","        \n","    def normal_pdf(self, pos, mu, sigma):\n","        a = pos - mu\n","        log_p = -1*torch.mul(a, a)/(2*(sigma**2)) - torch.log(sigma)\n","        return torch.nn.functional.softmax(log_p, dim=1)\n","\n","    def forward(self, inputs):\n","        pdfs = self.normal_pdf(self.positions, self.mu, self.sigma)\n","        pos_enc = torch.matmul(pdfs, self.embedding)\n","        \n","        return inputs + pos_enc.unsqueeze(0).repeat(inputs.size(0), 1, 1)\n","\n","class TransformerEncoderLayer(nn.Module):\n","    def __init__(self, d_model, heads, _heads, dropout, seq_len):\n","        super(TransformerEncoderLayer, self).__init__()\n","        \n","        self.attention = nn.MultiheadAttention(d_model, heads, batch_first=True)\n","        self._attention = nn.MultiheadAttention(seq_len, _heads, batch_first=True)\n","        \n","        self.attn_norm = nn.LayerNorm(d_model)\n","        \n","        self.cnn_units = 1\n","        \n","        self.cnn = nn.Sequential(\n","            nn.Conv2d(1, self.cnn_units, (1, 1)),\n","            nn.BatchNorm2d(self.cnn_units),\n","            nn.Dropout(dropout),\n","            nn.ReLU(),\n","            nn.Conv2d(self.cnn_units, self.cnn_units, (3, 3), padding=1),\n","            nn.BatchNorm2d(self.cnn_units),\n","            nn.Dropout(dropout),\n","            nn.ReLU(),\n","            nn.Conv2d(self.cnn_units, 1, (5, 5), padding=2),\n","            nn.BatchNorm2d(1),\n","            nn.Dropout(dropout),\n","            nn.ReLU()\n","        )\n","        \n","        self.final_norm = nn.LayerNorm(d_model)\n","\n","    def forward(self, src, src_mask=None):\n","        src = self.attn_norm(src + self.attention(src, src, src)[0] + self._attention(src.transpose(-1, -2), src.transpose(-1, -2), src.transpose(-1, -2))[0].transpose(-1, -2))\n","        \n","        src = self.final_norm(src + self.cnn(src.unsqueeze(dim=1)).squeeze(dim=1))\n","            \n","        return src\n","\n","class TransformerEncoder(nn.Module):\n","    def __init__(self, d_model, heads, _heads, seq_len, num_layer=2, dropout=0.1):\n","        super(TransformerEncoder, self).__init__()\n","\n","        self.layers = nn.ModuleList()\n","        for i in range(num_layer):\n","            self.layers.append(TransformerEncoderLayer(d_model, heads, _heads, dropout, seq_len))\n","\n","    def forward(self, src):\n","        for layer in self.layers:\n","            src = layer(src)\n","\n","        return src\n","\n","class Transformer(nn.Module):\n","    def __init__(self, num_layer, d_model, k, heads, _heads, seq_len, trg_len, dropout):\n","        super(Transformer, self).__init__()\n","\n","        self.pos_encoding = PositionalEncoding(k, d_model, seq_len)\n","\n","        self.encoder = TransformerEncoder(d_model, heads, _heads, seq_len, num_layer, dropout)\n","\n","    def forward(self, inputs):\n","        encoded_inputs = self.pos_encoding(inputs)\n","\n","        return self.encoder(encoded_inputs)\n","\n","class Model(nn.Module):\n","    def __init__(self, feature_count, l, trg_len, num_classes):\n","        super(Model, self).__init__()\n","        \n","        self.imu_transformer = Transformer(5, feature_count, 100, 4, 4, l, trg_len, 0.1)\n","        \n","        self.linear_imu = nn.Sequential(\n","            nn.Linear(feature_count*l, (feature_count*l)//2),\n","            nn.ReLU(),\n","            nn.Dropout(0.1),\n","            nn.Linear((feature_count*l)//2, trg_len),\n","            nn.ReLU()\n","        )\n","        \n","        # Batch normalization and dropout layers\n","        self.batch_norm = nn.BatchNorm1d(trg_len)\n","        self.dropout = nn.Dropout(0.5)\n","        \n","        # Classifier layer\n","        self.classifier = nn.Linear(trg_len, num_classes)\n","\n","    def forward(self, inputs):\n","        \n","        embedding = self.linear_imu(torch.flatten(self.imu_transformer(inputs), start_dim=1, end_dim=2))\n","        \n","        # Apply batch normalization\n","        embedding = self.batch_norm(embedding)\n","        \n","        # Apply dropout\n","        embedding = self.dropout(embedding)\n","        \n","        # Get class scores\n","        class_scores = self.classifier(embedding)\n","        \n","        return class_scores, embedding"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.03319,"end_time":"2024-10-25T16:53:34.133584","exception":false,"start_time":"2024-10-25T16:53:34.100394","status":"completed"},"tags":[]},"source":["# If you have GPU"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:42:57.556302Z","iopub.status.busy":"2024-10-26T09:42:57.555873Z","iopub.status.idle":"2024-10-26T09:42:57.567714Z","shell.execute_reply":"2024-10-26T09:42:57.566627Z","shell.execute_reply.started":"2024-10-26T09:42:57.556258Z"},"papermill":{"duration":0.045144,"end_time":"2024-10-25T16:53:34.212143","exception":false,"start_time":"2024-10-25T16:53:34.166999","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["torch.set_default_tensor_type('torch.cuda.FloatTensor')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:42:57.576499Z","iopub.status.busy":"2024-10-26T09:42:57.576088Z","iopub.status.idle":"2024-10-26T09:42:57.582811Z","shell.execute_reply":"2024-10-26T09:42:57.581796Z","shell.execute_reply.started":"2024-10-26T09:42:57.576461Z"},"papermill":{"duration":0.042738,"end_time":"2024-10-25T16:53:34.292228","exception":false,"start_time":"2024-10-25T16:53:34.24949","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["batch_size=BATCH_SIZE\n","epoch_batch_count=EPOCH_BATCH_COUNT\n","imu_l=SEQ_LENTH    #sequence length\n","imu_feature_count=IMU_FEATURE_COUNT\n","trg_len=128 # this will be the size of feature embedding\n","classes=CLASSES"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:42:57.584387Z","iopub.status.busy":"2024-10-26T09:42:57.584018Z","iopub.status.idle":"2024-10-26T09:42:57.60544Z","shell.execute_reply":"2024-10-26T09:42:57.604322Z","shell.execute_reply.started":"2024-10-26T09:42:57.584351Z"},"papermill":{"duration":0.052428,"end_time":"2024-10-25T16:53:34.378156","exception":false,"start_time":"2024-10-25T16:53:34.325728","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["best_model_save_path = '/kaggle/working/best_models'\n","checkpoint_save_path = '/kaggle/working/checkpoints'\n","\n","subprocess.run(f\"mkdir {best_model_save_path}\", shell=True)\n","subprocess.run(f\"mkdir {checkpoint_save_path}\", shell=True)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.041967,"end_time":"2024-10-25T16:53:34.467186","exception":false,"start_time":"2024-10-25T16:53:34.425219","status":"completed"},"tags":[]},"source":["# Train Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:42:57.607024Z","iopub.status.busy":"2024-10-26T09:42:57.606725Z","iopub.status.idle":"2024-10-26T09:42:57.614303Z","shell.execute_reply":"2024-10-26T09:42:57.613291Z","shell.execute_reply.started":"2024-10-26T09:42:57.606992Z"},"papermill":{"duration":0.051219,"end_time":"2024-10-25T16:53:34.552015","exception":false,"start_time":"2024-10-25T16:53:34.500796","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# class TrainDataset(Dataset):\n","#     def __init__(self, training_data, batch_size, epoch_batch_count):\n","#         self.training_data = training_data\n","#         self.batch_size = batch_size\n","#         self.epoch_batch_count = epoch_batch_count\n","\n","#     def __len__(self):\n","#         return self.batch_size * self.epoch_batch_count\n","\n","#     def __getitem__(self, idx):\n","#         while True:\n","#             try:\n","#                 genuine_user_idx = np.random.randint(0, len(self.training_data))\n","#                 imposter_user_idx = np.random.randint(0, len(self.training_data))\n","                \n","#                 # Ensure imposter_user_idx is different from genuine_user_idx\n","#                 while imposter_user_idx == genuine_user_idx:\n","#                     imposter_user_idx = np.random.randint(0, len(self.training_data))\n","                \n","#                 # Validate the lengths of genuine_user and imposter_user data\n","#                 if len(self.training_data[genuine_user_idx]) == 0 or len(self.training_data[imposter_user_idx]) == 0:\n","#                     raise ValueError(\"Empty user data detected.\")\n","                \n","#                 genuine_sess_1 = np.random.randint(0, len(self.training_data[genuine_user_idx]))\n","#                 genuine_sess_2 = np.random.randint(0, len(self.training_data[genuine_user_idx]))\n","                \n","#                 # Ensure genuine_sess_2 is different from genuine_sess_1\n","#                 while genuine_sess_2 == genuine_sess_1:\n","#                     genuine_sess_2 = np.random.randint(0, len(self.training_data[genuine_user_idx]))\n","                \n","#                 # Validate the lengths of genuine_sess_1 and genuine_sess_2 data\n","#                 if len(self.training_data[genuine_user_idx][genuine_sess_1]) == 0 or len(self.training_data[genuine_user_idx][genuine_sess_2]) == 0:\n","#                     raise ValueError(\"Empty session data detected.\")\n","                \n","#                 imposter_sess = np.random.randint(0, len(self.training_data[imposter_user_idx]))\n","                \n","#                 # Validate the length of imposter_sess data\n","#                 if len(self.training_data[imposter_user_idx][imposter_sess]) == 0:\n","#                     raise ValueError(\"Empty imposter session data detected.\")\n","                \n","#                 genuine_seq_1 = np.random.randint(0, len(self.training_data[genuine_user_idx][genuine_sess_1]))\n","#                 genuine_seq_2 = np.random.randint(0, len(self.training_data[genuine_user_idx][genuine_sess_2]))\n","#                 imposter_seq = np.random.randint(0, len(self.training_data[imposter_user_idx][imposter_sess]))\n","# #                 debug_log(f\"{genuine_user_idx}, {genuine_sess_1}, {genuine_sess_2}, {imposter_user_idx}, {imposter_sess}\")\n","#                 anchor = self.training_data[genuine_user_idx][genuine_sess_1][genuine_seq_1]\n","#                 positive = self.training_data[genuine_user_idx][genuine_sess_2][genuine_seq_2]\n","#                 negative = self.training_data[imposter_user_idx][imposter_sess][imposter_seq]\n","\n","#                 return anchor, positive, negative, genuine_user_idx, imposter_user_idx\n","            \n","#             except ValueError as e:\n","#                 error_log(f\"Encountered ValueError: {str(e)}. Retrying with new indices.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:42:57.616342Z","iopub.status.busy":"2024-10-26T09:42:57.615857Z","iopub.status.idle":"2024-10-26T09:42:57.635259Z","shell.execute_reply":"2024-10-26T09:42:57.634165Z","shell.execute_reply.started":"2024-10-26T09:42:57.616287Z"},"trusted":true},"outputs":[],"source":["class TrainDataset(Dataset):\n","    def __init__(self, training_data, batch_size, epoch_batch_count, num_negatives):\n","        self.training_data = training_data\n","        self.batch_size = batch_size\n","        self.epoch_batch_count = epoch_batch_count\n","        self.num_negatives = num_negatives\n","\n","    def __len__(self):\n","        return self.batch_size * self.epoch_batch_count\n","\n","    def __getitem__(self, idx):\n","        while True:\n","            try:\n","                genuine_user_idx = np.random.randint(0, len(self.training_data))\n","                imposter_user_idxs = []\n","                # Validate the lengths of genuine_user and imposter_user data\n","                if len(self.training_data[genuine_user_idx]) == 0:\n","                    raise ValueError(\"Empty user data detected.\")\n","                \n","                for i in range(0, self.num_negatives):\n","                    imposter_user_idx =  np.random.randint(0, len(self.training_data))\n","                \n","                    # Ensure imposter_user_idx is different from genuine_user_idx and diffrent form other imposter_user_idx\n","                    while imposter_user_idx == genuine_user_idx or imposter_user_idx in imposter_user_idxs:\n","                        imposter_user_idx = np.random.randint(0, len(self.training_data))\n","                \n","                    # Validate the lengths of genuine_user and imposter_user data\n","                    if len(self.training_data[imposter_user_idx]) == 0:\n","                        raise ValueError(\"Empty user data detected.\")\n","                    \n","                    imposter_user_idxs.append(imposter_user_idx)\n","                \n","                \n","                genuine_sess_1 = np.random.randint(0, len(self.training_data[genuine_user_idx]))\n","                genuine_sess_2 = np.random.randint(0, len(self.training_data[genuine_user_idx]))\n","                \n","                # Ensure genuine_sess_2 is different from genuine_sess_1\n","                while genuine_sess_2 == genuine_sess_1:\n","                    genuine_sess_2 = np.random.randint(0, len(self.training_data[genuine_user_idx]))\n","                \n","                # Validate the lengths of genuine_sess_1 and genuine_sess_2 data\n","                if len(self.training_data[genuine_user_idx][genuine_sess_1]) == 0 or len(self.training_data[genuine_user_idx][genuine_sess_2]) == 0:\n","                    raise ValueError(\"Empty session data detected.\")\n","                \n","                \n","                genuine_seq_1 = np.random.randint(0, len(self.training_data[genuine_user_idx][genuine_sess_1]))\n","                genuine_seq_2 = np.random.randint(0, len(self.training_data[genuine_user_idx][genuine_sess_2]))\n","                \n","                anchor = self.training_data[genuine_user_idx][genuine_sess_1][genuine_seq_1]\n","                positive = self.training_data[genuine_user_idx][genuine_sess_2][genuine_seq_2]\n","                negatives = []\n","                \n","                for i in imposter_user_idxs:\n","                    imposter_sess = np.random.randint(0, len(self.training_data[imposter_user_idx]))\n","                \n","                    # Validate the length of imposter_sess data\n","                    if len(self.training_data[imposter_user_idx][imposter_sess]) == 0:\n","                        raise ValueError(\"Empty imposter session data detected.\")\n","                \n","                \n","                    imposter_seq = np.random.randint(0, len(self.training_data[imposter_user_idx][imposter_sess]))\n","                    negative = self.training_data[imposter_user_idx][imposter_sess][imposter_seq]\n","                    negatives.append(negative)\n","\n","                return anchor, positive, negatives, genuine_user_idx, imposter_user_idxs\n","            \n","            except ValueError as e:\n","                error_log(f\"Encountered ValueError: {str(e)}. Retrying with new indices.\")"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.033253,"end_time":"2024-10-25T16:53:34.620411","exception":false,"start_time":"2024-10-25T16:53:34.587158","status":"completed"},"tags":[]},"source":["# Loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:42:57.637252Z","iopub.status.busy":"2024-10-26T09:42:57.63675Z","iopub.status.idle":"2024-10-26T09:42:57.648255Z","shell.execute_reply":"2024-10-26T09:42:57.647275Z","shell.execute_reply.started":"2024-10-26T09:42:57.637176Z"},"papermill":{"duration":0.044337,"end_time":"2024-10-25T16:53:34.697605","exception":false,"start_time":"2024-10-25T16:53:34.653268","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["class TripletLoss(nn.Module):\n","    def __init__(self, margin=1.0):\n","        super(TripletLoss, self).__init__()\n","        self.margin = margin\n","        \n","    def calc_euclidean(self, x1, x2):\n","        return (x1 - x2).pow(2).sum(dim=1).sqrt()\n","    \n","    def calc_cosine(self, x1, x2):\n","        dot_product_sum = (x1*x2).sum(dim=1)\n","        norm_multiply = (x1.pow(2).sum(dim=1).sqrt()) * (x2.pow(2).sum(dim=1).sqrt())\n","        return dot_product_sum / norm_multiply\n","    \n","    def calc_manhattan(self, x1, x2):\n","        return (x1-x2).abs().sum(dim=1)\n","    \n","    def forward(self, anchor, positive, negative):\n","        distance_positive = self.calc_euclidean(anchor, positive)\n","        distance_negative = self.calc_euclidean(anchor, negative)\n","        losses = torch.relu(distance_positive - distance_negative + self.margin)\n","        return losses.mean()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:42:57.650215Z","iopub.status.busy":"2024-10-26T09:42:57.649772Z","iopub.status.idle":"2024-10-26T09:42:57.664313Z","shell.execute_reply":"2024-10-26T09:42:57.663369Z","shell.execute_reply.started":"2024-10-26T09:42:57.650168Z"},"trusted":true},"outputs":[],"source":["# import torch\n","# import torch.nn as nn\n","\n","# class NPairLoss(nn.Module):\n","#     def __init__(self):\n","#         super(NPairLoss, self).__init__()\n","\n","#     def calc_cosine(self, x1, x2):\n","#         dot_product_sum = (x1 * x2).sum(dim=2)  # Change to dim=2 since x2 is now [batch_size, num_negatives, embedding_dim]\n","#         norm_multiply = (x1.pow(2).sum(dim=2).sqrt()) * (x2.pow(2).sum(dim=2).sqrt())\n","#         return dot_product_sum / norm_multiply\n","\n","#     def forward(self, anchor, positive, negatives):\n","#         # Calculate cosine similarities for the positive pair\n","#         pos_sim = self.calc_cosine(anchor.unsqueeze(1), positive.unsqueeze(1))  # Shape: (8, 1)\n","        \n","#         # Calculate cosine similarities for each negative\n","#         neg_sim = self.calc_cosine(anchor.unsqueeze(1), negatives.permute(1, 0, 2))  # Shape: (8, 4)\n","\n","#         # Exponential of the similarities\n","#         pos_exp = torch.exp(pos_sim.squeeze(1))  # Shape: (8,)\n","#         neg_exp = torch.exp(neg_sim)  # Shape: (8, 4)\n","\n","#         # Compute loss\n","#         numerator = pos_exp  # Shape: (8,)\n","#         denominator = pos_exp.unsqueeze(1) + neg_exp.sum(dim=1)  # Shape: (8, 1) + (8, 4) -> (8, 4)\n","\n","#         # Calculate loss\n","#         loss = -torch.log(numerator / denominator)\n","#         return loss.mean()  # Average loss over the batch\n","\n","# # Example usage:\n","# batch_size = 8\n","# embedding_dim = 128\n","# num_negatives = 4\n","\n","# # Example embeddings\n","# anchor_features = torch.randn(batch_size, embedding_dim)  # Shape: (8, 128)\n","# positive_features = torch.randn(batch_size, embedding_dim)  # Shape: (8, 128)\n","# negative_features = torch.randn(num_negatives, batch_size, embedding_dim)  # Shape: (4, 8, 128)\n","\n","# # Debug prints to check sizes\n","# print(f\"Anchor features shape: {anchor_features.shape}\")\n","# print(f\"Positive features shape: {positive_features.shape}\")\n","# print(f\"Negative features shape: {negative_features.shape}\")\n","        \n","\n","# # Initialize loss function\n","# loss_fn = NPairLoss()\n","# loss = loss_fn(anchor_features, positive_features, negative_features)\n","\n","# print(\"Loss:\", loss.item())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:42:57.665755Z","iopub.status.busy":"2024-10-26T09:42:57.665397Z","iopub.status.idle":"2024-10-26T09:42:57.67952Z","shell.execute_reply":"2024-10-26T09:42:57.678362Z","shell.execute_reply.started":"2024-10-26T09:42:57.66572Z"},"papermill":{"duration":0.042755,"end_time":"2024-10-25T16:53:34.773452","exception":false,"start_time":"2024-10-25T16:53:34.730697","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class NPairLoss(nn.Module):\n","    def __init__(self):\n","        super(NPairLoss, self).__init__()\n","\n","    def calc_cosine(self, x1, x2):\n","        dot_product_sum = (x1 * x2).sum(dim=2)  # Change to dim=2 since x2 is now [batch_size, num_negatives, embedding_dim]\n","        norm_multiply = (x1.pow(2).sum(dim=2).sqrt()) * (x2.pow(2).sum(dim=2).sqrt())\n","        return dot_product_sum / norm_multiply\n","\n","    def forward(self, anchor, positive, negatives):\n","        # Calculate cosine similarities for the positive pair\n","        pos_sim = self.calc_cosine(anchor.unsqueeze(1), positive.unsqueeze(1))  # Shape: (8, 1)\n","        \n","        # Calculate cosine similarities for each negative\n","        neg_sim = self.calc_cosine(anchor.unsqueeze(1), negatives.permute(1, 0, 2))  # Shape: (8, 4)\n","\n","        # Exponential of the similarities\n","        pos_exp = torch.exp(pos_sim.squeeze(1))  # Shape: (8,)\n","        neg_exp = torch.exp(neg_sim)  # Shape: (8, 4)\n","\n","        # Compute loss\n","        numerator = pos_exp  # Shape: (8,)\n","        denominator = pos_exp.unsqueeze(1) + neg_exp.sum(dim=1)  # Shape: (8, 1) + (8, 4) -> (8, 4)\n","\n","        # Calculate loss\n","        loss = -torch.log(numerator / denominator)\n","        return loss.mean()  # Average loss over the batch"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.033573,"end_time":"2024-10-25T16:53:34.840754","exception":false,"start_time":"2024-10-25T16:53:34.807181","status":"completed"},"tags":[]},"source":["# Set the classes"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:42:57.680957Z","iopub.status.busy":"2024-10-26T09:42:57.680646Z","iopub.status.idle":"2024-10-26T09:42:57.692693Z","shell.execute_reply":"2024-10-26T09:42:57.691731Z","shell.execute_reply.started":"2024-10-26T09:42:57.680922Z"},"papermill":{"duration":0.041655,"end_time":"2024-10-25T16:53:34.916309","exception":false,"start_time":"2024-10-25T16:53:34.874654","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["train_data = imu_data_sequence_train[0:CLASSES]\n","test_data = imu_data_sequence_test[0:CLASSES]\n","validation_data = imu_data_sequence_validation[0:CLASSES]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:42:57.694233Z","iopub.status.busy":"2024-10-26T09:42:57.693862Z","iopub.status.idle":"2024-10-26T09:42:57.704198Z","shell.execute_reply":"2024-10-26T09:42:57.703177Z","shell.execute_reply.started":"2024-10-26T09:42:57.694194Z"},"papermill":{"duration":0.040107,"end_time":"2024-10-25T16:53:34.989852","exception":false,"start_time":"2024-10-25T16:53:34.949745","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# len(train_data[17][9][88])\n","# train_data[17][9][88][999]"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.031657,"end_time":"2024-10-25T16:53:35.054692","exception":false,"start_time":"2024-10-25T16:53:35.023035","status":"completed"},"tags":[]},"source":["# Using only some activity classes for improve our model accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:42:57.705823Z","iopub.status.busy":"2024-10-26T09:42:57.705446Z","iopub.status.idle":"2024-10-26T09:42:57.71488Z","shell.execute_reply":"2024-10-26T09:42:57.713926Z","shell.execute_reply.started":"2024-10-26T09:42:57.705779Z"},"papermill":{"duration":0.04186,"end_time":"2024-10-25T16:53:35.129537","exception":false,"start_time":"2024-10-25T16:53:35.087677","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# input_classes = [0,1,2,4, 6, 7]\n","# train_data = [imu_data_sequence_train[i] for i in input_classes] \n","# test_data = [imu_data_sequence_test[i] for i in input_classes]  \n","# validation_data = [imu_data_sequence_validation[i] for i in input_classes] "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:42:57.716309Z","iopub.status.busy":"2024-10-26T09:42:57.715943Z","iopub.status.idle":"2024-10-26T09:42:57.766085Z","shell.execute_reply":"2024-10-26T09:42:57.764946Z","shell.execute_reply.started":"2024-10-26T09:42:57.716273Z"},"papermill":{"duration":0.367414,"end_time":"2024-10-25T16:53:35.529626","exception":false,"start_time":"2024-10-25T16:53:35.162212","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# dataset = TrainDataset(train_data, batch_size, epoch_batch_count)\n","# dataloader = DataLoader(dataset, batch_size=batch_size)\n","num_negatives = NUM_NEGATIVES  # Number of negatives to sample for each anchor\n","\n","# Instantiate dataset and dataloader\n","train_dataset = TrainDataset(train_data, batch_size, epoch_batch_count, num_negatives)\n","\n","dataloader = DataLoader(train_dataset, batch_size=batch_size)\n","\n","\n","model = Model(imu_feature_count, imu_l, trg_len,classes)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:42:57.767996Z","iopub.status.busy":"2024-10-26T09:42:57.767618Z","iopub.status.idle":"2024-10-26T09:42:57.777114Z","shell.execute_reply":"2024-10-26T09:42:57.775485Z","shell.execute_reply.started":"2024-10-26T09:42:57.767955Z"},"papermill":{"duration":1.546992,"end_time":"2024-10-25T16:53:37.110148","exception":false,"start_time":"2024-10-25T16:53:35.563156","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["loss_fn = NPairLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE,weight_decay=1e-5) # change the learning rate\n","# optimizer = optim.Adam(model.parameters(), lr=0.0005)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:42:57.779663Z","iopub.status.busy":"2024-10-26T09:42:57.779198Z","iopub.status.idle":"2024-10-26T09:42:57.785231Z","shell.execute_reply":"2024-10-26T09:42:57.784133Z","shell.execute_reply.started":"2024-10-26T09:42:57.779607Z"},"papermill":{"duration":0.08433,"end_time":"2024-10-25T16:53:37.229805","exception":false,"start_time":"2024-10-25T16:53:37.145475","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["g_eer = math.inf\n","init_epoch = 0\n","epochs=EPOCHS #increase the epochs"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:42:57.787232Z","iopub.status.busy":"2024-10-26T09:42:57.786818Z","iopub.status.idle":"2024-10-26T09:42:57.796647Z","shell.execute_reply":"2024-10-26T09:42:57.795464Z","shell.execute_reply.started":"2024-10-26T09:42:57.787191Z"},"papermill":{"duration":0.040794,"end_time":"2024-10-25T16:53:37.303948","exception":false,"start_time":"2024-10-25T16:53:37.263154","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["plt.style.use('seaborn-v0_8-bright')\n","plt.rcParams['axes.facecolor'] = 'white'\n","mpl.rcParams.update({\"axes.grid\" : True, \"grid.color\": \"black\"})\n","mpl.rc('axes',edgecolor='black')\n","mpl.rcParams.update({'font.size': 13})"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.03067,"end_time":"2024-10-25T16:53:37.366025","exception":false,"start_time":"2024-10-25T16:53:37.335355","status":"completed"},"tags":[]},"source":["# Test Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:42:57.798347Z","iopub.status.busy":"2024-10-26T09:42:57.797896Z","iopub.status.idle":"2024-10-26T09:42:57.807768Z","shell.execute_reply":"2024-10-26T09:42:57.806789Z","shell.execute_reply.started":"2024-10-26T09:42:57.798267Z"},"papermill":{"duration":0.040555,"end_time":"2024-10-25T16:53:37.439832","exception":false,"start_time":"2024-10-25T16:53:37.399277","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# class TestDataset(Dataset):\n","#     def __init__(self, eval_data):\n","#         self.eval_data = eval_data\n","#         self.num_sessions = len(self.eval_data[0])\n","#         self.num_seqs = len(self.eval_data[0][0])\n","\n","#     def __len__(self):\n","#         return math.ceil(len(self.eval_data) * self.num_sessions * self.num_seqs)\n","\n","#     def __getitem__(self, idx):\n","#         t_session = idx // self.num_seqs\n","#         user_idx = t_session // self.num_sessions\n","#         session_idx = t_session % self.num_sessions\n","#         seq_idx = idx % self.num_seqs\n","        \n","#         # Debugging statements\n","#         debug_log(f\"Index: {idx}, User Index: {user_idx}, Session Index: {session_idx}, Sequence Index: {seq_idx}\")\n","        \n","#         # Ensure that indices are within valid range\n","#         if user_idx < len(self.eval_data) and session_idx < len(self.eval_data[user_idx]) and seq_idx < len(self.eval_data[user_idx][session_idx]):\n","            \n","#             debug_log(f\"The length {len(self.eval_data[user_idx][session_idx][seq_idx])} \")\n","#             debug_log(f\"test ,{user_idx}, {session_idx},{ seq_idx}\")\n","#             data = self.eval_data[user_idx][session_idx][seq_idx]\n","\n","#             # Debugging statement to check the returned data\n","#             if data is None:\n","#                 error_log(f\"Returned data is None for index: {idx} in testdata\")\n","\n","#             return data,user_idx\n","#         else:\n","#             pass\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:42:57.809246Z","iopub.status.busy":"2024-10-26T09:42:57.80891Z","iopub.status.idle":"2024-10-26T09:42:57.823052Z","shell.execute_reply":"2024-10-26T09:42:57.821925Z","shell.execute_reply.started":"2024-10-26T09:42:57.809209Z"},"papermill":{"duration":0.047063,"end_time":"2024-10-25T16:53:37.51907","exception":false,"start_time":"2024-10-25T16:53:37.472007","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# The lenth of sequence calculate dynamically\n","class TestDataset(Dataset):\n","    def __init__(self, eval_data):\n","        self.eval_data = eval_data\n","        self.num_sessions = [len(user_sessions) for user_sessions in self.eval_data]  # List of number of sessions for each user\n","        self.num_seqs = [len(session) for user_sessions in self.eval_data for session in user_sessions]  # Total sequences across all users\n","\n","    def __len__(self):\n","        # Total length of dataset will be the sum of all sequences across all users and sessions\n","        return sum(len(self.eval_data[user_idx][session_idx]) for user_idx in range(len(self.eval_data))\n","                   for session_idx in range(len(self.eval_data[user_idx])))\n","\n","    def __getitem__(self, idx):\n","        # Find the user index and session index dynamically\n","        cumulative_length = 0\n","        for user_idx in range(len(self.eval_data)):\n","            for session_idx in range(len(self.eval_data[user_idx])):\n","                session_length = len(self.eval_data[user_idx][session_idx])\n","                if cumulative_length + session_length > idx:\n","                    seq_idx = idx - cumulative_length\n","                    data = self.eval_data[user_idx][session_idx][seq_idx]\n","\n","                    # Debugging statements\n","                    debug_log(f\"Index: {idx}, User Index: {user_idx}, Session Index: {session_idx}, Sequence Index: {seq_idx}\")\n","\n","                    # Check if data is None\n","                    if data is None:\n","                        error_log(f\"Returned data is None for index: {idx} in testdata\")\n","                    return data, user_idx\n","\n","                cumulative_length += session_length\n","        \n","        # If we get here, idx is out of bounds\n","        raise IndexError(\"Index out of bounds for dataset.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:42:57.824856Z","iopub.status.busy":"2024-10-26T09:42:57.824477Z","iopub.status.idle":"2024-10-26T09:42:57.836092Z","shell.execute_reply":"2024-10-26T09:42:57.835148Z","shell.execute_reply.started":"2024-10-26T09:42:57.824821Z"},"papermill":{"duration":0.048945,"end_time":"2024-10-25T16:53:37.60062","exception":false,"start_time":"2024-10-25T16:53:37.551675","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["best_accuracy = BEST_ACCURACY\n","best_loss = BEST_LOSS\n","training_losses = []\n","validation_losses = []\n","feature_embeddings_train=[]\n","saved_models = []\n","max_saved_models = MAX_SAVED_MODELS  "]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.037337,"end_time":"2024-10-25T16:53:37.678743","exception":false,"start_time":"2024-10-25T16:53:37.641406","status":"completed"},"tags":[]},"source":["# Training phase and store the model using both losses and Accuracy, Store Training accuracy\n","\n","The number of model store based on variable MAX_SAVED_MODEL"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:42:57.837969Z","iopub.status.busy":"2024-10-26T09:42:57.837644Z","iopub.status.idle":"2024-10-26T09:51:14.07653Z","shell.execute_reply":"2024-10-26T09:51:14.075521Z","shell.execute_reply.started":"2024-10-26T09:42:57.837935Z"},"trusted":true},"outputs":[],"source":["def find_worst_loss_and_acc_in_existing_model(saved_models):\n","    worst_loss = float('-inf')\n","    worst_accuracy = float('inf')\n","    for model_info in saved_models:\n","        loss = model_info['loss']\n","        accuracy = model_info['accuracy']\n","        if worst_loss <  loss:\n","            worst_loss = loss\n","        if worst_accuracy > accuracy:\n","            worst_accuracy = accuracy\n","    return  worst_loss, worst_accuracy\n","\n","def memory_is_low():\n","    \"\"\"Check if memory is low. You can implement your own logic here.\"\"\"\n","    # Placeholder logic; replace with actual memory checking\n","    import psutil\n","    return psutil.virtual_memory().available < (100 * 1024 * 1024)  # Less than 100 MB\n","\n","def find_worst_model_by_accuracy(saved_models):\n","    worst_model = None\n","    worst_accuracy = float('inf')\n","    for model_info in saved_models:\n","        if model_info['accuracy'] < worst_accuracy:\n","            worst_accuracy = model_info['accuracy']\n","            worst_model = model_info\n","    if worst_model:\n","        print_log(f\"Worst model - Path: {worst_model['path']}, Accuracy: {worst_model['accuracy']:.6f}, Loss: {worst_model['loss']:.6f}\")\n","    if worst_model:\n","        if os.path.exists(worst_model['path']):\n","            os.remove(worst_model['path'])\n","            print_log(f\"Deleted worst model by less accuracy: {worst_model['path']} with Accuracy: {worst_model['accuracy']:.6f}, Loss: {worst_model['loss']:.6f}\")\n","        saved_models.remove(worst_model)\n","\n","def find_worst_model_by_loss(saved_models):\n","    worst_model = None\n","    worst_loss = float('-inf')\n","    for model_info in saved_models:\n","        if model_info['loss'] > worst_loss:\n","            worst_loss = model_info['loss']\n","            worst_model = model_info\n","    if worst_model:\n","        print_log(f\"Worst model - Path: {worst_model['path']}, Accuracy: {worst_model['accuracy']:.6f}, Loss: {worst_model['loss']:.6f}\")\n","    if worst_model:\n","        if os.path.exists(worst_model['path']):\n","            os.remove(worst_model['path'])\n","            print_log(f\"Deleted worst model by higher loss: {worst_model['path']} with Accuracy: {worst_model['accuracy']:.6f}, Loss: {worst_model['loss']:.6f}\")\n","        saved_models.remove(worst_model)\n","\n","def find_worst_model_by_combined_metric(saved_models, weight_loss=0.5, weight_accuracy=0.5):\n","    worst_model = None\n","    worst_combined_metric = float('inf')\n","    combined_matrix_values = []\n","    max_loss = 100\n","    min_loss = 0\n","    max_accuracy = 100\n","    min_accuracy = 0\n","    for model_info in saved_models:\n","        normalized_loss = (model_info['loss'] - min_loss) / (max_loss - min_loss)\n","        normalized_accuracy = (model_info['accuracy'] - min_accuracy) / (max_accuracy - min_accuracy)\n","        # find combined metric, We want high accuracy and less losses\n","        combined_metric = (weight_accuracy * normalized_accuracy) -(weight_loss * normalized_loss)\n","        combined_matrix_values.append(combined_metric)\n","        model_info[\"combined_metric\"] = combined_metric\n","        \n","        # Find the model with the smallest combined metric\n","        if combined_metric < worst_combined_metric:\n","            worst_combined_metric = combined_metric\n","            worst_model = model_info\n","    if worst_model:\n","        print_log(f\"Worst model : Path: {worst_model['path']}, Accuracy: {worst_model['accuracy']:.6f}, Loss: {worst_model['loss']:.6f}\")\n","        print_log(f\"Because this is have minimum combined matrix = {worst_combined_metric}, All combined matric {combined_matrix_values}\")\n","    if worst_model:\n","        if os.path.exists(worst_model['path']):\n","            os.remove(worst_model['path'])\n","            print_log(f\"Deleted worst model by less accuracy and higher loss: {worst_model['path']} with Accuracy: {worst_model['accuracy']:.6f}, Loss: {worst_model['loss']:.6f}\")\n","        saved_models.remove(worst_model)\n","\n","total_losses = []\n","total_accuracy = []\n","\n","for i in range(init_epoch, epochs):\n","# for epoch in range(epochs):\n","    model_saved = 0\n","    print(f\"Epoch {i+1} started\")\n","    t_loss = 0.0\n","    start = time.time()\n","    model.train(True)\n","    \n","    for batch_idx, item in enumerate(dataloader):\n","        anchor, positive, negatives, anchor_class, negative_classes  = item\n","        optimizer.zero_grad()\n","\n","        # Forward pass for anchor, positive, and negatives\n","        anchor_scores, anchor_features = model(anchor.float())\n","        positive_scores, positive_features = model(positive.float())\n","\n","        # Separate negative class scores and features\n","        negative_class_scores, negative_features = zip(*[model(neg.float()) for neg in negatives])\n","        negative_class_scores = []\n","        negative_features = []\n","        for neg in negatives:\n","            negative_class_score, negative_feature = model(neg.float())\n","            negative_class_scores.append(negative_class_score)\n","            negative_features.append(negative_feature)\n","        \n","        # Convert negative features to a tensor\n","        negative_features = torch.stack(negative_features)  # Shape: (batch_size, num_negatives, embedding_dim)\n","        \n","        # Debug prints to check sizes\n","#         print(f\"Anchor features shape: {anchor_features.shape}\")\n","#         print(f\"Positive features shape: {positive_features.shape}\")\n","#         print(f\"Negative features shape: {negative_features.shape}\")\n","\n","#         # Check if dimensions are correct\n","#         assert anchor_features.size(0) == negative_features.size(0), \"Batch sizes must match\"\n","\n","        # Compute N-Pair Loss with negative features\n","        nPair_loss = loss_fn(anchor_features, positive_features, negative_features)\n","\n","        # Classification with CrossEntropy Loss\n","        all_class_scores = torch.cat([anchor_scores, positive_scores] + negative_class_scores, dim=0)\n","        all_labels = torch.cat([anchor_class, anchor_class] + negative_classes, dim=0)\n","        class_loss = nn.CrossEntropyLoss()(all_class_scores, all_labels)\n","\n","        # Combine the losses\n","        total_loss = WEIGHT_NPAIR_LOSS * nPair_loss + WEIGHT_CLASSIFIER_LOSS * class_loss\n","        total_loss.backward()\n","        optimizer.step()\n","        \n","        \n","        t_loss += total_loss.item()\n","        \n","    \n","    t_loss /= len(dataloader)\n","    training_losses.append(t_loss)\n","    \n","#     accuracy_train = accuracy_score(all_labels_train, predicted_classes_train)\n","#     print(\"accuracy of train\", accuracy_train)\n","\n","    \n","    # Validation phase\n","    model.eval()\n","    v_loss = 0.0\n","    all_preds = []\n","    all_labels = []\n","    t_dataset = TestDataset(validation_data)\n","    \n","    t_dataloader = DataLoader(t_dataset, batch_size=batch_size, shuffle=False)\n","    tot = 0\n","    print_log(f\"The lenth of t_dataloader : {len(t_dataloader)}\")\n","    for batch_idx_t, item_t in enumerate(t_dataloader):\n","        with torch.no_grad():\n","            val = tot // 992\n","            tot += 1\n","\n","            item_t_in, class_label = item_t\n","            # Get model outputs\n","            logits = model(item_t_in.float()) \n","            \n","            # Apply softmax to get probabilities\n","            probabilities = torch.softmax(logits[0], dim=1)  # Assuming logits[0] contains class scores\n","            predicted_classes = torch.argmax(probabilities, dim=1)  # Get predicted classes\n","            true_labels = class_label\n","            correct_predictions = (predicted_classes == true_labels)\n","            \n","            accuracy = correct_predictions.sum().item() / len(true_labels)\n","            \n","#             print(f\"Batch {batch_idx_t} - True Labels: {true_labels.tolist()} - Predicted: {predicted_classes.tolist()} -Class Labels: {class_label}\")\n","            all_preds.extend(predicted_classes.tolist())\n","            all_labels.extend(true_labels.tolist())\n","#             val_loss = classification_loss_fn(class_scores, true_labels)\n","#             v_loss += val_loss.item()\n","#     v_loss /= len(t_dataloader)\n","#     validation_losses.append(v_loss)\n","    accuracy = accuracy_score(all_labels, all_preds)\n","    precision = precision_score(all_labels, all_preds, average='macro')\n","    recall = recall_score(all_labels, all_preds, average='macro')\n","    f1 = f1_score(all_labels, all_preds, average='macro')\n","    print_log(f\"loop done  {tot}\")\n","    end = time.time()\n","    \n","    total_losses.append(t_loss)\n","    total_accuracy.append(accuracy)\n","    \n","    print_log(f\"------> Epoch No: {i+1} => Loss: {t_loss:.6f} >> Accuracy: {accuracy:.6f} >> Precision: {precision:.6f} >> Recall: {recall:.6f} >> F1: {f1:.6f} >> Time: {end-start:.2f}\")\n","#     scheduler.step(t_loss)\n","    # log metrics to wandb\n","    wandb.log({\"accuracy\": accuracy, \"loss\": t_loss, \"precision\" : precision, \"recall\" : recall, \"f1\" : f1})\n","\n","    \n","    # Check memory status\n","    if memory_is_low():\n","        print_log(f\"Memory is running low, attempting to free up space by deleting the worst model.\")\n","        # find_worst_model_by_loss(saved_models)\n","        # find_worst_model_by_accuracy(saved_models)\n","        find_worst_model_by_combined_metric(saved_models)\n","        # assign last best loss and accuracy in existing models\n","        if len(saved_models) > max_saved_models:\n","            best_loss, best_accuracy = find_worst_loss_and_acc_in_existing_model(saved_models)\n","\n","    # Save model if validation loss improves and This model not saved before\n","    if t_loss < best_loss:\n","        print_log(f\"Loss improved from {best_loss:.6f} to {t_loss:.6f}. \")\n","        if model_saved:\n","            print_log(\"Already model saved. Skip this model by loss.\")\n","        else:\n","            print_log(\"Saving model.................\")\n","            model_path = f\"{best_model_save_path}/epoch_{i+1}_accuracy_{accuracy:.6f}_loss_{t_loss:.6f}.pt\"\n","            torch.save(model, model_path)\n","            print_log(f\"Best model saved at (by loss): {model_path}\")\n","            saved_models.append({'path': model_path, 'loss': t_loss, 'accuracy' : accuracy})\n","            # Model saved\n","            model_saved = 1\n","    \n","            if len(saved_models) > max_saved_models:\n","                # find_worst_model_by_loss(saved_models)\n","                # find_worst_model_by_accuracy(saved_models)\n","                find_worst_model_by_combined_metric(saved_models)\n","            \n","                # assign last best loss and accuracy in existing models\n","                best_loss, best_accuracy = find_worst_loss_and_acc_in_existing_model(saved_models)\n","\n","\n","    # Save model if validation accuracy improves\n","    if accuracy > best_accuracy :\n","        print_log(f\"Accuracy improved from {best_accuracy:.6f} to {accuracy:.6f}.\")\n","        if model_saved:\n","            print_log(\"But, Already model saved. Skip this model by accuracy.\")\n","        else:\n","            print_log(\"Saving model.................\")\n","            model_path = f\"{best_model_save_path}/epoch_{i+1}_accuracy_{accuracy:.6f}_loss_{t_loss:.6f}.pt\"\n","            torch.save(model, model_path)\n","            print_log(f\"Best model saved at (by accuracy): {model_path}\")\n","            saved_models.append({'path': model_path, 'loss' : t_loss, 'accuracy': -accuracy})\n","            # Model saved\n","            model_saved = 1\n","    \n","            if len(saved_models) > max_saved_models:\n","                # find_worst_model_by_loss(saved_models)\n","                # find_worst_model_by_accuracy(saved_models)\n","                find_worst_model_by_combined_metric(saved_models)\n","            \n","                # assign last best loss and accuracy in existing models\n","                best_loss, best_accuracy = find_worst_loss_and_acc_in_existing_model(saved_models)\n","    \n","\n","print(f\"total_losses : {total_losses}\")\n","print(f\"total_accuracy : {total_accuracy}\")"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.052091,"end_time":"2024-10-25T17:50:23.714674","exception":false,"start_time":"2024-10-25T17:50:23.662583","status":"completed"},"tags":[]},"source":["# Get best model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:51:14.079198Z","iopub.status.busy":"2024-10-26T09:51:14.078779Z","iopub.status.idle":"2024-10-26T09:51:14.39177Z","shell.execute_reply":"2024-10-26T09:51:14.390761Z","shell.execute_reply.started":"2024-10-26T09:51:14.079151Z"},"papermill":{"duration":0.508947,"end_time":"2024-10-25T17:50:24.274782","exception":false,"start_time":"2024-10-25T17:50:23.765835","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["directory = '/kaggle/working/best_models'\n","\n","# Get the list of files in the directory\n","files = os.listdir(directory)\n","\n","# Print the list of files\n","print_log(f\"The model file directory : {directory}\")\n","for file in files:\n","    print_log(f\"file  :  {file}\")\n","\n","# # Function to extract EER from filename\n","# def extract_accuracy(filename):\n","#     try:\n","#         parts = filename.split('_')\n","#         accuracy_index = parts.index('accuracy') + 1\n","#         accuracy_value = float(parts[accuracy_index].replace('.pt', ''))\n","#         return accuracy_value\n","#     except (ValueError, IndexError):\n","#         return float('inf')\n","\n","# # Find the file with the lowest EER\n","# best_model_file = max(files, key=extract_accuracy)\n","# best_acc = extract_accuracy(best_model_file)\n","\n","# Function to extract accuracy and loss from filename\n","def extract_info(filename):\n","    parts = filename.split('_')\n","    epoch_value = int(parts[parts.index('epoch') + 1])\n","    accuracy_value = float(parts[parts.index('accuracy') + 1])\n","    loss_value = float(parts[parts.index('loss') + 1].replace('.pt', ''))\n","    return epoch_value, accuracy_value, loss_value\n","\n","\n","# Function to find the best model using both loss and accuracy\n","def find_best_model_using_normalization(file_list, weight_loss=0.5, weight_accuracy=0.5):\n","    best_model = None\n","    best_combined_metric = float('-inf')\n","    combined_matrix_values = []\n","    saved_models = []\n","\n","    # Extract accuracy and loss from the filenames\n","    for f in file_list:\n","        file_info =  extract_info(f)\n","        saved_models.append({'filename': f,'epoch' : file_info[0], 'accuracy': file_info[1], 'loss': file_info[2]})\n","\n","    # Calculate statistical parameters needed for normalization\n","    losses = [model['loss'] for model in saved_models]\n","    accuracies = [model['accuracy'] for model in saved_models]\n","\n","    max_loss = 100\n","    min_loss = 0\n","    max_accuracy = 100\n","    min_accuracy = 0\n","\n","    # Apply the selected normalization method\n","    for model_info in saved_models:\n","        loss = model_info['loss']\n","        accuracy = model_info['accuracy']\n","        normalized_loss = (loss - min_loss) / (max_loss - min_loss)\n","        normalized_accuracy = (accuracy - min_accuracy)/(max_accuracy - min_accuracy)\n","        \n","        # Combined metric: prioritize higher accuracy and lower loss\n","        combined_metric = (weight_accuracy * normalized_accuracy) - (weight_loss * normalized_loss)\n","        combined_matrix_values.append(combined_metric)\n","\n","        # Find the model with the highest combined metric\n","        if combined_metric > best_combined_metric:\n","            best_combined_metric = combined_metric\n","            best_model = model_info\n","\n","    # Print best model information\n","    if best_model:\n","        print_log(f\"Best model - File: {best_model['filename']}, Accuracy: {best_model['accuracy']:.6f}, Loss: {best_model['loss']:.6f}\")\n","        print_log(f\"Best combined metric: {best_combined_metric} in all combined metrics: {combined_matrix_values}\")\n","\n","    return best_model\n","\n","# Find the best model using Min-Max Normalization\n","print_log(\"Best Model using Min-Max Normalization\")\n","best_model = find_best_model_using_normalization(files)\n","\n","\n","\n","best_model_file = best_model['filename']\n","best_acc = best_model['accuracy']\n","best_loss = best_model['loss']\n","wandb.log({\n","    \"best_model_epoch\": best_model[\"epoch\"],\n","    \"best_model_accuracy\": best_model['accuracy'],\n","    \"best_model_loss\": best_model['loss']\n","})\n","\n","# Load the best model\n","best_model_path = os.path.join(directory, best_model_file)\n","test_model = torch.load(best_model_path)\n","\n","print_log(f\"Best model: {best_model_file} with accuracy: {best_acc} and loss: {best_loss}\")\n","print_log(f\"Loaded model from: {best_model_path}\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:51:14.393302Z","iopub.status.busy":"2024-10-26T09:51:14.392936Z","iopub.status.idle":"2024-10-26T09:51:14.702525Z","shell.execute_reply":"2024-10-26T09:51:14.701533Z","shell.execute_reply.started":"2024-10-26T09:51:14.393263Z"},"papermill":{"duration":0.395437,"end_time":"2024-10-25T17:50:24.722713","exception":false,"start_time":"2024-10-25T17:50:24.327276","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Plotting training and validation loss\n","plt.figure(figsize=(10, 5))\n","plt.plot(training_losses, label='Training Loss')\n","plt.plot(validation_losses, label='Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.title('Training Loss Over Epochs')\n","plt.show()\n","print_log(\"Training loss over epoches polt is create sucessfully\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:51:14.704176Z","iopub.status.busy":"2024-10-26T09:51:14.703829Z","iopub.status.idle":"2024-10-26T09:51:14.723753Z","shell.execute_reply":"2024-10-26T09:51:14.722631Z","shell.execute_reply.started":"2024-10-26T09:51:14.704141Z"},"papermill":{"duration":0.062886,"end_time":"2024-10-25T17:50:24.839711","exception":false,"start_time":"2024-10-25T17:50:24.776825","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["test_dataset = TestDataset(test_data)\n","test_dataloader = DataLoader(test_dataset, batch_size=2)\n","feature_embeddings = []\n","tot=0\n","all_preds_test = []\n","all_labels_test = []"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:51:14.725183Z","iopub.status.busy":"2024-10-26T09:51:14.724871Z","iopub.status.idle":"2024-10-26T09:51:56.054072Z","shell.execute_reply":"2024-10-26T09:51:56.053259Z","shell.execute_reply.started":"2024-10-26T09:51:14.72515Z"},"papermill":{"duration":40.866522,"end_time":"2024-10-25T17:51:05.759084","exception":false,"start_time":"2024-10-25T17:50:24.892562","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["tot=0\n","for batch_idx_t, item_t in enumerate(test_dataloader):\n","    with torch.no_grad():\n","        val=tot//162 ## this value needs to be changed based on the sequence from each activity\n","        tot+=1\n","        item_t_in,class_label=item_t\n","#         print(\"tot\",tot)\n","        true_labels=class_label\n","\n","        item_out = test_model(item_t_in.float())\n","        class_scores=item_out[0]\n","#         print(class_scores)\n","        feature_embeddings.append(item_out[1])\n","        predicted_classes = torch.argmax(class_scores, dim=1)\n","        all_preds_test.extend(predicted_classes.tolist())\n","        all_labels_test.extend(true_labels.tolist())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:51:56.055647Z","iopub.status.busy":"2024-10-26T09:51:56.055305Z","iopub.status.idle":"2024-10-26T09:51:56.675852Z","shell.execute_reply":"2024-10-26T09:51:56.674903Z","shell.execute_reply.started":"2024-10-26T09:51:56.055611Z"},"papermill":{"duration":1.010718,"end_time":"2024-10-25T17:51:06.82091","exception":false,"start_time":"2024-10-25T17:51:05.810192","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Move each tensor to CPU, detach from computation graph, and convert to NumPy array\n","feature_embeddings_cpu = [emb.cpu().detach().numpy() for emb in feature_embeddings]\n","\n","# Check for NaNs in individual arrays\n","for i, emb_np in enumerate(feature_embeddings_cpu):\n","    if np.isnan(emb_np).any():\n","        print(f\"NaN detected in feature_embeddings_cpu at index {i}\")\n","\n","# Concatenate the list of numpy arrays into a single numpy array\n","feature_embeddings_np = np.concatenate(feature_embeddings_cpu, axis=0)\n","\n","# Check for NaNs in the concatenated array\n","if np.isnan(feature_embeddings_np).any():\n","    print(\"NaN detected in feature_embeddings_np\")\n","\n","# Perform PCA to reduce dimensions to 2\n","from sklearn.decomposition import PCA\n","\n","pca = PCA(n_components=2)\n","principal_components = pca.fit_transform(feature_embeddings_np)\n","\n","# Check for NaNs in PCA results\n","if np.isnan(principal_components).any():\n","    print(\"NaN detected in principal_components\")\n","\n","# Plot PCA results\n","import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(8, 6))\n","plt.scatter(principal_components[:, 0], principal_components[:, 1], alpha=0.5)\n","plt.title('PCA of Feature Embeddings')\n","plt.xlabel('Principal Component 1')\n","plt.ylabel('Principal Component 2')\n","plt.grid(True)\n","plt.show()\n","print_log(\"PCA of Feature embeddings polt is create sucessfully\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:51:56.677504Z","iopub.status.busy":"2024-10-26T09:51:56.67715Z","iopub.status.idle":"2024-10-26T09:51:57.94408Z","shell.execute_reply":"2024-10-26T09:51:57.943153Z","shell.execute_reply.started":"2024-10-26T09:51:56.677468Z"},"papermill":{"duration":1.452719,"end_time":"2024-10-25T17:51:08.331483","exception":false,"start_time":"2024-10-25T17:51:06.878764","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Assuming all_labels_test contains the true labels for the data points\n","# Map each class to a different color using a colormap\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.decomposition import PCA\n","import seaborn as sns\n","\n","# Convert feature embeddings to NumPy array (already done in previous steps)\n","# feature_embeddings_cpu = [emb.cpu().detach().numpy() for emb in feature_embeddings]\n","# feature_embeddings_np = np.concatenate(feature_embeddings_cpu, axis=0)\n","\n","# Perform PCA to reduce dimensions to 2\n","pca = PCA(n_components=2)\n","principal_components = pca.fit_transform(feature_embeddings_np)\n","\n","# Create a scatter plot with different colors for each class\n","plt.figure(figsize=(8, 6))\n","\n","# Get a colormap with enough colors for each class\n","num_classes = len(np.unique(all_labels_test))\n","palette = sns.color_palette(\"hsv\", num_classes)\n","\n","# Scatter plot with colors based on class labels\n","for class_id in np.unique(all_labels_test):\n","    # Filter the points belonging to the current class\n","    indices = np.where(np.array(all_labels_test) == class_id)\n","    plt.scatter(principal_components[indices, 0], \n","                principal_components[indices, 1], \n","                alpha=0.6, \n","                label=activity_id_mapping[class_id], \n","                color=palette[class_id])\n","\n","# Add plot details\n","plt.title('PCA of Feature Embeddings by Class')\n","plt.xlabel('Principal Component 1')\n","plt.ylabel('Principal Component 2')\n","plt.legend(title=\"Class\", loc='best', bbox_to_anchor=(1, 1), ncol=1)  # Show legend outside plot\n","plt.grid(True)\n","plt.tight_layout()\n","\n","# Show the plot\n","plt.show()\n","\n","print_log(\"PCA of Feature embeddings plot with class colors created successfully\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:51:57.945874Z","iopub.status.busy":"2024-10-26T09:51:57.945594Z","iopub.status.idle":"2024-10-26T09:51:58.005606Z","shell.execute_reply":"2024-10-26T09:51:58.00455Z","shell.execute_reply.started":"2024-10-26T09:51:57.945842Z"},"papermill":{"duration":0.121522,"end_time":"2024-10-25T17:51:08.515334","exception":false,"start_time":"2024-10-25T17:51:08.393812","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["accuracy_test = accuracy_score(all_labels_test, all_preds_test)\n","precision_test = precision_score(all_labels_test, all_preds_test, average='macro')\n","recall_test = recall_score(all_labels_test, all_preds_test, average='macro')\n","f1_test = f1_score(all_labels_test, all_preds_test, average='macro')\n","\n","print_log(f\"Accuracy: {accuracy_test:.6f} - Precision: {precision_test:.6f} - Recall: {recall_test:.6f} - F1: {f1_test:.6f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:51:58.007789Z","iopub.status.busy":"2024-10-26T09:51:58.00732Z","iopub.status.idle":"2024-10-26T09:52:00.114655Z","shell.execute_reply":"2024-10-26T09:52:00.113667Z","shell.execute_reply.started":"2024-10-26T09:51:58.007742Z"},"papermill":{"duration":2.056571,"end_time":"2024-10-25T17:51:10.634603","exception":false,"start_time":"2024-10-25T17:51:08.578032","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["conf_matrix = confusion_matrix(all_labels_test, all_preds_test)\n","\n","# Calculate the accuracy for each class\n","class_accuracies = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n","\n","# Print the accuracy for each class\n","for i, class_accuracy in enumerate(class_accuracies):\n","    print_log(f\"Accuracy for class {i}: {class_accuracy:.6f}\")\n","\n","\n","plt.figure(figsize=(10, 7))\n","sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","plt.title('Confusion Matrix')\n","plt.savefig('confusion_matrix.png')\n","plt.show()\n","print_log(\"Confusion matrix is create sucessfully using test\")\n","wandb.log({\"confusion_matrix\": wandb.Image('confusion_matrix.png')})"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:52:00.116742Z","iopub.status.busy":"2024-10-26T09:52:00.116077Z","iopub.status.idle":"2024-10-26T09:52:00.939483Z","shell.execute_reply":"2024-10-26T09:52:00.938411Z","shell.execute_reply.started":"2024-10-26T09:52:00.116699Z"},"papermill":{"duration":0.818041,"end_time":"2024-10-25T17:51:11.521174","exception":false,"start_time":"2024-10-25T17:51:10.703133","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["from sklearn.metrics import precision_score, recall_score, f1_score\n","import pandas as pd\n","\n","# Initialize lists to store results\n","class_names = []\n","precision_list = []\n","recall_list = []\n","f1_list = []\n","\n","# Calculate precision, recall, and f1-score for each class\n","for class_id, class_name in activity_id_mapping.items():\n","    # Precision, recall, and f1 for the current class without averaging\n","    precision = precision_score(all_labels_test, all_preds_test, labels=[class_id], average=None, zero_division=0)\n","    recall = recall_score(all_labels_test, all_preds_test, labels=[class_id], average=None, zero_division=0)\n","    f1 = f1_score(all_labels_test, all_preds_test, labels=[class_id], average=None, zero_division=0)\n","    \n","    # Append results to respective lists\n","    class_names.append(class_name)\n","    precision_list.append(precision[0])  # Since labels=[class_id] returns a list\n","    recall_list.append(recall[0])        # Extract the first element for that class\n","    f1_list.append(f1[0])\n","\n","\n","class_names = class_names[0: CLASSES]\n","precision_list = precision_list[0: CLASSES]\n","recall_list = recall_list[0: CLASSES]\n","f1_list = f1_list[0: CLASSES]\n","\n","\n","# Calculate the average across all classes (macro average)\n","average_precision = sum(precision_list) / len(precision_list)\n","average_recall = sum(recall_list) / len(recall_list)\n","average_f1 = sum(f1_list) / len(f1_list)\n","\n","\n","\n","class_names.append('Average')\n","recall_list.append(average_recall)\n","precision_list.append(average_precision)\n","f1_list.append(average_f1)\n","\n","# Create a DataFrame to store the results\n","results_df = pd.DataFrame({\n","    'Class': class_names,\n","    'Recall': recall_list,\n","    'Precision': precision_list,\n","    'F1-score': f1_list\n","})\n","\n","results_df.to_csv('class_precision_recall_f1.csv', index=False)\n","\n","# Display the final table\n","results_df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:52:00.94122Z","iopub.status.busy":"2024-10-26T09:52:00.940797Z","iopub.status.idle":"2024-10-26T09:52:02.686616Z","shell.execute_reply":"2024-10-26T09:52:02.685608Z","shell.execute_reply.started":"2024-10-26T09:52:00.941173Z"},"papermill":{"duration":1.866157,"end_time":"2024-10-25T17:51:13.452167","exception":false,"start_time":"2024-10-25T17:51:11.58601","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["conf_matrix = confusion_matrix(all_labels, all_preds)\n","\n","# Calculate the accuracy for each class\n","class_accuracies = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n","\n","# Print the accuracy for each class\n","for i, class_accuracy in enumerate(class_accuracies):\n","    print_log(f\"Accuracy for class {i}: {class_accuracy:.6f}\")\n","\n","plt.figure(figsize=(10, 7))\n","sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","plt.title('Confusion Matrix')\n","plt.savefig('confusion_matrix_validation.png')\n","plt.show()\n","print_log(\"Validation Confusion matrix is create sucessfully\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:52:02.688641Z","iopub.status.busy":"2024-10-26T09:52:02.688226Z","iopub.status.idle":"2024-10-26T09:52:04.335095Z","shell.execute_reply":"2024-10-26T09:52:04.333898Z","shell.execute_reply.started":"2024-10-26T09:52:02.688598Z"},"papermill":{"duration":2.612416,"end_time":"2024-10-25T17:51:16.135373","exception":false,"start_time":"2024-10-25T17:51:13.522957","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# [optional] finish the wandb run, necessary in notebooks\n","wandb.finish()"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":1246162,"sourceId":2078683,"sourceType":"datasetVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"papermill":{"default_parameters":{},"duration":3551.555585,"end_time":"2024-10-25T17:51:18.926873","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-10-25T16:52:07.371288","version":"2.6.0"}},"nbformat":4,"nbformat_minor":4}
